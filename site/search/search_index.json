{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mysite","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/","title":"About","text":""},{"location":"about/#my-name-is-haoxin-sang","title":"My name is Haoxin Sang","text":""},{"location":"notes/naming_conventions/","title":"Naming Conventions in Python","text":"<p>Naming conventions are an important part of writing clean and readable code. In Python, the PEP 8 style guide provides recommendations on naming conventions. Here\u2019s a summary of the conventions along with special cases and additional tips:</p>"},{"location":"notes/naming_conventions/#naming-conventions","title":"Naming Conventions","text":"<ol> <li> <p>Variables:</p> </li> <li> <p>Use <code>snake_case</code> for variable names.</p> </li> <li>Example: <code>my_variable</code>, <code>total_count</code>, <code>is_valid</code>.</li> <li> <p>Functions:</p> </li> <li> <p>Use <code>snake_case</code> for function names.</p> </li> <li>Example: <code>def calculate_total()</code>, <code>def get_user_name()</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> (also known as <code>CamelCase</code>).</p> </li> <li>Example: <code>class MyClass</code>, <code>class UserProfile</code>.</li> <li> <p>Modules:</p> </li> <li> <p>Use <code>snake_case</code> for module names (file names).</p> </li> <li>Example: <code>import my_module</code>, <code>from user_profile import get_user_name</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>PI = 3.14159</code>, <code>MAX_CONNECTIONS = 10</code>.</li> </ol>"},{"location":"notes/naming_conventions/#special-cases","title":"Special Cases","text":"<ol> <li> <p>Abbreviations:</p> </li> <li> <p>Treat abbreviations as normal words and follow the same naming convention.</p> </li> <li>Variables and functions: <code>html_parser</code>, <code>parse_html</code>.</li> <li>Classes: <code>HtmlParser</code>.</li> <li> <p>Dashes (<code>-</code>):</p> </li> <li> <p>Dashes are not allowed in Python identifiers. Use underscores instead.</p> </li> <li>Example: <code>short_term</code> instead of <code>short-term</code>.</li> <li> <p>Names:</p> </li> <li> <p>Follow the same naming conventions for names. Treat them as any other word.</p> </li> <li>Example: <code>jack_smith</code> for a variable or function, <code>class JackSmith</code> for a class.</li> <li> <p>Numbers:</p> </li> <li> <p>Numbers can be included, but they should not start with a number.</p> </li> <li>Variables and functions: <code>user_123</code>, <code>find_25th_element</code>.</li> <li>Classes: <code>User123</code>, <code>Element25th</code>.</li> </ol>"},{"location":"notes/naming_conventions/#additional-tips","title":"Additional Tips","text":"<ol> <li> <p>Descriptive Names:</p> </li> <li> <p>Choose names that clearly describe the purpose of the variable, function, or class.</p> </li> <li>Avoid single-character names except for counters or iterators (e.g., <code>i</code>, <code>j</code>, <code>k</code>).</li> <li> <p>Avoid Ambiguity:</p> </li> <li> <p>Ensure that names are not ambiguous and convey the intended meaning.</p> </li> <li>Example: <code>calculate_total_price</code> is more descriptive than <code>calculate</code>.</li> <li> <p>Consistency:</p> </li> <li> <p>Be consistent with your naming conventions throughout your codebase.</p> </li> <li>Stick to the same style to improve readability and maintainability.</li> <li> <p>Avoid Using Reserved Keywords:</p> </li> <li> <p>Do not use Python reserved keywords as names for variables, functions, or classes.</p> </li> <li>Example: <code>class</code>, <code>def</code>, <code>return</code>, etc.</li> </ol>"},{"location":"notes/naming_conventions/#summary","title":"Summary","text":"<p>Here's a summary of the recommended naming conventions:</p> <ul> <li>Variables: <code>snake_case</code></li> <li>Functions: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Modules: <code>snake_case</code></li> <li>Constants: <code>UPPER_CASE</code></li> </ul>"},{"location":"notes/naming_conventions/#examples","title":"Examples","text":"<pre><code># Variables\nuser_name = \"JohnDoe\"\ntotal_count = 10\n\n# Functions\ndef calculate_total(price, tax):\n    return price + tax\n\ndef get_user_name():\n    return \"John Doe\"\n\n# Classes\nclass UserProfile:\n    def __init__(self, name):\n        self.name = name\n\n# Constants\nMAX_CONNECTIONS = 100\nPI = 3.14159\n\n# Modules (file names)\n# user_profile.py\n# html_parser.py\n\n# Example with abbreviations, names, and numbers\nhtml_parser = HtmlParser()\njack_smith = \"Jack Smith\"\nuser_123 = User123()\nfind_25th_element = find_25th_element()\n</code></pre>"},{"location":"notes/naming_conventions/#underscores","title":"Underscores","text":"<p>Underscores are used in Python for various special naming conventions. Understanding when and how to use underscores is important for writing idiomatic Python code. Here's a detailed explanation of the different uses of underscores in Python:</p>"},{"location":"notes/naming_conventions/#single-leading-underscore-_var","title":"Single Leading Underscore <code>_var</code>","text":"<ul> <li>Purpose: Indicates a weak \"internal use\" indicator. This is a convention to tell other programmers that the variable or method is intended for internal use. It does not prevent access but suggests that it should be treated as a non-public part of the API.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self._internal_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-trailing-underscore-var_","title":"Single Trailing Underscore <code>var_</code>","text":"<ul> <li>Purpose: Used to avoid conflicts with Python keywords or built-in names.</li> <li>Example: <pre><code>def function_(parameter):\n    return parameter + 1\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-underscore-__var","title":"Double Leading Underscore <code>__var</code>","text":"<ul> <li>Purpose: Triggers name mangling, where the interpreter changes the name of the variable in a way that makes it harder to create subclasses that accidentally override the private attributes and methods. This is used to avoid name conflicts in subclasses.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self.__private_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-and-trailing-underscore-var","title":"Double Leading and Trailing Underscore <code>__var__</code>","text":"<ul> <li>Purpose: Indicates special methods (also known as \"magic methods\" or \"dunder methods\") that have special meaning in Python. These are predefined methods used to perform operator overloading, object creation, and other fundamental behaviors.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        pass\n\n    def __str__(self):\n        return \"MyClass instance\"\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-underscore-_","title":"Single Underscore <code>_</code>","text":"<ul> <li>Purpose: Used as a throwaway variable name. This is a convention for variables that are temporary or insignificant.</li> <li>Example: <pre><code>for _ in range(5):\n    print(\"Hello, World!\")\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#references","title":"References","text":"<ol> <li> <p>PEP 8 - Style Guide for Python Code:</p> </li> <li> <p>The official Python style guide, PEP 8, covers naming conventions, formatting, and best practices for writing Python code.</p> </li> <li>Link: PEP 8 - Style Guide for Python Code</li> <li> <p>Python Documentation:</p> </li> <li> <p>The official Python documentation provides guidelines on naming conventions and special methods (also known as magic methods or dunder methods).</p> </li> <li>Link: Python Documentation</li> </ol>"},{"location":"notes/naming_conventions/#specific-sections-in-pep-8","title":"Specific Sections in PEP 8","text":"<ol> <li> <p>Naming Conventions:</p> </li> <li> <p>PEP 8 includes a section on naming conventions that describes how to name variables, functions, classes, constants, modules, and packages.</p> </li> <li>Link: PEP 8 - Naming Conventions</li> <li> <p>Method Names and Instance Variables:</p> </li> <li> <p>This section discusses conventions for naming methods and instance variables, including the use of single and double underscores.</p> </li> <li>Link: PEP 8 - Method Names and Instance Variables</li> <li> <p>Public and Internal Interfaces:</p> </li> <li> <p>Guidelines on how to distinguish between public and internal interfaces using naming conventions.</p> </li> <li>Link: PEP 8 - Public and Internal Interfaces</li> </ol>"},{"location":"notes/naming_conventions/#summary-of-naming-conventions-from-pep-8","title":"Summary of Naming Conventions from PEP 8","text":"<ul> <li> <p>Variables and Functions:</p> </li> <li> <p>Use <code>snake_case</code> for variables and function names.</p> </li> <li>Example: <code>my_variable</code>, <code>calculate_total</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> for class names.</p> </li> <li>Example: <code>MyClass</code>, <code>UserProfile</code>.</li> <li> <p>Modules and Packages:</p> </li> <li> <p>Use <code>snake_case</code> for module and package names.</p> </li> <li>Example: <code>my_module</code>, <code>user_profile</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>MAX_CONNECTIONS</code>, <code>PI</code>.</li> <li> <p>Private Variables and Methods:</p> </li> <li> <p>Use a single leading underscore <code>_</code> for weak internal use.</p> </li> <li>Example: <code>_internal_variable</code>, <code>_internal_method</code>.</li> <li> <p>Name Mangling:</p> </li> <li> <p>Use double leading underscores <code>__</code> to invoke name mangling.</p> </li> <li>Example: <code>__private_variable</code>, <code>__private_method</code>.</li> <li> <p>Special Methods:</p> </li> <li> <p>Use double leading and trailing underscores <code>__</code> for special methods.</p> </li> <li>Example: <code>__init__</code>, <code>__str__</code>.</li> <li> <p>Throwaway Variables:</p> </li> <li> <p>Use a single underscore <code>_</code> for throwaway variables.</p> </li> <li>Example: <code>for _ in range(10):</code>.</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/","title":"Week 1","text":""},{"location":"notes/lecture_notes/stat541_week1/#requirement","title":"Requirement","text":"<p>Statistics: Multiple Regression, Bias-var Decomposition</p> <p>Calculus: Interated Intigration</p>"},{"location":"notes/lecture_notes/stat541_week1/#supervised-learning","title":"Supervised Learning","text":"<p>Suppose we have data pairs</p> \\[ (x^{(1)},y^{(1)}),\\dots, (x^{(n)},y^{(n)}) \\sim P \\quad(iid.) \\] <p>where \\(P\\) is some distribution.</p>"},{"location":"notes/lecture_notes/stat541_week1/#our-goal","title":"Our Goal","text":"<p>Find a function \\(f:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) such that \\(f(x)\\approx y\\) when \\((x,y)\\sim P\\), where</p> <ul> <li>\\(x\\) is called: predictor variable / covariates / independent var. / inputs / features</li> <li>\\(y\\) is called: response var. / output var. / dependent var.</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#examples","title":"Examples:","text":"<ul> <li>\\(\\mathcal{Y} = \\mathbb{R}\\): Regression Problem</li> <li>\\(\\mathcal{Y}\\) is a finite set: Image Classification</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#learning-algorithm","title":"Learning Algorithm","text":"<p>To obtain an \\(f\\) we use the training data to output on Learning algorithm</p> \\[ \\phi_n: (\\mathcal{X}\\times \\mathcal{Y})^n \\longrightarrow \\mathcal{Y}^\\mathcal{X} \\] <p>where \\(\\mathcal{Y}^\\mathcal{X}\\) is the set of all functions from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\). Therefore,</p> \\[ \\hat{f} = \\phi_n(x^{(1)},y^{(1)},\\dots, x^{(n)},y^{(n)}) \\] <p>is a random function(1) determined by the data and the learning algorithm \\(\\phi_n\\). </p> <ol> <li>Note that a random function is a deterministic function. More precisely, a function of an arbitrary argument \\(t\\) (defined on the set \\(T\\) of its values, and taking numerical values or, more generally, values in a vector space) whose values are defined in terms of a certain experiment and may vary with the outcome of this experiment according to a given probability distribution.</li> </ol>"},{"location":"notes/lecture_notes/stat541_week1/#definition-loss-function","title":"Definition Loss function","text":"<p>Loss function \\(L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0,+\\infty)\\) is to estimate the error of \\(f\\). How close \\(f(x)\\) is to \\(y\\) is gauged by \\(L(f(x),y)\\).Examples: squared error loss and \\(0-1\\) loss.</p>"},{"location":"notes/lecture_notes/stat541_week1/#definition-risk-function","title":"Definition Risk function","text":"<ul> <li>Risk function (a.k.a. prediction error)</li> </ul> \\[ R(f,P) = E_{(x,y)}\\left(L(f(x),y)\\right) = \\int_{\\mathcal{X}\\times \\mathcal{Y}} L\\left(f(x),y\\right) p(x,y) \\,\\mathrm{d}x\\mathrm{d}y \\] <ul> <li>Oracle prediction error (a.k.a. Bayes Risk):</li> </ul> \\[ R^*(P) = \\inf_{f} R(f,P) \\] <ul> <li>Oracle predictor \\(f^*\\) satisfies</li> </ul> \\[ R(f^*,P) = R^*(P) \\]"},{"location":"notes/lecture_notes/stat541_week1/#compute-rp","title":"Compute \\(R^*(P)\\)","text":"<p>Compute \\(R^*(P)\\) for squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left((f(x,y) - y^2)\\right) \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} (y - f(x)^2) p(y|x) p(x) \\,\\mathrm{d}y \\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}p(x)\\left(\\int_{\\mathcal{Y}}(y - f(x)^2) p(y|x)\\,\\mathrm{d}y\\right)\\,\\mathrm{d}x \\end{aligned} \\] <p>For fixed \\(x\\), we minimize over the value of \\(f(x)\\), that is, it's suffice to set</p> \\[ (x) = \\operatorname*{arg\\, min}\\limits_z \\int_{\\mathcal{Y}}(y - f(x)^2) p(y|x)\\,\\mathrm{d}y. \\] <p>It is equivalent to minimize:</p> \\[ \\int_{\\mathcal{Y}} y^2 p(y|x)\\,\\mathrm{d}y - 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2\\cdot \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y. \\] <p>For the above equation, the first term is independent on \\(z\\) and</p> \\[ \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y = 1, \\] <p>for fixed \\(x\\). Then we have</p> \\[ \\begin{aligned} f(x) &amp;= \\operatorname*{arg\\, min}\\limits_{z} - 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2 \\\\ &amp;= \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y \\\\ &amp;= E(y|x).  \\end{aligned} \\] <p>Therefore, oracle predictor is given by \\(f^*(\\tilde{x})=E(y|X=\\tilde{x})\\). Similar arguments can be obtained for other loss functions. </p> <p>Additionally, our computation shows that making assumptions about the allowable \\(f\\), i.e. assume \\(f\\) lies in some set of functions \\(\\mathcal{F}\\), is somewhat equivalent to making assumptions about \\(P\\).</p>"},{"location":"notes/lecture_notes/stat541_week1/#making-assumptions","title":"Making Assumptions","text":"<p>Ideally we would like a \\(f\\) such that \\(R(f,P)\\) is small for all distribution \\(P\\). However, this is not possible by the No Free Lunch Theorem. Roughly, this says that for any \\(f\\), there exists a \\(P\\) such that \\(R(f,P)\\) is large. In classification, this says that there exists a \\(P\\) such that \\(f\\) is no better than random guessing(1). </p> <ol> <li>Random guessing is that we flip a coin and predict \\(y\\) based on the coin being heads or tails.</li> </ol> <p>Our solution is to make assumptions about \\(P\\):  </p> <p>Assume \\(P\\in\\mathcal{P}\\), where \\(\\mathcal{P}\\) is some subset of probability distributions. This suggest assumptions that we can make about the function class of predictors that we use. For example, \\(E(y|x)\\) is optimal for squared error loss. Given \\(\\mathcal{P}\\) we may want to restrict \\(\\mathcal{F}\\) to functions that have the form \\(E(y|x)\\) for \\(P\\in \\mathcal{P}\\).  </p>"},{"location":"notes/lecture_notes/stat541_week1/#complexity-bias-variance-tradeoff","title":"Complexity (bias-variance) tradeoff:","text":"<ul> <li>More complex function classes \\(\\mathcal{F}\\) -- low bias (i.e. able to approximate the oracle \\(E(y|x)\\))</li> <li>Large class of \\(\\mathcal{F}\\) -- it is hard to find the best \\(f\\).</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#error-decomposition","title":"Error Decomposition","text":"<p>For \\(f\\in \\mathcal{F}\\), since \\(\\mathcal{F}\\) may not be large enough, \\(R^*(P)\\) and \\(\\inf_{f\\in \\mathcal{F}} R(f,P)\\) may not be equal. We have the following decomposition of the risk function \\(R(f,P)\\):</p> \\[ {\\color{red} R(f,P) - \\inf_{f\\in \\mathcal{F}} R(f,P)} + {\\color{green} \\inf_{f\\in \\mathcal{F}} R(f,P) - R^*(P)} + {\\color{blue} R^*(P)}. \\] <ul> <li>\\({\\color{red} \\text{Red}}\\): Estimation error, which is non-negative.</li> <li>\\({\\color{green} \\text{Green}}\\): Approximation error, which is non-negative.</li> <li>\\({\\color{blue} \\text{Blue}}\\): the inherent error, which is the best we can do!</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#emperical-risk-minimization-erm","title":"Emperical Risk Minimization (ERM)","text":"<p>Idea is to find an approximation of \\(R(f,P)\\) and minimize this over \\(f\\) (also assume that \\(f\\) lies in some specified class of functions \\(\\mathcal{F}\\)). The ERM predictor \\(\\hat{f}\\) is defined as </p> \\[ \\hat{f} = \\operatorname*{arg\\, min}_{g\\in \\mathcal{F}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}L\\left(g(x^{(i)}),y^{(i)}\\right)\\right), \\] <p>which is called the average loss or emperical risk. Note that </p> <ul> <li>The emperical risk depends on the choice of function class \\(\\mathcal{F}\\), such as linear functions and complicated neural networks.</li> <li>Finding the <code>argmin</code> is not always easy, and hence there are various optimization algorithms approximating the <code>argmin</code>.</li> </ul>"},{"location":"notes/lecture_notes/stat541_week1/#gauge-learning-algorithms","title":"Gauge Learning Algorithms","text":"<p>Given a learning algorithm \\(\\phi_n\\), how can we gauge the performance of \\(\\phi_n\\)? We can look at \\(R(\\hat{f},P)\\), that is, we view the training data \\(\\mathcal{D}_n\\) as random and then \\(R(\\hat{f},P)\\)(1) is a random variable. The expected risk (a.k.a. expected prediction error) is given by</p> <ol> <li>Here \\(\\hat{f}\\) is dependent on \\(\\mathcal{D}_n\\), which should be written as \\(\\hat{f}_{\\mathcal{D}_n}\\), but for simplicity, we still denote it as \\(\\hat{f}\\). </li> </ol> \\[ R(\\phi_n,P) = E_{\\mathcal{D}_n}\\left(R(\\hat{f},P)\\right) = E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(L(\\hat{f},y)\\right)\\right).  \\] <p>There are two sources of randomness: </p> <ul> <li>The training data \\(\\mathcal{D}_n\\) that determines \\(\\hat{f}\\). </li> <li>The pair \\((x,y)\\) where we predict \\(y\\) using \\(\\hat{f}(x)\\). </li> </ul>"},{"location":"notes/lecture_notes/stat541_week2/","title":"Week2","text":""},{"location":"notes/lecture_notes/stat541_week2/#a-useful-technique","title":"A Useful Technique","text":"<p>Recall that the oracle predictor is given by</p> \\[ E(y|x) = \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y, \\] <p>which is a function of \\(x\\), denoted as \\(f^*(x)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#key-property","title":"Key Property","text":"<p>We will prove the following equation</p> \\[ E_{(x,y)}\\left(g(x)\\cdot E(y|x)\\right) = E_{(x,y)}\\left(g(x)\\cdot y\\right), \\] <p>which holds for all functions \\(g\\) where \\(Var\\left(g(x)\\right)&lt;+\\infty\\)(1). Specifically, when taking \\(g(x)\\equiv 1\\), we obtain the law of total expectation, i.e. \\(E\\left(y\\right) = E\\left(E(y|x)\\right)\\)(2).</p> <ol> <li>This a technical assumption which is usually satisfied in practical problems.</li> <li>One special case states that if \\(\\left\\{A_i\\right\\}\\) is a finite or countable partition of the sample space, then \\(E(X)=\\sum_i E\\left(X | A_i\\right) \\cdot Pr\\left(A_i\\right)\\). </li> </ol> <p>Obviously, it is equivalent to</p> \\[ E_{(x,y)}\\left(g(x)\\left(y-E(y|x)\\right)\\right) = 0, \\] <p>which is an orthogonality property.</p>"},{"location":"notes/lecture_notes/stat541_week2/#proof","title":"Proof","text":"\\[ \\begin{aligned} E_(x,y)\\left(g(x)\\cdot E(y|x)\\right)  &amp;= E_x\\left(g(x)\\cdot E(y|x)\\right) \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot E(y|x) p(x) \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot p(x) \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} g(x)\\cdot y \\cdot p(y|x) p(x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= E_{(x,y)}\\left(g(x)\\cdot y\\right), \\end{aligned} \\] <p>where the last equation is from \\(p(y|x) p(x) = p(x,y)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#reprove-the-oracle-predictor","title":"Reprove the Oracle Predictor","text":"<p>Let us show again that \\(E(y|x)\\) is the oracle predictor under the squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left(\\left(f(x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)+E(y|x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right) + 2E_{(x,y)}\\left({\\color{red} \\left(f(x)-E(y|x)\\right)}\\left(E(y|x)-y\\right)\\right) \\\\  &amp;\\quad + E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right) \\\\ &amp;= {\\color{green} E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right)} + {\\color{blue} E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right)}, \\end{aligned} \\] <p>where the last equation is from the equivalent form of the key property regarding the above \\(\\color{red}{\\text{red}}\\) term as a function of \\(x\\). Note that both the \\(\\color{green}{\\text{green}}\\) and \\(\\color{blue}{\\text{blue}}\\) term are non-negative, and the \\(\\color{blue}{\\text{blue}}\\) term is independent of \\(f\\). Therefore, to minimize \\(R(f,P)\\), it is sufficient to minimize the \\(\\color{green}{\\text{green}}\\) term, and the minimizer \\(f^*(x) = E(y|x)\\).</p>"},{"location":"notes/lecture_notes/stat541_week2/#geometric-illustration","title":"Geometric Illustration","text":"<p>Pythogorean Theorem of \\(R(f,P)\\):</p>"}]}
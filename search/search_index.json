{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-my-homepage","title":"Welcome to My Homepage!","text":""},{"location":"#my-name-is-haoxin-sang-hollis","title":"My name is Haoxin Sang (Hollis).","text":"<p>I'm excited to share my journey as a PhD student in CS at the University of Alberta, under the guidance of Xiaoqi Tan. </p> <p>Here, you'll find insights into my research, experiences as an international student, and reflections on various topics in Computer Science. (1)</p> <ol> <li>My content is written in English and, occasionally, in Chinese. I'm happy to exchange ideas in either language.</li> </ol>"},{"location":"#id-love-to-hear-your-thoughts-suggestions-or-just-have-a-chat-feel-free-to-connect-with-me-at-or-on","title":"I\u2019d love to hear your thoughts, suggestions, or just have a chat! Feel free to connect with me at  or on  .","text":""},{"location":"academic_profile/","title":"Academic Background","text":""},{"location":"academic_profile/#education","title":"Education","text":""},{"location":"academic_profile/#university-of-chinese-academy-of-sciences","title":"University of Chinese Academy of Sciences","text":"<pre><code>- M.S. in Fundamental Mathematics     Sep 2021 -- Jul 2024\n</code></pre>"},{"location":"academic_profile/#harbin-institute-of-technology","title":"Harbin Institute of Technology","text":"<pre><code>- B.S. in Mathematics and Applied Mathematics     Sep 2017 -- Jul 2021\n</code></pre>"},{"location":"academic_profile/#research-interest","title":"Research Interest","text":"<p>My research interests include online optimization and Riemannian optimization. I focus on online matching problems inspired by real-world applications such as real-time advertising, resource allocation in cloud computing, and matching users in ride-sharing platforms. Within online matching, I am particularly intrigued by stochastic models, including stochastic rewards and the Philosopher Inequalities.</p> <p>Additionally, I am passionate about Riemannian Optimization and learning approaches. This field leverages the geometric properties of Riemannian manifolds to develop algorithms that address challenges associated with non-Euclidean data.</p>"},{"location":"academic_profile/#publication","title":"Publication","text":"<ul> <li>Haoxin Sang, Yingyi Wu, Some analytic results and applications     in extremal Hermitian metrics*[J]. Journal of University of     Chinese Academy of Sciences, DOI:     10.7523/j.ucas.2023.091.</li> </ul>"},{"location":"academic_profile/#teaching-experience","title":"Teaching Experience","text":""},{"location":"academic_profile/#university-of-chinese-academy-of-sciences_1","title":"University of Chinese Academy of Sciences","text":"<pre><code>-   Teaching assistant for Differential Manifolds       Fall 2023   \n-   Teaching assistant for Calculus II, B       Spring 2023 \n-   Teaching assistant for Calculus I, B        Fall 2022\n</code></pre>"},{"location":"notes/naming_conventions/","title":"Naming Conventions in Python","text":"<p>Naming conventions are an important part of writing clean and readable code. In Python, the PEP 8 style guide provides recommendations on naming conventions. Here\u2019s a summary of the conventions along with special cases and additional tips:</p>"},{"location":"notes/naming_conventions/#naming-conventions","title":"Naming Conventions","text":"<ol> <li> <p>Variables:</p> </li> <li> <p>Use <code>snake_case</code> for variable names.</p> </li> <li>Example: <code>my_variable</code>, <code>total_count</code>, <code>is_valid</code>.</li> <li> <p>Functions:</p> </li> <li> <p>Use <code>snake_case</code> for function names.</p> </li> <li>Example: <code>def calculate_total()</code>, <code>def get_user_name()</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> (also known as <code>CamelCase</code>).</p> </li> <li>Example: <code>class MyClass</code>, <code>class UserProfile</code>.</li> <li> <p>Modules:</p> </li> <li> <p>Use <code>snake_case</code> for module names (file names).</p> </li> <li>Example: <code>import my_module</code>, <code>from user_profile import get_user_name</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>PI = 3.14159</code>, <code>MAX_CONNECTIONS = 10</code>.</li> </ol>"},{"location":"notes/naming_conventions/#special-cases","title":"Special Cases","text":"<ol> <li> <p>Abbreviations:</p> </li> <li> <p>Treat abbreviations as normal words and follow the same naming convention.</p> </li> <li>Variables and functions: <code>html_parser</code>, <code>parse_html</code>.</li> <li>Classes: <code>HtmlParser</code>.</li> <li> <p>Dashes (<code>-</code>):</p> </li> <li> <p>Dashes are not allowed in Python identifiers. Use underscores instead.</p> </li> <li>Example: <code>short_term</code> instead of <code>short-term</code>.</li> <li> <p>Names:</p> </li> <li> <p>Follow the same naming conventions for names. Treat them as any other word.</p> </li> <li>Example: <code>jack_smith</code> for a variable or function, <code>class JackSmith</code> for a class.</li> <li> <p>Numbers:</p> </li> <li> <p>Numbers can be included, but they should not start with a number.</p> </li> <li>Variables and functions: <code>user_123</code>, <code>find_25th_element</code>.</li> <li>Classes: <code>User123</code>, <code>Element25th</code>.</li> </ol>"},{"location":"notes/naming_conventions/#additional-tips","title":"Additional Tips","text":"<ol> <li> <p>Descriptive Names:</p> </li> <li> <p>Choose names that clearly describe the purpose of the variable, function, or class.</p> </li> <li>Avoid single-character names except for counters or iterators (e.g., <code>i</code>, <code>j</code>, <code>k</code>).</li> <li> <p>Avoid Ambiguity:</p> </li> <li> <p>Ensure that names are not ambiguous and convey the intended meaning.</p> </li> <li>Example: <code>calculate_total_price</code> is more descriptive than <code>calculate</code>.</li> <li> <p>Consistency:</p> </li> <li> <p>Be consistent with your naming conventions throughout your codebase.</p> </li> <li>Stick to the same style to improve readability and maintainability.</li> <li> <p>Avoid Using Reserved Keywords:</p> </li> <li> <p>Do not use Python reserved keywords as names for variables, functions, or classes.</p> </li> <li>Example: <code>class</code>, <code>def</code>, <code>return</code>, etc.</li> </ol>"},{"location":"notes/naming_conventions/#summary","title":"Summary","text":"<p>Here's a summary of the recommended naming conventions:</p> <ul> <li>Variables: <code>snake_case</code></li> <li>Functions: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Modules: <code>snake_case</code></li> <li>Constants: <code>UPPER_CASE</code></li> </ul>"},{"location":"notes/naming_conventions/#examples","title":"Examples","text":"<pre><code># Variables\nuser_name = \"JohnDoe\"\ntotal_count = 10\n\n# Functions\ndef calculate_total(price, tax):\n    return price + tax\n\ndef get_user_name():\n    return \"John Doe\"\n\n# Classes\nclass UserProfile:\n    def __init__(self, name):\n        self.name = name\n\n# Constants\nMAX_CONNECTIONS = 100\nPI = 3.14159\n\n# Modules (file names)\n# user_profile.py\n# html_parser.py\n\n# Example with abbreviations, names, and numbers\nhtml_parser = HtmlParser()\njack_smith = \"Jack Smith\"\nuser_123 = User123()\nfind_25th_element = find_25th_element()\n</code></pre>"},{"location":"notes/naming_conventions/#underscores","title":"Underscores","text":"<p>Underscores are used in Python for various special naming conventions. Understanding when and how to use underscores is important for writing idiomatic Python code. Here's a detailed explanation of the different uses of underscores in Python:</p>"},{"location":"notes/naming_conventions/#single-leading-underscore-_var","title":"Single Leading Underscore <code>_var</code>","text":"<ul> <li>Purpose: Indicates a weak \"internal use\" indicator. This is a convention to tell other programmers that the variable or method is intended for internal use. It does not prevent access but suggests that it should be treated as a non-public part of the API.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self._internal_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-trailing-underscore-var_","title":"Single Trailing Underscore <code>var_</code>","text":"<ul> <li>Purpose: Used to avoid conflicts with Python keywords or built-in names.</li> <li>Example: <pre><code>def function_(parameter):\n    return parameter + 1\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-underscore-__var","title":"Double Leading Underscore <code>__var</code>","text":"<ul> <li>Purpose: Triggers name mangling, where the interpreter changes the name of the variable in a way that makes it harder to create subclasses that accidentally override the private attributes and methods. This is used to avoid name conflicts in subclasses.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        self.__private_variable = 42\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#double-leading-and-trailing-underscore-var","title":"Double Leading and Trailing Underscore <code>__var__</code>","text":"<ul> <li>Purpose: Indicates special methods (also known as \"magic methods\" or \"dunder methods\") that have special meaning in Python. These are predefined methods used to perform operator overloading, object creation, and other fundamental behaviors.</li> <li>Example: <pre><code>class MyClass:\n    def __init__(self):\n        pass\n\n    def __str__(self):\n        return \"MyClass instance\"\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#single-underscore-_","title":"Single Underscore <code>_</code>","text":"<ul> <li>Purpose: Used as a throwaway variable name. This is a convention for variables that are temporary or insignificant.</li> <li>Example: <pre><code>for _ in range(5):\n    print(\"Hello, World!\")\n</code></pre></li> </ul>"},{"location":"notes/naming_conventions/#references","title":"References","text":"<ol> <li> <p>PEP 8 - Style Guide for Python Code:</p> </li> <li> <p>The official Python style guide, PEP 8, covers naming conventions, formatting, and best practices for writing Python code.</p> </li> <li>Link: PEP 8 - Style Guide for Python Code</li> <li> <p>Python Documentation:</p> </li> <li> <p>The official Python documentation provides guidelines on naming conventions and special methods (also known as magic methods or dunder methods).</p> </li> <li>Link: Python Documentation</li> </ol>"},{"location":"notes/naming_conventions/#specific-sections-in-pep-8","title":"Specific Sections in PEP 8","text":"<ol> <li> <p>Naming Conventions:</p> </li> <li> <p>PEP 8 includes a section on naming conventions that describes how to name variables, functions, classes, constants, modules, and packages.</p> </li> <li>Link: PEP 8 - Naming Conventions</li> <li> <p>Method Names and Instance Variables:</p> </li> <li> <p>This section discusses conventions for naming methods and instance variables, including the use of single and double underscores.</p> </li> <li>Link: PEP 8 - Method Names and Instance Variables</li> <li> <p>Public and Internal Interfaces:</p> </li> <li> <p>Guidelines on how to distinguish between public and internal interfaces using naming conventions.</p> </li> <li>Link: PEP 8 - Public and Internal Interfaces</li> </ol>"},{"location":"notes/naming_conventions/#summary-of-naming-conventions-from-pep-8","title":"Summary of Naming Conventions from PEP 8","text":"<ul> <li> <p>Variables and Functions:</p> </li> <li> <p>Use <code>snake_case</code> for variables and function names.</p> </li> <li>Example: <code>my_variable</code>, <code>calculate_total</code>.</li> <li> <p>Classes:</p> </li> <li> <p>Use <code>PascalCase</code> for class names.</p> </li> <li>Example: <code>MyClass</code>, <code>UserProfile</code>.</li> <li> <p>Modules and Packages:</p> </li> <li> <p>Use <code>snake_case</code> for module and package names.</p> </li> <li>Example: <code>my_module</code>, <code>user_profile</code>.</li> <li> <p>Constants:</p> </li> <li> <p>Use <code>UPPER_CASE</code> for constants.</p> </li> <li>Example: <code>MAX_CONNECTIONS</code>, <code>PI</code>.</li> <li> <p>Private Variables and Methods:</p> </li> <li> <p>Use a single leading underscore <code>_</code> for weak internal use.</p> </li> <li>Example: <code>_internal_variable</code>, <code>_internal_method</code>.</li> <li> <p>Name Mangling:</p> </li> <li> <p>Use double leading underscores <code>__</code> to invoke name mangling.</p> </li> <li>Example: <code>__private_variable</code>, <code>__private_method</code>.</li> <li> <p>Special Methods:</p> </li> <li> <p>Use double leading and trailing underscores <code>__</code> for special methods.</p> </li> <li>Example: <code>__init__</code>, <code>__str__</code>.</li> <li> <p>Throwaway Variables:</p> </li> <li> <p>Use a single underscore <code>_</code> for throwaway variables.</p> </li> <li>Example: <code>for _ in range(10):</code>.</li> </ul>"},{"location":"notes/r_objects/","title":"R Objects","text":"<p>In R, every piece of data is stored as an object. These objects come in different forms depending on the structure and type of the data. Understanding these forms (or \"classes\") is essential because different functions require or produce specific object types.</p>"},{"location":"notes/r_objects/#common-object-types","title":"Common Object Types","text":""},{"location":"notes/r_objects/#1-vectors","title":"1. Vectors","text":"<p>Vectors are the simplest R objects. They\u2019re one-dimensional arrays that hold elements of the same type (e.g., all numbers, all characters, or all logical values).</p> <ul> <li>Examples: <pre><code>numeric_vector &lt;- c(1, 2, 3)\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\nlogical_vector &lt;- c(TRUE, FALSE, TRUE)\n</code></pre></li> <li>Use Case:   Vectors form the building blocks of more complex data structures.</li> </ul>"},{"location":"notes/r_objects/#2-matrices","title":"2. Matrices","text":"<p>Matrices are two-dimensional arrays with rows and columns where every element is of the same type.</p> <ul> <li>Examples: <pre><code>my_matrix &lt;- matrix(1:9, nrow = 3)  # 3 rows, 3 columns\n</code></pre></li> <li>Use Case:   Ideal for mathematical computations where the data is homogeneous (all numbers, for instance).</li> </ul>"},{"location":"notes/r_objects/#3-data-frames","title":"3. Data Frames","text":"<p>Data frames are like spreadsheets or tables where each column can be a different type (numeric, character, factor, etc.).</p> <ul> <li>Examples: <pre><code>my_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 35),\n  Married = c(TRUE, FALSE, TRUE)\n)\n</code></pre></li> <li>Use Case:   Widely used for storing datasets in statistics and data analysis. They allow you to mix different data types by column.</li> </ul>"},{"location":"notes/r_objects/#4-lists","title":"4. Lists","text":"<p>Lists are very flexible containers that can hold elements of different types and even other lists. Each element of a list can be a vector, matrix, data frame, or another list.</p> <ul> <li>Examples: <pre><code>my_list &lt;- list(\n  numbers = 1:5,\n  letters = c(\"a\", \"b\", \"c\"),\n  data = data.frame(x = 1:3, y = c(\"A\", \"B\", \"C\"))\n)\n</code></pre></li> <li> <p>Use Case:   Great for bundling diverse outputs, such as results from a complex model.</p> </li> <li> <p>Compare with Data Frames:</p> <ol> <li>Structure: Data frames are inherently 2-dimensional (like tables), whereas lists can be one-dimensional collections of arbitrary objects.</li> <li>Uniformity: Data frames require each column to be of equal length, but lists can contain elements of varying lengths and types.</li> <li>Usage: Use data frames when you need a structured table for analysis. Use lists when you need a flexible container for storing assorted objects.</li> </ol> </li> </ul>"},{"location":"notes/r_objects/#5-factors","title":"5. Factors","text":"<p>Factors are used for categorical data. They store data as a set of discrete levels, which is useful for statistical modeling and plotting.</p> <ul> <li>Examples: <pre><code>gender &lt;- factor(c(\"male\", \"female\", \"female\", \"male\"))\n</code></pre></li> <li>Use Case:   Helps R understand that a variable is categorical, which influences how functions like modeling or plotting treat the variable.</li> </ul>"},{"location":"notes/r_objects/#conversion-functions-the-as-family","title":"Conversion Functions: The \"as.\" Family","text":"<p>R provides a variety of functions that start with <code>as.</code> to convert or \"coerce\" objects from one type to another. Here are some key ones:</p> <ul> <li> <p><code>as.vector(x)</code>   Ensures that <code>x</code> is a vector.</p> </li> <li> <p><code>as.matrix(x)</code>   Converts an object (like a data frame or vector) into a matrix. This is useful when you need a 2D, homogeneous data structure.</p> </li> <li> <p><code>as.data.frame(x)</code>   Converts an object into a data frame, which is handy when you need tabular data with potentially mixed data types.</p> </li> <li> <p><code>as.list(x)</code>   Converts <code>x</code> into a list.</p> </li> <li> <p><code>as.numeric(x)</code>   Converts an object to a numeric vector. This is especially important when numbers are stored as characters.</p> </li> <li> <p><code>as.character(x)</code>   Converts an object to a character vector.</p> </li> <li> <p><code>as.logical(x)</code>   Converts data to logical values (TRUE/FALSE).</p> </li> <li> <p><code>as.factor(x)</code>   Converts a vector into a factor for categorical data.</p> </li> </ul> <p>These functions help ensure that your data is in the correct format for analysis. For example, when using modeling functions like <code>glmnet</code>, you might need to convert your predictors to a matrix with <code>as.matrix()</code>.</p>"},{"location":"notes/comparison_thm/comp_thm/","title":"Comparison Theorems in Riemannian Geometry","text":"<ul> <li>Jacobi field and conjugate points</li> <li>Rauch Comparison Theorem</li> <li>Morse index form and proof of Rauch comparison theorem</li> </ul>"},{"location":"notes/comparison_thm/comp_thm_app/","title":"Comparison Theorems in Riemannian Geometry and Applications","text":"<p>This note hasn't been carefully reviewed yet. </p> <ul> <li>Applications of Rauch Comparison Theorem</li> <li>Hessian Comparison Theorem and its application</li> <li>Toponogov\u2019s Theorem</li> </ul> <p></p>"},{"location":"notes/comparison_thm/preliminaries/","title":"Preliminaries to Comparison Theorems","text":"<ul> <li>Notation</li> <li>Induced covariant derivatives</li> </ul>"},{"location":"notes/hermitian_metric/readme/","title":"Hermitian Metric","text":"<ul> <li>The almost complex structure on vector spaces</li> <li>The fundamental form on vector spaces</li> <li>Complex and almost complex manifold</li> <li>Hermitian structure</li> </ul>"},{"location":"notes/laplacian_kahler_mfd/readme/","title":"The Laplacian Operator on Kahler Manifolds","text":""},{"location":"notes/lecture_notes/math515_intro/","title":"Introduction","text":"<ul> <li>Mathematical Finance I (MATH 515, Fall 2025)</li> <li>Instructor: Tahir Choulli</li> </ul>"},{"location":"notes/lecture_notes/math515_intro/#course-description","title":"Course Description","text":"<p>This course gives an introduction to Mathematical Finance. We explain how martingale and other stochastic analysis tools are tailor made for addressing problems in Finance/Financial Economics. This exact fit between Stochastic and Finance/Financial Economics is illustrated on the easiest market models such as discrete and discrete-time market models. Thus, this course is also suitable for students from Math and Stats (with almost all branches), Finance, Management Sciences, Economics, Engineering, and Engineering Management. In this course, we will attempt to cover the following topics:</p> <ul> <li>Probability tools and their financial role/interpretations: a review Conditional probabilities and expectations. Filtration, adapted and predictable pro-cesses. Martingales, submartingales and supermartingales in discrete time. Doob-Meyer decomposition for supermartingales.</li> <li>Discrete-Time financial models: (Single period and multi-periods models) Model specifications, Arbitrage, completeness and other economic considerations. Self-financing property, value and gain processes. Valuation of contingent claims.</li> <li>Binomial model: Model specifications. Perfect hedging.</li> <li>Utility functions and consumption/investment problems.</li> <li>Incomplete markets: Imperfect hedging like mean-variance or quantile hedging.</li> <li>American options, futures and forward contracts.</li> <li>Transition to the continuous-time framework (if time allows it).</li> </ul>"},{"location":"notes/lecture_notes/math515_intro/#textbooks","title":"Textbooks","text":"<ul> <li>Introduction to Mathematical Finance. Discrete Time Models by Stanley Pliska, Black-well, 1997</li> <li>Stochastic Calculus for Finance I: The Binomial Asset Pricing Model by Steven Shreve, Springer, 2004.</li> </ul>"},{"location":"notes/lecture_notes/math515_intro/#concepts-and-references","title":"Concepts and References","text":""},{"location":"notes/lecture_notes/math515_intro/#week-1-2","title":"Week 1-2","text":"<p>Concepts:  Probability space (\\(\\sigma\\)-algebra, \\(\\mathcal{F}\\)-measurable), (conditional) expectation, stochastic process. </p>"},{"location":"notes/lecture_notes/math515_week1/","title":"Week 1-2","text":""},{"location":"notes/lecture_notes/cmput501/intro/","title":"Introduction","text":"<ul> <li>Advanced Algorithms (CMPUT 501, Fall 2025) course website. </li> <li>Instructor: Mohammad R. Salavatipour</li> </ul>"},{"location":"notes/lecture_notes/cmput501/intro/#course-description","title":"Course Description","text":"<p>This is a new course that will cover topics in Advanced Algorithms in Theoretical Computer Science (TCS),  targeted for senior undergraduate and graduate students interested in TCS.  The tentative list of topics to be covered in this course are listed below. We cover some classic topics and more recent advances with applications in other areas: streaming and sketching algorithms for big data, online and randomized algorithms, and approximation algorithms.</p> <p>Tentative topics (could change later):</p> <ul> <li> <p>Module 1: Classics (Greedy and Dynamic Programming)     Stable matching, Interval scheduling,     Weighted Interval Scheduling, Segmented least Squares,     Advanced DP     Minimum Spanning Trees and Arborescences</p> </li> <li> <p>Module 2: Randomized Algorithms     Simple deviation bounds, Randomized Min-Cut     Chernoff bound, Hypercube routing,     Randomized load balancing, Hashing     Balls and bins, power of two choices,     Random Walks in Graphs</p> </li> <li> <p>Module 3: Linear Programming and Combinatorial Optimization     Integer/Linear Programming, Duality, Integrality gap     Matchings, matching polytope</p> </li> <li> <p>Module 4: Approximation Algorithms     Vertex/Set cover, LP rounding     Knapsack, Bin Packing     Max-Sat     Max-Cut</p> </li> <li> <p>Module 5: Streaming and Sketching     Probabilistic Counting     Frequency moments, Distinct elements estimation     Sketching</p> </li> <li> <p>Module 6: Online Algorithms, Learning from experts     Online algorithms, Ski rental, paging     Secretary problem,     Learning from experts, Multiplicative Weight Update</p> </li> </ul> <p>By the end of this course you should have a basic knowledge of some of the basic techniques used in TCS in design and analysis of algorithms. You should also learn some tools and tricks to tackle problems that might arise in different settings (such as online setting, streaming setting, or interactable/hard to solve optimization problems).</p>"},{"location":"notes/lecture_notes/cmput501/intro/#concepts","title":"Concepts","text":""},{"location":"notes/lecture_notes/cmput501/intro/#greedy-dynamic-programming","title":"Greedy, Dynamic Programming","text":"<ul> <li> <p>Lecture 1: Stable Matching, Interval Scheduling, Minimizing lateness, Weighted Interval Scheduling</p> <p>Also see sections 1.1, 4.1, 4.2, 6.1 (KT), and slides for Chapter 1, 4 and 6 by (KW)</p> </li> <li> <p>Lecture 2: Segmented Least Square, Sequence Alignments, BST</p> <p>Also see sections 6.3, 6.6, 6.6 in (KT), and slides for Chapter 6 by (KW), extra notes for Advanced algorithms by (JE), and this survey paper.</p> </li> <li> <p>Lecture 3: Advanced DP: Saving time using monotonicity, SMWAK</p> <p>Extra notes for Advanced algorithms by (JE), and this survey paper.</p> </li> </ul>"},{"location":"notes/lecture_notes/cmput501/intro/#minimum-spanning-tree-minimum-arborescence","title":"Minimum Spanning Tree, Minimum Arborescence:","text":"<ul> <li> <p>Lecture 4: Minimum Spanning Tree (MST), Fredman-Tarjan Algorithm</p> <p>Also see 1.1-1.3 from (AG) combined notes and Lec 6 by (SA).</p> </li> <li> <p>Lecture 5: MST in linear time, Minimum Arborescence</p> <p>Also 1.4-1.5 from (AG) and Lec 7 by (SA); and 4.9 from (KT).</p> </li> </ul>"},{"location":"notes/lecture_notes/cmput501/intro/#randomized-algrithms","title":"Randomized Algrithms:","text":"<ul> <li> <p>Lecture 6: Introduction, simple deviation bounds, randomized min-cut</p> <p>Also, lecture 1-3 from (RA).</p> </li> <li> <p>Lecture 7: Chernoff bound, Hypercube routing</p> <p>Also Lecture 5 from (RA), and Chapter 10.3 and 10.5 by (AG), and 4.1-4.2 from (MR)</p> </li> <li> <p>Lecture 8: Balls and Bins, power of two choices</p> <p>Also Lecture 7 from (RA), Lec 5 from (SA), GuptaS21 Lec6</p> </li> <li> <p>Lecture 9: Randomized load balancing, Hashing</p> <p>Also lecture 16 from (RA), Lec 3 from (SA) and, notes 5 from (JE), and these notes.</p> </li> <li> <p>Lecture 10: Random Walks, resistence graph</p> <p>Also, Lecturse 12 and 13 from (RA), Chapter 6 from (MR), Lec 23 from (SA)</p> </li> <li> <p>Lecture 11: Finger printing, Polynomial identity testing</p> <p>Also Lecture 13 from (RA), Lecture 16 from (SA) and Chapter 7 from (MR), and Chapter 8 from (AG)</p> </li> </ul>"},{"location":"notes/lecture_notes/cmput501/intro/#reference","title":"Reference","text":"<p>The following is a list of most commonly referred to references:</p> <ul> <li> <p>(KT) J. Kleinberg and E. Tardos, Algorithm Design, 2006 and (KW) Lecture slides by Kevin Wayne for this book.</p> </li> <li> <p>(JE) J. Erickson, Algorithms Textbook, available (free).</p> </li> <li> <p>(MR) R. Motwani and P. Raghavan, Randomized Algorithms (free access), Cambridge University Press, 2013</p> </li> <li> <p>(V) V. Vazirani, Approximation Algorithms, available free (also from authors here), Springer-Verlag, Berlin, 2001.</p> </li> <li> <p>(WS) D. Willamson and D. Shmoys, The Design of Approximation Algorithms, (free download), Cambridge University Press, 2011</p> </li> <li> <p>A. Blum, J. Hopcroft, and R. Kannan, Foundations of Data Science.</p> </li> </ul>"},{"location":"notes/lecture_notes/cmput501/lecture1/","title":"Lecture 1","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture1/#stable-matching-may-not-be-unique-section-12","title":"Stable Matching May Not Be Unique (Section 1.2)","text":"<p>In the stable matching problem, there are instances of stable matching with \\(n\\) doctors and \\(n\\) hospitals where the number of stable matchings (solutions) can be exponential in \\(n\\).(1)</p> <ol> <li>Consider the following example with \\(n=2\\). The preference lists are as follows: \\(d_1\\): \\(h_1 \\succ h_2\\), \\(d_2\\): \\(h_1 \\succ h_2\\); \\(h_1\\): \\(d_1 \\succ d_2\\), \\(h_2\\): \\(d_1 \\succ d_2\\). There are two stable matchings: \\(\\{(d_1,h_1),(d_2,h_2)\\}\\) and \\(\\{(d_1,h_2),(d_2,h_1)\\}\\). By combining multiple such instances, we can construct instances with \\(n\\) doctors and \\(n\\) hospitals that have \\(2^{n/2}\\) stable matchings.</li> </ol>"},{"location":"notes/lecture_notes/cmput501/lecture1/#minimize-the-number-of-machines-section-131","title":"Minimize the Number of Machines (Section 1.3.1)","text":"<p>What if we consider scheduling jobs on two machines? Consider the following greedy algorithm:</p> <ul> <li>Sort the jobs in increasing order of their finishing times.</li> <li>For each job in this order, assign it to an available machine. If both machines are available, assign it to the tighter loaded machine (i.e., the machine with the larger total assigned processing time so far).</li> <li>If no machine is available, discard the job.</li> </ul> <p>How to design a greedy algorithm that minimizes the number of machines used to schedule all jobs? Consider the greedy algorithm with jobs sorted in increasing order of their starting times.</p>"},{"location":"notes/lecture_notes/cmput501/lecture10/","title":"Lecture 10","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture10/#commute-time-by-effective-resistance-section-1021","title":"Commute Time by Effective Resistance (Section 10.2.1)","text":"<p>When we consider the effective resistance between two nodes \\(u\\) and \\(v\\) in a graph \\(G\\), we can think of setting each edge in \\(G\\) as a resistor with resistance 1 ohm and then connecting a battery with voltage 1 volt between nodes \\(u\\) and \\(v\\). Let the resulting current (sum of currents on edges incident to \\(u\\)) that flows from \\(u\\) to \\(v\\) be \\(I_{uv}\\). Then the effective resistance \\(R_{uv}\\) is simply given by Ohm's law: \\(R_{uv} = 1/I_{uv}\\).</p> <p>Interpretation: The effective resistance \\(R_{uv}\\) can be interpreted as a measure of connectivity between nodes \\(u\\) and \\(v\\). A smaller \\(R_{uv}\\) indicates more parallel paths (more connectivity) between \\(u\\) and \\(v\\), while a larger \\(R_{uv}\\) indicates fewer paths (less connectivity).</p> <p>The effective resistance of an edge \\((u,v)\\) is defined as the effective resistance between nodes \\(u\\) and \\(v\\). The effective resistance of each edge is the probability that the edge is chosen in a uniform spanning tree distribution. (1)</p> <ol> <li>The probability is given by \\(Pr(e\\in T) = \\tau(G\\cdot e)/ \\tau(G)\\), where \\(\\tau(G)\\) is the number of spanning trees in graph \\(G\\), and \\(G\\cdot e\\) is the graph obtained by contracting edge \\(e\\) in \\(G\\). The ratio can be derived using Kirchoff's matrix-tree theorem.</li> </ol>"},{"location":"notes/lecture_notes/cmput501/lecture10/#cover-time-for-complete-graphs","title":"Cover Time for Complete Graphs","text":"<p>We can improve the bound in the lecture notes for the cover time of complete graphs.  This is equivalent to the coupon collector's problem (Section 6.5), where the expected time to collect all \\(n\\) coupons is \\(\\Theta(n\\log n)\\).</p>"},{"location":"notes/lecture_notes/cmput501/lecture11/","title":"Lecture 11","text":""},{"location":"notes/lecture_notes/cmput501/lecture12/","title":"Lecture 12","text":""},{"location":"notes/lecture_notes/cmput501/lecture13/","title":"Lecture 13","text":""},{"location":"notes/lecture_notes/cmput501/lecture14/","title":"Lecture 14","text":""},{"location":"notes/lecture_notes/cmput501/lecture15/","title":"Lecture 15","text":""},{"location":"notes/lecture_notes/cmput501/lecture16/","title":"Lecture 16","text":""},{"location":"notes/lecture_notes/cmput501/lecture17/","title":"Lecture 17","text":""},{"location":"notes/lecture_notes/cmput501/lecture18/","title":"Lecture 18","text":""},{"location":"notes/lecture_notes/cmput501/lecture19/","title":"Lecture 19","text":""},{"location":"notes/lecture_notes/cmput501/lecture2/","title":"Lecture 2","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture2/#notation-explanation-section-25","title":"Notation Explanation (Section 2.5)","text":"<p>Dummy keys: Queried keys that are not in the set being stored in the data structure. So, to eventually find out that the a key is not in the set, we need to reach the imaginary node (leaf), and therefore cost us one more computation step.</p>"},{"location":"notes/lecture_notes/cmput501/lecture20/","title":"Lecture 20","text":""},{"location":"notes/lecture_notes/cmput501/lecture21/","title":"Lecture 21","text":""},{"location":"notes/lecture_notes/cmput501/lecture22/","title":"Lecture 22","text":""},{"location":"notes/lecture_notes/cmput501/lecture23/","title":"Lecture 23","text":""},{"location":"notes/lecture_notes/cmput501/lecture24/","title":"Lecture 24","text":""},{"location":"notes/lecture_notes/cmput501/lecture25/","title":"Lecture 25","text":""},{"location":"notes/lecture_notes/cmput501/lecture3/","title":"Lecture 3","text":""},{"location":"notes/lecture_notes/cmput501/lecture4/","title":"Lecture 4","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture4/#improved-mst-algorithm-section-44","title":"Improved MST Algorithm (Section 4.4)","text":"<p>Compared to the priority queue, the Fibonacci heap has a better amortized time for the insert and decrease-key operations, which is \\(O(1)\\) instead of \\(O(\\log V)\\). Recall that in Prim's algorithm, we need to perform \\(O(E)\\) decrease-key operations and \\(O(V)\\) insert operations. Therefore, by using Fibonacci heaps, the overall time complexity of Prim's algorithm can be improved to \\(O(E + V \\log V)\\).</p>"},{"location":"notes/lecture_notes/cmput501/lecture5/","title":"Lecture 5","text":""},{"location":"notes/lecture_notes/cmput501/lecture6/","title":"Lecture 6","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture6/#max-flow-min-cut-problem-section-64","title":"Max-flow Min-cut Problem (Section 6.4)","text":"<p>The max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i.e. the smallest total weight of the edges which if removed would disconnect the source from the sink.</p> <p>To find the minimum cut in a graph, it suffices to run max-flow algorithms with a fixed source and all possible sinks (i.e. run max-flow for \\(|V|-1\\) times), and then take the minimum over all computed cuts.</p> <p>Correctness: Let the true global minimum cut be \\((S^*,T^*)\\) such that \\(S^*\\cup T^* = V\\) and \\(S^*\\cap T^* = \\emptyset\\). Suppose the fixed source is \\(s_0\\), and assume w.l.o.g. that \\(s_0\\in S^*\\).  Since \\(T^*\\) is non-empty, there exists at least one vertex \\(t_0\\in T^*\\).  When we run the max-flow algorithm with source \\(s_0\\) and sink \\(t_0\\), the min-cut found by the algorithm must be equal to the true global minimum cut \\((S^*,T^*)\\).</p> <p>Count edges of subgraphs (Corollary 6.3): For an \\(n\\)-vertex graph with minimum cut of size \\(k\\), the number of edges is at least \\(\\frac{kn}{2}\\). This is because each vertex has degree at least \\(k\\) (to avoid a cut of size less than \\(k\\)).</p>"},{"location":"notes/lecture_notes/cmput501/lecture7/","title":"Lecture 7","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture7/#routing-on-hypercube-section-72","title":"Routing on Hypercube (Section 7.2)","text":"<p>In the problem, we may assume either an edge can carry only one packet at a time step, or two packets (in opposite directions) at a time step. This only differs by a factor of 2 in the running time analysis.</p> <p>Since we want a decentralized and efficient algorithm, in Algorithm 2, we sample a random intermediate destination for each packet, rather than a permutation routing which requires global knowledge of all packets' destinations.</p>"},{"location":"notes/lecture_notes/cmput501/lecture8/","title":"Lecture 8","text":""},{"location":"notes/lecture_notes/cmput501/lecture9/","title":"Lecture 9","text":"<p>Notes below are meant to supplement the scribed notes.</p>"},{"location":"notes/lecture_notes/cmput501/lecture9/#universal-hash-functions-section-92","title":"Universal Hash Functions (Section 9.2)","text":""},{"location":"notes/lecture_notes/cmput501/lecture9/#introduction","title":"Introduction","text":"<ul> <li>\\(\\mathcal{H}\\) is a family of deterministic hash functions mapping a universe \\(U\\) to the indices of a hash table \\(T\\) of size \\(n\\).</li> <li>We want to hash a subset \\(S \\subset U\\) (may be chosen adversarially) into \\(T\\).</li> <li>To get a good expected performance regardless of \\(S\\), for each set \\(S\\), we sample a function \\(h\\in\\mathcal{H}\\) uniformly at random. </li> <li>Use hash function \\(h\\) to hash elements in \\(S\\) into table \\(T\\).</li> </ul>"},{"location":"notes/lecture_notes/cmput501/lecture9/#interpretation-of-different-hash-function-properties","title":"Interpretation of Different Hash Function Properties","text":"<p>The universal hash function property (Definition 2) requires a family \\(\\mathcal{H}\\) to have an expected collision bound, even for adversarially chosen sets \\(S\\).  The (strongly) 2-universal hash function property (Definition 3) indicates for any distinct elements \\(x,y\\in U\\),</p> <p> \\(\\mathrm{Pr}_{h\\in\\mathcal{H}}[h(x) = a] = \\frac{1}{n}\\) for any \\(a\\in [n]\\),(1)</p> <ol> <li>By \\(\\mathrm{Pr}_{h\\in\\mathcal{H}}[h(x) = a] = \\sum_{b\\in [n]} \\mathrm{Pr}_{h\\in\\mathcal{H}}[h(x) = a \\wedge h(y)=b]\\).</li> </ol> <p> \\(h(x)\\) and \\(h(y)\\) are independent random variables when \\(h\\) is sampled uniformly from \\(\\mathcal{H}\\).(1)</p> <ol> <li>For any \\(a,b\\in [n]\\), \\(\\mathrm{Pr}[h(x) = a \\wedge h(y)=b]= \\frac{1}{n^2} = \\mathrm{Pr}[h(x) = a] \\cdot \\mathrm{Pr}[h(y) = b]\\). </li> </ol> <p>Note that pairwise independence DOES NOT imply mutual independence for three or more random variables.(1)</p> <ol> <li>For example, consider three random variables \\(X,Y,Z\\) such that each takes values in \\(\\{0,1\\}\\) with equal probability, and \\(Z = X \\oplus Y\\) (XOR operation). Then any pair of them are independent, but all three are not mutually independent since knowing any two determines the third.</li> </ol> <p>The inequality in the definition of the \\(k\\)-wise independent property (Definition 4) actually holds as an equality(1) for any distinct \\(x_1,x_2,\\dots,x_k\\in U\\) and any \\(\\alpha_1,\\alpha_2,\\dots,\\alpha_k\\in [n]\\). Therefore, the \\(k\\)-wise independent property generalizes the independence property in the 2-universal property to \\(k\\) elements, and the above two properties extend naturally to the \\(k\\)-wise independent property.(2)</p> <ol> <li> <p>Since \\(\\sum_{\\alpha_1,\\alpha_2,\\dots,\\alpha_k\\in [n]} \\mathrm{Pr}_{h\\in\\mathcal{H}}[h(x_1) = \\alpha_1 \\wedge \\dots \\wedge h(x_k) = \\alpha_k] = 1\\), we have each term must equal \\(\\frac{1}{n^k}\\). Some authors write \u201c\\(\\leq\\)\u201d out of habit or to emphasize a collision bound rather than strict independence. </p> </li> <li> <p>That is, each \\(h(x_i)\\) is uniformly distributed over \\([n]\\), and any subset of \\(\\{h(x_1),h(x_2),\\dots,h(x_k)\\}\\) are mutually independent random variables when \\(h\\) is sampled uniformly from \\(\\mathcal{H}\\).</p> </li> </ol>"},{"location":"notes/lecture_notes/cmput501/lecture9/#perfect-hashing-section-95","title":"Perfect Hashing (Section 9.5)","text":""},{"location":"notes/lecture_notes/cmput501/lecture9/#bound-on-the-sum-of-squared-bucket-sizes","title":"Bound on the Sum of Squared Bucket Sizes","text":"<p>Let \\(L_i\\) be the length of a chain at index \\(i\\), then by Markov's inequality, we have the total number of collisions is bounded by \\(N\\) with probability at least \\(1/2\\):</p> \\[ \\sum_{i=0}^{n-1} \\binom{L_i}{2} \\leq n. \\] <p>This implies, with probability at least \\(1/2\\),</p> \\[ \\sum_{i=0}^{n-1} L_i^2 = \\sum_{i=0}^{n-1} \\left(2\\binom{L_i}{2} + L_i\\right) = 2\\sum_{i=0}^{n-1} \\binom{L_i}{2} + \\sum_{i=0}^{n-1} L_i \\leq 2n + n = 3n. \\]"},{"location":"notes/lecture_notes/cmput501/lecture9/#bloom-filters-section-96","title":"Bloom Filters (Section 9.6)","text":"<p>Each time we add an item, we set \\(k\\) bits to 1 using \\(k\\) different hash functions, and the probability that a specific bit is set to 1 by one hash function for one item is \\(\\frac{1}{n}\\). Therefore, the probability that any individual bit is still 0 after adding \\(N\\) items is</p> \\[ \\left(1 - \\frac{1}{n}\\right)^{kN} \\approx e^{-\\frac{kN}{n}}. \\]"},{"location":"notes/lecture_notes/stat541/stat541_assignment1/","title":"Assignment 1","text":"<p>Key Takeaways</p> <ul> <li> <p>Problem 1(c): For classification problems, the oracle risk of the equal weight classifier is given by \\(R(f^*, P) = Pr( y \\neq c^*)\\), where \\(c^*\\) is the class that occurs with the largest probability. Hence, more unbalanced distributions result in easier classification problems (in the sense of the oracle risk).</p> </li> <li> <p>Problem 2(a): The risk of the oracle predictor for a regression problem (under squared error loss) is \\(E_x\\left(Var(y\\mid x)\\right)\\).</p> </li> <li> <p>Problem 3(a): To minimize the loss function, we let the gradient (with respect to \\(\\beta\\)) be 0 and get a least squares solution. It should also be checked that this solution is a minimum, not a maximum or saddle point. To do this, we look at the second order condition on the Hessian matrix of the objective function and find it a strictly positive definite matrix.</p> </li> <li> <p>Problem 4(a): The matrix \\(X X^{\\top}=V D^2 V^{\\top} \\in \\mathbb{R}^{n \\times n}\\) has \\(p\\) eigenvectors given by the \\(p\\) columns of \\(V\\) with corresponding eigenvalues \\(d_{11}^2, \\ldots, d_{p p}^2\\). However, we have to be careful as \\(X X^{\\top}\\) is a \\(n \\times n\\) symmetric matrix so it will have a total of \\(n \\geq k\\) eigenvectors and values, resulting in \\(n-k\\) additional eigenvectors from \\(V^{\\perp}\\) each with eigenvalue zero. </p> </li> <li> <p>Problem 5(e): Overall, for small data samples, the lower degree fits do better than the higher degree fits. As the sample size increases the prediction errors across all three models get smaller.</p> </li> </ul> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_assignment2/","title":"Assignment 2","text":"<p>Key Takeaways</p> <ul> <li> <p>Problem 1(e): When design matrix \\(X\\) has orthonormal columns, the best subset variable selection problem becomes easier -- we can minimize the AIC by including variable \\(i\\) if and only if \\(2 \\hat{\\sigma}^2 \\leq \\hat{\\beta}_{i}^2\\). Thus, in this special case minimizing the AIC is no harder computationally than finding \\(\\hat{\\boldsymbol{\\beta}}_{\\text {Full }}\\)!</p> </li> <li> <p>Problem 2(b): The \\(i\\) th (\\(i=1, \\dots, p\\)) largest eigenvalue of the ridge regression covariance matrix is always smaller than the \\(i\\) th largest eigenvalue of the OLS covariance matrix, which implies \\(\\operatorname{Var}\\left(\\mathbf{x}_*^{\\top} \\hat{\\boldsymbol{\\beta}}_{\\text {Ridge }} \\mid \\mathbf{X}, \\mathbf{x}_*\\right) \\leq \\operatorname{Var}\\left(\\mathbf{x}_*^{\\top} \\hat{\\boldsymbol{\\beta}}_{O L S} \\mid \\mathbf{X}, \\mathbf{x}_*\\right)\\) for every \\(\\mathbf{x}_*\\). </p> </li> <li> <p>Problem 2(c): For \\(\\lambda=0\\) we recover the OLS estimates which we know are unbiased. For \\(\\lambda \\rightarrow \\infty\\) we penalize the regression coefficients more and more and the ridge estimates converge to zero. The corresponding bias is then \\(\\mathbf{X} \\boldsymbol{\\beta}-\\mathbf{0}=\\mathbf{X} \\boldsymbol{\\beta}\\).</p> </li> <li> <p>Problem 2(d): By minimizing over each \\(\\beta_i\\), we get: The coefficient \\(\\hat{\\beta}_{L A S S O, i}=0\\) if and only if \\(\\left|\\left[\\mathbf{X}^{\\top} \\mathbf{Y}\\right]_i\\right| \\leq \\frac{\\lambda}{2}\\), meaning that the signal from the data \\(\\left[\\mathbf{X}^{\\top} \\mathbf{Y}\\right]_i\\) is not large enough relative to the penalty \\(\\lambda\\) to justify keeping this feature around. (Of course, a judicious choice of \\(\\lambda\\) is needed in order for this estimator to)</p> </li> </ul> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_assignment3/","title":"Assignment 3","text":"<p>Key Takeaways</p> <ul> <li> <p>Problem 3(c): Let \\(x^{(i')}\\) be the unique closest point to \\(x_*\\). As \\(\\lambda \\rightarrow 0^{+}\\), we get \\(\\hat{\\beta}_0\\left(x_*\\right) \\rightarrow x^{\\left(i^{\\prime}\\right)}\\). In other words, the kernel smoother converges to the 1-NN predictor. The 1-NN predictor is a very \"jumpy\" function and has high variance but will have low bias as it is able to approximate a wide variety of functions. As \\(\\lambda\\rightarrow \\infty\\), the predictor \\(\\hat{f}\\left(x_*\\right)=\\frac{1}{n} \\sum_{i=1}^n y^{(i)}\\) is a constant function (it is actually the \\(n\\)-NN predictor) with respect to \\(x_*\\) and so has low variance but has high bias. </p> </li> <li> <p>Problem 4(d): LDA variant is like a nearest neighbour search except that it only requires searching over the 10 mean images rather than the 60,000 images in the training data set. Overall, this algorithm can make predictions much more quickly than the 1-NN algorithm since it does not have to search over the entire dataset. (Note that we are timing how long it takes to make predictions, not fit the given models.) Likewise, while logistic regression takes little while to actually fit the model, it makes predictions on new data points very fast.</p> </li> </ul> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_assignment4/","title":"Assignment 4","text":"<p>Key Takeaways</p> <ul> <li> <p>Problem 1(b): A regression tree for a univariate feature space \\(\\mathcal{X} = \\mathbb{R}\\) is simply a piecewise constant function. Suppose we have determined that the splits for our regression tree and get the disjoint regions \\(R_s\\). We find that \\(\\hat{\\beta}_s\\) is the average of all the response values for all observations that have features contained in \\(R_s\\).</p> </li> <li> <p>Problem 2(a): A bagged prediction function \\(\\hat{f}\\) is a linear function of \\(\\boldsymbol{x}\\) with a regression coefficient \\(\\frac{1}{B} \\sum_{i=1}^B \\hat{\\boldsymbol{\\beta}}^{(i)}\\), i.e. the average of the regression coefficients of the fits on \\(\\mathcal{D}^{(i)}\\).</p> </li> <li> <p>Problem 2(d): The model averaging (functions of different degrees) regularizes the coefficients of the higher order polynomial terms by shrinking them toward zero.</p> </li> <li> <p>Problem 3(e): We see that the function plot for pH versus potassium (K) shows a trend reversal. (For the observed data as we increase potassium it seems that the pH also increases. However, in our GAM fit the fitted partial function is mostly decreasing as K increases.) A trend reversal of this nature does not necessarily indicate that the GAM fit is poor, but it does suggest that there is dependence or collinearity between the different features.</p> </li> </ul> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_assignment5/","title":"Assignment 5","text":"<p>Key Takeaways</p> <ul> <li> <p>Problem 1(b): K-means clustering (and any algorithm that utilizes means) are sensitive to observations that are outliers. A fix to this is to instead compute the geometric median. </p> </li> <li> <p>Problem 2(a-d): PC directions can also be interpreted as directions where the variance of the data is maximized.</p> </li> </ul> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/","title":"Introduction","text":"<ul> <li>Statistics for Learning (STAT 541, Winter 2025) course website. </li> <li>Instructor: Andrew McCormack</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#course-description","title":"Course Description","text":"<p>The course focuses on statistical learning techniques, in particular those of supervised classification, both from statistical (logistic regression, discriminant analysis, nearest neighbours, and others) and machine learning background (tree-based methods, neural networks, support vector machines), with the emphasis on decision-theoretic underpinnings and other statistical aspects, flexible model building (regularization with penalties), and algorithmic solutions. Selected methods of unsupervised classification (clustering) and some related regression methods are covered as well.</p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#concepts-and-references","title":"Concepts and References","text":""},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-1","title":"Week 1","text":"<p>Concepts:  Types of learning, supervised learning problem setup, regression, classification, loss, risk, oracle (Bayes) risk and predictors, conditional expectation, expected risk.</p> <p>References: A more in-depth treatment of the topics covered this week can be found in Chapter 2, Learning theory from first principles (1).</p> <ol> <li>F. Bach, Learning theory from first principles. in Adaptive computation and machine learning. Cambridge, Massachusetts: The MIT Press, 2024.</li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-2","title":"Week 2","text":"<p>Concepts:  Empirical risk minimization, bias-variance decomposition. </p> <p>References: For the bias-variance decomposition see Introduction to Statistical Learning with R (ISLR)(1) Section 2.2 and Elements of Statistical Learning (ESL)(2) Section 7.3. In particular, equation (7.9) of ESL is essentially the bias-variance decomposition we derived in class, except that we also took an expectation over \\(x_0\\). </p> <ol> <li>G. James, D. Witten, T. Hastie, and R. Tibshirani, An introduction to statistical learning: with applications in R, Second edition. in Springer texts in statistics. New York: Springer, 2021.</li> <li>T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. in Springer Series in Statistics. New York, NY: Springer New York, 2009. doi: 10.1007/978-0-387-84858-7.</li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-3","title":"Week 3","text":"<p>Concepts: Means and covariances of random vectors, orthogonal matrices, the singular value decomposition, the spectral decomposition, the linear regression model, ordinary least squares estimates of regression coefficients, bias and variance of the OLS estimators, prediction interval based on the t-distribution, feature transformations, regression with categorical predictors, interaction effects, overfitting and issues with including too many features, variable selection (AIC and BIC), forward or backward selection.  </p> <p>References: Section 3.3.1 of Izenman's Modern Multivariate Statistical Techniques (MMST) discusses means and covariances of random vectors; equations (3.92) and (3.93) are very important. MMST also discusses the full SVD in 3.2.6 for a short and wide matrix (take a transpose to get the full SVD for a tall and narrow matrix). The full SVD is almost the same as the thin SVD discussed in class except that the full SVD includes extra, unnecessary rows/columns in the constituent matrix factors. For the rest, see ISLR: 3.1-3.3, 7.1-7.3, 6.1, ESL: 3.1-3.3. </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-4","title":"Week 4","text":"<p>Concepts: Properties and the various optimization problem formulations of ridge regression and the LASSO, data splitting, K-fold cross-validation, leave-one-out cross-validation.</p> <p>References: ISLR: 6.2, 5.1, ESL: 3.4, 7.10.</p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-5","title":"Week 5","text":"<p>Concepts: Introduction to the logistic function and logistic regression. How to make predictions using a logistic regression model. Finding the beta coefficients in logistic regression via maximum likelihood estimation. </p> <p>References: ISLR: 4.1-4.3.  </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-6","title":"Week 6","text":"<p>Concepts: Linear discriminant analysis (LDA), quadratic discriminant analysis (QDA) and naive Bayes, classification boundaries for multi-class problems. </p> <p>References: ISLR: 4.4-4.5. </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-7-reading-week","title":"Week 7 \u2013 Reading Week","text":""},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-8","title":"Week 8","text":"<p>Concepts: Basis functions. Regression splines and counting the number of basis functions. Smoothing spline optimization problem. The smoothing matrix and effective degrees of freedom. Kernel smoothers. Boxcar, Gaussian, and Epanechnikov kernels. The effect of the bandwidth parameter.</p> <p>References: ISLR: 7.3-7.5, ESL: 5.2, 5.4, Have a look at equation (5.30) to see how smoothing splines can be applied to logistic regression. 6.1-6.2 (Note ISLR does not have any material on kernel smoothing). </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-9","title":"Week 9","text":"<p>Concepts: Local polynomial regression. Bias of KNN and kernel smoothing near boundaries and near maxima or minima. Smoothing methods for classification. Smoothing methods for higher dimensional features. The importance of standardizing features. The curse of dimensionality. Generalized additive models and the backfitting algorithm.  </p> <p>References: ISLR: 7.6-7.7, ESL: 6.1.1-6.1.2, 6.3, 9.1.</p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-10","title":"Week 10","text":"<p>Concepts: Classification and decision trees (CART). Impurity measures. How to choose how big of a tree to grow. Ensemble methods including model averaging, bagging, and random forests. </p> <p>References: ISLR: 8.1, 8.2.1, 8.2.2. ESL: 8.7-8.8, 9.2</p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-11-guest-lecture","title":"Week 11 - Guest Lecture","text":"<p>Concepts: Projection Pursuit Regression model: ridge functions, showing PPR model as a a universal approximator, and how to fit the PPR model. The study of neural networks \u2013 deep learning \u2013 is a enormous field. If you are interested in learning more, one canonical reference is this book. See also the other sections in chapter 10 of ISLR and chapter 11 of ESL. </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-12","title":"Week 12","text":"<p>Concepts: The idea behind clustering. The k-means algorithm: derivation of the iterations, scaling features, convergence of the algorithm, how to choose k. Hierarchical clustering:  dissimilarity measures between clusters, including complete, average, and single linkage, the dendrogram and how to interpret it, brief discussion on divisive clustering and how clustering can be extended to more exotic objects like DNA sequences. Gaussian mixture models (GMMs). The EM algorithm updates for GMMs and their interpretation. The (tautological) observation that generative models are able to generate, new, never before seen data. Brief mention on the flexibility of GMMs. </p> <p>References: ISLR 12.4, 14.4.1-14.3.6, 14.3.12. Neither ISLR or ESL has an extensive discussion on GMM. For this, see Section 9.2, Pattern Recognition and Machine Learning(1). </p> <ol> <li>By Christopher M. Bishop (2006).   </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-13","title":"Week 13","text":"<p>Concepts: Two perspective on PCA as variance maximization or distance minimization of projections. The PCA solution for the optimal projection of points onto an affine subspace. The principal component directions of the affine subspace and the principal component scores of the projected points.  How to use the PC scores in compression, visualization, and as input for supervised learning algorithms.  A discussion of principal components regression that uses the PC scores as input features in a linear regression. The link between the eigenvectors of the sample covariance matrix and the PC directions as well as the eigenvalues and the reconstruction error of PCA. The choice of the dimension k of the affine subspace via scree/elbow plots of the eigenvalues of the sample covariance. </p> <p>References: ISLR: 6.3.1, 12.2, ESL: 8.5.1, 14.3.7., 14.5.1. </p>"},{"location":"notes/lecture_notes/stat541/stat541_intro/#week-14-last-partial-week-of-class","title":"Week 14 (Last partial week of class)","text":"<p>Concepts: (Classical) Multidimensional scaling as a method for obtaining low dimensional embeddings of data into R^d using only a distance matrix. How to derive the embedding when the distance is assumed to be the standard Euclidean distance in R^p. Relationship between the MDS Gram matrix and the principal component scores. Scree plot of the eigenvalues of the Gram matrix for finding a reasonable dimension d of the embedding space. Curves, surfaces and higher-dimensional surfaces and the difference between intrinsic (shortest-path) and extrinsic distances on such manifolds. The k-NN graph as a way to approximate the manifold that a point cloud of data lies close to. Distances on the k-NN graph approximate the intrinsic distance of the underlying manifold. Effects of the choice of k in the k-NN graph, too large we lose the manifold structure, and too small we might end up with a disconnected graph. The isomap algorithm which first constructs a k-NN graph to obtain shortest-path distances within the graph and then subsequently runs MDS on these distances to get an embedding into R^d.       </p> <p>References: ESL has a little bit of material in Chapter 14 but it is not extensive. Instead, I recommend looking at Izenmann Section 13.6 for MDS. There are some nice illustrative examples in this section. For Isomap see Izenmann Section 16.6.3. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/","title":"Week 1","text":""},{"location":"notes/lecture_notes/stat541/stat541_week1/#requirement","title":"Requirement","text":"<p>Statistics: Multiple Regression, Bias-var Decomposition</p> <p>Calculus: Interated Intigration</p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#supervised-learning","title":"Supervised Learning","text":"<p>Suppose we have data pairs</p> \\[ (x^{(1)},y^{(1)}),\\dots, (x^{(n)},y^{(n)}) \\sim P \\quad(iid.) \\] <p>where \\(P\\) is some distribution.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#our-goal","title":"Our Goal","text":"<p>Find a function \\(f:\\mathcal{X}\\rightarrow \\mathcal{Y}\\) such that \\(f(x)\\approx y\\) when \\((x,y)\\sim P\\), where</p> <ul> <li>\\(x\\) is called: predictor variable / covariates / independent var. / inputs / features</li> <li>\\(y\\) is called: response var. / output var. / dependent var.</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#examples","title":"Examples:","text":"<ul> <li>\\(\\mathcal{Y} = \\mathbb{R}\\): Regression Problem</li> <li>\\(\\mathcal{Y}\\) is a finite set: Image Classification</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#learning-algorithm","title":"Learning Algorithm","text":"<p>To obtain an \\(f\\) we use the training data to output on Learning algorithm</p> \\[ \\phi_n: (\\mathcal{X}\\times \\mathcal{Y})^n \\longrightarrow \\mathcal{Y}^\\mathcal{X} \\] <p>where \\(\\mathcal{Y}^\\mathcal{X}\\) is the set of all functions from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\). Therefore,</p> \\[ \\hat{f} = \\phi_n(x^{(1)},y^{(1)},\\dots, x^{(n)},y^{(n)}) \\] <p>is a random function(1) determined by the data and the learning algorithm \\(\\phi_n\\). </p> <ol> <li>Note that a random function is a deterministic function. More precisely, a function of an arbitrary argument \\(t\\) (defined on the set \\(T\\) of its values, and taking numerical values or, more generally, values in a vector space) whose values are defined in terms of a certain experiment and may vary with the outcome of this experiment according to a given probability distribution.</li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#definition-loss-function","title":"Definition Loss function","text":"<p>Loss function \\(L: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0,+\\infty)\\) is to estimate the error of \\(f\\). How close \\(f(x)\\) is to \\(y\\) is gauged by \\(L(f(x),y)\\). Examples: squared error loss</p> \\[ L(f(x),y) = (y - f(x))^2,  \\] <p>and \\(0-1\\) loss</p> \\[ L(f(x),y) = I\\left(f(x) \\neq y\\right) = \\left\\{\\begin{matrix} 1,\\quad &amp; \\text{ if } f(x) \\neq y, \\\\ 0,\\quad &amp; \\text{ if } f(x) = y, \\end{matrix}\\right. \\] <p>where \\(I\\) is the indicator function. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#definition-risk-function","title":"Definition Risk function","text":"<ul> <li>Risk function (a.k.a. prediction error)</li> </ul> \\[ R(f,P) = E_{(x,y)}\\left(L(f(x),y)\\right) = \\int_{\\mathcal{X}\\times \\mathcal{Y}} L\\left(f(x),y\\right) p(x,y) \\,\\mathrm{d}x\\mathrm{d}y \\] <ul> <li>Oracle prediction error (a.k.a. Bayes Risk):</li> </ul> \\[ R^*(P) = \\inf_{f} R(f,P) \\] <ul> <li>Oracle predictor \\(f^*\\) satisfies</li> </ul> \\[ R(f^*,P) = R^*(P) \\]"},{"location":"notes/lecture_notes/stat541/stat541_week1/#compute-the-oracle-predictor","title":"Compute the Oracle Predictor","text":"<p>Compute \\(R^*(P)\\) for squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left((f(x) - y)^2\\right) \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} (y - f(x))^2 p(y|x) p(x) \\,\\mathrm{d}y \\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}p(x)\\left(\\int_{\\mathcal{Y}}(y - f(x))^2 p(y|x)\\,\\mathrm{d}y\\right)\\,\\mathrm{d}x \\end{aligned} \\] <p>For fixed \\(x\\), we minimize over the value of \\(f(x)\\), that is, it's suffice to set</p> \\[ f(x) = \\operatorname*{arg\\, min}\\limits_z \\int_{\\mathcal{Y}}(y - z)^2 p(y|x)\\,\\mathrm{d}y. \\] <p>It is equivalent to minimize:</p> \\[ \\int_{\\mathcal{Y}} y^2 p(y|x)\\,\\mathrm{d}y - 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2\\cdot \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y. \\] <p>For the above equation, the first term is independent on \\(z\\) and</p> \\[ \\int_{\\mathcal{Y}} p(y|x)\\,\\mathrm{d}y = 1, \\] <p>for fixed \\(x\\). Then we have</p> \\[ \\begin{aligned} f(x) &amp;= \\operatorname*{arg\\, min}\\limits_{z} \\left(- 2z\\cdot \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y + z^2 \\right) \\\\ &amp;= \\int_{\\mathcal{Y}} y\\cdot p(y|x)\\,\\mathrm{d}y \\\\ &amp;= E(y|x).  \\end{aligned} \\] <p>Therefore, oracle predictor is given by \\(f^*(\\tilde{x})=E(y|X=\\tilde{x})\\). Similar arguments can be obtained for other loss functions. </p> <p>Additionally, our computation shows that making assumptions about the allowable \\(f\\), i.e. assume \\(f\\) lies in some set of functions \\(\\mathcal{F}\\), is somewhat equivalent to making assumptions about \\(P\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#making-assumptions","title":"Making Assumptions","text":"<p>Ideally we would like a \\(f\\) such that \\(R(f,P)\\) is small for all distribution \\(P\\). However, this is not possible by the No Free Lunch Theorem. Roughly, this says that for any \\(f\\), there exists a \\(P\\) such that \\(R(f,P)\\) is large. In classification, this says that there exists a \\(P\\) such that \\(f\\) is no better than random guessing(1). </p> <ol> <li>Random guessing is that we flip a coin and predict \\(y\\) based on the coin being heads or tails.</li> </ol> <p>Our solution is to make assumptions about \\(P\\):  </p> <p>Assume \\(P\\in\\mathcal{P}\\), where \\(\\mathcal{P}\\) is some subset of probability distributions. This suggest assumptions that we can make about the function class of predictors that we use. For example, \\(E(y|x)\\) is optimal for squared error loss. Given \\(\\mathcal{P}\\) we may want to restrict \\(\\mathcal{F}\\) to functions that have the form \\(E(y|x)\\) for \\(P\\in \\mathcal{P}\\).  </p>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#complexity-bias-variance-tradeoff","title":"Complexity (bias-variance) tradeoff:","text":"<ul> <li>More complex function classes \\(\\mathcal{F}\\) -- low bias (i.e. able to approximate the oracle \\(E(y|x)\\))</li> <li>Large class of \\(\\mathcal{F}\\) -- it is hard to find the best \\(f\\).</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#error-decomposition","title":"Error Decomposition","text":"<p>For \\(f\\in \\mathcal{F}\\), since \\(\\mathcal{F}\\) may not be large enough, \\(R^*(P)\\) and \\(\\inf_{f\\in \\mathcal{F}} R(f,P)\\) may not be equal. We have the following decomposition of the risk function \\(R(f,P)\\):</p> \\[ {\\color{red} R(f,P) - \\inf_{f\\in \\mathcal{F}} R(f,P)} + {\\color{green} \\inf_{f\\in \\mathcal{F}} R(f,P) - R^*(P)} + {\\color{blue} R^*(P)}. \\] <ul> <li>\\({\\color{red} \\text{Red}}\\): Estimation error, which is non-negative.</li> <li>\\({\\color{green} \\text{Green}}\\): Approximation error, which is non-negative.</li> <li>\\({\\color{blue} \\text{Blue}}\\): the inherent error, which is the best we can do!</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#empirical-risk-minimization-erm","title":"Empirical Risk Minimization (ERM)","text":"<p>Idea is to find an approximation of \\(R(f,P)\\) and minimize this over \\(f\\) (also assume that \\(f\\) lies in some specified class of functions \\(\\mathcal{F}\\)). The ERM predictor \\(\\hat{f}\\) is defined as </p> \\[ \\hat{f} = \\operatorname*{arg\\, min}_{g\\in \\mathcal{F}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}L\\left(g(x^{(i)}),y^{(i)}\\right)\\right), \\] <p>which is called the average loss or empirical risk. Note that </p> <ul> <li>The empirical risk depends on the choice of function class \\(\\mathcal{F}\\), such as linear functions and complicated neural networks.</li> <li>Finding the <code>argmin</code> is not always easy, and hence there are various optimization algorithms approximating the <code>argmin</code>.</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week1/#gauge-learning-algorithms","title":"Gauge Learning Algorithms","text":"<p>Given a learning algorithm \\(\\phi_n\\), how can we gauge the performance of \\(\\phi_n\\)? We can look at \\(R(\\hat{f},P)\\), that is, we view the training data \\(\\mathcal{D}_n\\) as random and then \\(R(\\hat{f},P)\\)(1) is a random variable. The expected risk (a.k.a. expected prediction error) is given by</p> <ol> <li>Here \\(\\hat{f}\\) is dependent on \\(\\mathcal{D}_n\\), which should be written as \\(\\hat{f}_{\\mathcal{D}_n}\\), but for simplicity, we still denote it as \\(\\hat{f}\\). </li> </ol> \\[ R(\\phi_n,P) = E_{\\mathcal{D}_n}\\left(R(\\hat{f},P)\\right) = E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(L(\\hat{f},y)\\right)\\right).  \\] <p>There are two sources of randomness: </p> <ul> <li>The training data \\(\\mathcal{D}_n\\) that determines \\(\\hat{f}\\). </li> <li>The pair \\((x,y)\\) where we predict \\(y\\) using \\(\\hat{f}(x)\\). </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week10/","title":"Week 10","text":""},{"location":"notes/lecture_notes/stat541/stat541_week10/#classification-and-regression-trees","title":"Classification and Regression Trees","text":"<p>Recall GAMs is complex univariate modelling, but no interaction effects between features. Here we introduce trees, which are simple univariate models (piecewise constant) but models interactions in a flexible manner. It can be motivated by a sequential decision making process:</p> \\[ \\begin{matrix} \\text{State } A \\rightarrow &amp; \\text{State } B_1 \\rightarrow &amp;\\text{State } C_1 \\rightarrow &amp; \\text{State } D_1 \\\\ \\downarrow &amp; \\downarrow  &amp; \\downarrow &amp; \\\\ \\text{State } B_2 &amp; \\text{State } C_2 &amp; \\text{State } D_2 \\rightarrow &amp; \\text{State } E_1 \\\\  &amp; &amp; \\downarrow &amp;  \\\\  &amp; &amp; \\text{State } E_2 &amp;  \\end{matrix} \\] <p>where at each state, we make a decision and switch to the corresponding state. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#regression-tree","title":"Regression Tree","text":"<p>For a regression tree, our prediction function has the form</p> \\[ f(x) = \\sum_{i=1}^m \\beta_i I(x\\in R_i),  \\] <p>where \\(R_1,\\dots,R_m\\) are rectangles that partition \\(\\mathbb{R}^p\\). The following figure shows an example for \\(p=2\\).  </p> <p>If the rectangles are known, we simply estimate \\(\\beta\\) as: for each \\(1\\leq j\\leq m\\), </p> \\[ \\hat{\\beta}_j = \\frac{\\sum_{i=1}^n I\\left(x^{(i)}\\in R_j\\right)y^{(i)}}{\\sum_{i=1}^n I\\left(x^{(i)}\\in R_j\\right)}.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week10/#classification-tree","title":"Classification Tree","text":"<p>For classification, the prediction function (with \\(K\\) classes) has the same form </p> \\[ f(x) = \\sum_{i=1}^m \\beta_i I(x\\in R_i),  \\] <p>where \\(\\beta_i\\in \\{1,\\dots,K\\}\\). The estimation is given by </p> \\[ \\hat{\\beta}_j = \\operatorname*{arg\\, max}_{t\\in \\{1,\\dots,n\\}} \\sum_{i=1}^n I\\left(x^{(i)}\\in R_j\\right)\\cdot I\\left(y^{(i)} = t\\right),  \\] <p>i.e. the majority vote of the data points located in \\(R_j\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#partition-process","title":"Partition Process","text":"<p>We recursively split the feature space \\(\\mathbb{R}^p\\) by directions that are parallel to the coordinates axis. Each split is chosen based on previous splits. The following figure shows an example of implementing 3 splits.  </p> <p>If we regard each split as a decision making, then choosing the partition is the same as a sequential decision making process: </p> \\[ \\begin{matrix} R_{01}\\,(\\mathbb{R}^p) \\rightarrow &amp; R_{11} \\rightarrow &amp; R_{21} \\rightarrow &amp; R_{31} \\\\ \\downarrow &amp; \\downarrow  &amp; \\downarrow &amp; \\\\ R_{12} &amp; R_{22} &amp; R_{32} \\rightarrow &amp; R_{41} \\\\  &amp; &amp; \\downarrow &amp;  \\\\  &amp; &amp; R_{42} &amp;  \\end{matrix} \\] <p>where \\(R_{ij}\\) stands for the \\(j\\)-th part of the \\(i\\)-th split. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#impurity-measure","title":"Impurity Measure","text":"<p>To specify each split, we need to tell whether a split is good(1) or not. Here we introduce an impurity measure that applies to the rectangles and our goal is to minimize the impurities. </p> <ol> <li>Intuitively, a split is good when the data points within the rectangle have similar \\(y^{(i)}\\) values. </li> </ol> <p>In regression, we just use MSE. Let \\(n_{ij}\\) be the number of observations in rectangle \\(R_{ij}\\). Then </p> \\[ {\\rm Impurity}(R_{ij}) = \\frac{1}{n_{ij}} \\sum_{l\\mid x^{(i)}\\in R_{ij}} \\left(y^{(l)} - \\bar{y}_{ij}\\right)^2, \\] <p>where \\(\\bar{y}_{ij}\\) is the mean of all \\(y^{(l)}\\) whose \\(x^{(l)}\\in R_{ij}\\). </p> <p>In classification, we use misclassification rate or Gini index. Let \\(\\hat{p}_l\\) be the proportion of observations in rectangle \\(R_{ij}\\) at class \\(l\\). </p> <ul> <li> <p>Misclassification Rate: \\(\\displaystyle {\\rm Impurity}(R_{ij}) = 1 - \\max_l \\hat{p}_l\\), where \\(l\\) ranges over the classes \\(\\{1,\\dots,K\\}\\). </p> </li> <li> <p>Gini Index: \\(\\displaystyle {\\rm Impurity}(R_{ij}) = \\sum_{s,t} \\hat{p}_s \\hat{p}_t\\), where \\(s,t\\) ranges over the classes \\(\\{1,\\dots,K\\}\\). </p> </li> </ul> <p>At each step we consider a split and we sum the impurity measure over every rectangle in the split. Choose the split with smallest impurity sum. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#how-to-choose-how-big-of-a-tree-to-grow","title":"How to Choose How Big of a Tree to Grow","text":"<p>How do we decide the depth of our decision tree (i.e. how many splits will we do)? The basic rules are: </p> <ul> <li>Stop splitting once we have \\(m\\) leaf nodes. </li> <li>Stop splitting once ever node has less than \\(\\tilde{m}\\) observations. </li> </ul> <p>We choose \\(m\\) or \\(\\tilde{m}\\) via cross-validation. Take the first rule as an example: Choose a grid of \\(m\\), such as \\(5,10,15,20\\). For each \\(m\\), compute the CV error using first rule. Choose \\(m\\) with the smallest CV error. </p> <p>A more common stopping rule is cost-complexity pruning: </p> <ol> <li>Grow a very large tree \\(T\\). </li> <li>Consider subtrees \\(T^\\prime \\subset T\\) (many different subtrees).</li> <li>Find the subtree \\(T^\\prime\\) that minimizes a loss function plus penalty \\(\\lambda \\left|T^\\prime\\right|\\), where \\(\\lambda&gt;0\\) is a hyperparameter and \\(\\left|T^\\prime\\right|\\) is the number of leaf nodes of the subtree. </li> </ol> <p>The 3rd step is given by </p> \\[ \\operatorname*{arg\\, min}_{T^\\prime\\subset T} \\left(\\frac{1}{n}\\sum_{i=1}^n L\\left(\\hat{f}_{T^\\prime}\\left(x^{(i)}\\right),y^{(i)}\\right)+\\lambda \\left|T^\\prime\\right|\\right), \\] <p>where \\(\\hat{f}_{T^\\prime}\\left(x^{(i)}\\right)\\) is the prediction function based on the subtree \\(T^\\prime\\). Note that there is actually clever way to search over subtrees and solve this minimization, and the hyperparameter \\(\\lambda\\) is chosen via cross-validation. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#summary","title":"Summary","text":"<p>Tree optimization algorithm (minimizing impurity) is greed: it only looks at how a split alter impurity at the current step, and does not account for splits that are mediocre right now but could be helpful later on. </p> <p>In general, decision trees are interpretable, but predictive accuracy is not always great (often overfit). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#ensemble-methods-model-averaging","title":"Ensemble Methods: Model Averaging","text":"<p>If we have \\(m\\) different prediction methods (such as trees, splines, linear regression, KNN, \\(\\dots\\)). For each method we have prediction functions \\(\\hat{f}_1,\\dots,\\hat{f}_m\\). We combine these to get a new prediction function: in regression,</p> \\[ \\hat{f}(x_*) = \\frac{1}{m}\\sum_{i=1}^m \\hat{f}_i(x_*);   \\] <p>in classification, \\(\\hat{f}(x_*)\\) is the majority vote of \\(\\hat{f}_1(x_*),\\dots,\\hat{f}_m(x_*)\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#summary_1","title":"Summary","text":"<p>Model averaging reduces the variance of predictions, but brings extra computational cost. If predictions among the models are similar, it maybe not so helpful (averaging the same predictor). It generally won't hurt to use model averaging unless we use some bad prediction \\(\\hat{f}_i\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#bagging-bootstrap-aggregation","title":"Bagging (Bootstrap Aggregation)","text":"<p>The bootstrap is a widely applicable and extremely powerful statistical tool bootstrap  that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. As an example, the bootstrap can be used to estimate the standard errors of the coefficients from a regression fit.</p> <p>Evaluate the Accuracy of an Estimator: Assume the observations follow a model with a parameter \\(\\alpha\\). If we use certain method to get an estimated parameter \\(\\hat{\\alpha}\\), we may want to know the accuracy of \\(\\hat{\\alpha}\\). If we can obtain another samples and use the same method to estimate \\(\\alpha\\) using new samples and repeat this again and again, the accuracy of \\(\\hat{\\alpha}\\) can then be evaluated by the standard deviation among all the rounds of estimations, denoted as \\(\\mathrm{SE}_B(\\hat{\\alpha})\\). Roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately \\(\\mathrm{SE}_B(\\hat{\\alpha})\\), on average.</p> <p>Bootstrap Method: We randomly select \\(n\\) observations from the data set to produce a bootstrap data set, \\(Z^{\u22171}\\). The sampling is performed with replacement(1). We can use \\(Z^{* 1}\\) to produce a new bootstrap estimate for \\(\\alpha\\), which we call \\(\\hat{\\alpha}^{* 1}\\). This procedure is repeated \\(B\\) times, in order to produce \\(B\\) different bootstrap data sets, \\(Z^{* 1}, Z^{* 2}, \\ldots, Z^{* B}\\), and \\(B\\) corresponding \\(\\alpha\\) estimates, \\(\\hat{\\alpha}^{* 1}, \\hat{\\alpha}^{* 2}, \\ldots, \\hat{\\alpha}^{* B}\\). We can compute the standard error of these bootstrap estimates using the formula</p> <ol> <li>This means that the same observation can occur more than once in the bootstrap data set. </li> </ol> \\[ \\mathrm{SE}_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1} \\sum_{r=1}^B\\left(\\hat{\\alpha}^{* r}-\\frac{1}{B} \\sum_{r^{\\prime}=1}^B \\hat{\\alpha}^{* r^{\\prime}}\\right)^2} \\] <p>This serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. This approach is illustrated in the following figure(1). </p> <ol> <li>A graphical illustration of the bootstrap approach on a small sample containing \\(n = 3\\) observations. Each bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of \\(\\alpha\\).</li> </ol> <p></p> <p>Bootstrap Aggregation: We find \\(B\\) different fits using \\(Z^{* 1}, Z^{* 2}, \\ldots, Z^{* B}\\) as training data, and obtain \\(\\hat{f}_{(1)},\\dots,\\hat{f}_{(B)}\\). Use model averaging to average these fits. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week10/#random-forests","title":"Random Forests","text":"<p>Random forest is  almost the same as bagging except when we fit each bootstrap data set using trees (i.e the prediction function \\(\\hat{f}_{(i)}\\) is given by trees). At each split, we choose a random subset of the variables over which we make a split. For example, for \\(p=2\\), we flip a coin before each split to decide to split horizontally or vertically (say heads for horizontal split). </p> <p>For problems with a large number of features (\\(p\\gg 2\\)), we may restrict the size of the random subsets. For example, for \\(p=50\\), we can choose a random subsets of size 20 of the features to split at each step. </p> <p>Due to the randomness(1), random forest avoids the greediness of the tree optimization. Compared to directly using trees, random forest improves the predictive accuracy, but not so interpretable. </p> <ol> <li>Recall that tree method usually suffer from overfitting. Therefore, it should be favorable to slightly increase the bias of the model while reducing the variance, which is done by the randomness of splitting subsets. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week11/","title":"Week 11","text":""},{"location":"notes/lecture_notes/stat541/stat541_week11/#projection-pursuit-regression","title":"Projection Pursuit Regression","text":"<p>Projection pursuit regression (PPR) is a statistical model that extends additive models. This model adapts the additive models in that it first projects the design matrix of features in the optimal direction before applying smoothing functions.</p> <p>Assume we have an input vector \\(X\\in \\mathbb{R}^p\\), and a target \\(Y\\). Let \\(\\omega_m\\in \\in \\mathbb{R}^p, m=1,2, \\ldots, M\\), be unit vectors of unknown parameters. The PPR model has the form</p> \\[ f(X)=\\sum_{m=1}^M g_m\\left(\\omega_m\\cdot X\\right). \\] <p>This is an additive model, but in the derived features \\(V_m=\\omega_m^T X\\) rather than the inputs themselves. The functions \\(g_m\\) are unspecified and are estimated along with the directions \\(\\omega_m\\) using some flexible smoothing method.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week11/#ridge-functions","title":"Ridge Functions","text":"<p>The function \\(g_m\\left(\\omega_m\\cdot X\\right)\\) is called a ridge function in \\(\\mathbb{R}^p\\). It varies only in the direction defined by \\(\\omega_m\\). The scalar variable \\(V_m\\) is the projection of \\(X\\) onto the unit vector \\(\\omega_m\\), and we seek \\(\\omega_m\\) so that the model fits well, hence the name \"projection pursuit.\" Figure 11.1 shows some examples of ridge functions. In the example on the left \\(\\omega=(1 / \\sqrt{2})(1,1)^T\\), so that the function only varies in the direction \\(X_1+X_2\\). In the example on the right, \\(\\omega=(1,0)\\).  </p> <p>The PPR model is very general, since forming nonlinear functions of linear combinations generates a surprisingly large class of models. For example, the product \\(X_1 \\cdot X_2\\) can be written as \\(\\frac{1}{4}\\left(\\left(X_1+X_2\\right)^2-\\left(X_1-X_2\\right)^2\\right)\\).</p> <p>Remarkable fact: If \\(M\\) is taken arbitrarily large, for appropriate choice of \\(g_m\\) the PPR model can approximate any continuous function in \\(\\mathbb{R}^p\\) arbitrarily well(1). </p> <ol> <li>Such a class of models is called a universal approximator. However this generality comes at a price. Interpretation of the fitted model is usually difficult, because each input enters into the model in a complex and multifaceted way. As a result, the PPR model is most useful for prediction, and not very useful for producing an understandable model for the data. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week11/#how-to-fit-the-ppr-model","title":"How to Fit the PPR Model","text":"<p>How do we fit a PPR model, given training data \\(\\left(x^{(i)}, y^{(i)}\\right), i=1,2, \\dots, n\\)? We seek the approximate minimizers of the error function</p> \\[ \\sum_{i=1}^n\\left(y^{(i)}-\\sum_{m=1}^M g_m\\left(\\omega_m\\cdot x^{(i)}\\right)\\right)^2 \\] <p>over \\(g_m\\) and \\(\\omega_m, m=1,2, \\dots, M\\). </p> <p>Consider just one term ( \\(M=1\\), and drop the subscript):</p> <ul> <li> <p>Given the direction vector \\(\\omega\\), we form the derived variables \\(v_i=\\omega^T x_i\\). Then we have a one-dimensional smoothing problem, and we can apply any scatterplot smoother, such as a smoothing spline, to obtain an estimate of \\(g\\). </p> </li> <li> <p>Given \\(g\\), we want to minimize the error function over \\(\\omega\\), where a Gauss-Newton search (see p.391, ESL) is convenient. </p> </li> </ul> <p>These two steps, estimation of \\(g\\) and \\(\\omega\\), are iterated until convergence. </p> <p>With more than one term in the PPR model, the model is built in a forward stage-wise manner(1), adding a pair \\(\\left(\\omega_m, g_m\\right)\\) at each stage.</p> <ol> <li>Forward stagewise modeling approximates the solution to the minimization of the error function \\(\\displaystyle \\min_{g_m,\\omega_m} \\sum_{i=1}^n\\left(y^{(i)}-\\sum_{m=1}^K g_m\\left(\\omega_m\\cdot x^{(i)}\\right)\\right)^2\\), \\(1\\leq K\\leq M-1\\), by sequentially adding new basis functions \\(g_{K+1}\\left(\\omega_{K+1}\\cdot x^{(i)}\\right)\\) to the minimization without adjusting the parameters of those that have already been added. For more details see Section 10.3, ESL. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week11/#implementation-details","title":"Implementation Details","text":"<p>There are a number of implementation details:</p> <ul> <li>After each step the \\(g_m\\) 's from previous steps can be readjusted using backfitting. (While this may lead ultimately to fewer terms, it is not clear whether it improves prediction performance.)</li> <li>Usually the \\(\\omega_m\\) are not readjusted (partly to avoid excessive computation), although in principle they could be as well.</li> <li>The number of terms \\(M\\) is usually estimated as part of the forward stage-wise strategy. The model building stops when the next term does not appreciably improve the fit of the model. Cross-validation can also be used to determine \\(M\\).</li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week11/#neural-networks","title":"Neural Networks","text":"<p>See Chapter 10, ISLR, for</p> <ul> <li>10.1 Single Layer Neural Networks</li> <li>10.2 Multilayer Neural Networks</li> <li>10.7 Fitting a Neural Network<ul> <li>10.7.1 Backpropagation</li> </ul> </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week12/","title":"Week 12","text":""},{"location":"notes/lecture_notes/stat541/stat541_week12/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Recall that in supervised learning, we have a data set \\(\\left(x^{(1)}, y^{(1)} \\right), \\dots, \\left(x^{(n)}, y^{(n)} \\right)\\) and we predict \\(y_*\\) using \\(x_*\\). In unsupervised learning, we no longer have \\(y\\), and just get a data set \\(x^{(1)}, \\dots, x^{(n)}\\in \\mathbb{R}^p\\). The goal is to learn about the distribution of \\(x\\)'s.  </p> <p>Two examples for why unsupervised learning is useful: </p> <ul> <li> <p>Market Segmentation: A company selling a product might want to divide their customer base into different groups. Specifically, if we have features \\(x^{(i)}\\) for customer \\(i\\), we want t o divide customers into certain blocks only using \\(x^{(i)}\\)'s. </p> </li> <li> <p>Initial Feature Transformation: Another example to do unsupervised learning can be an initial feature transformation step in a supervised learning pipeline.</p> </li> </ul> \\[ x \\rightarrow \\phi(x) \\rightarrow \\begin{aligned}     &amp;\\text{Using this an input into} \\\\     &amp;\\text{a supervised learning model} \\end{aligned} \\] <p>where \\(\\phi(x)\\) is a feature transformation. For example, we can use PCA to reduce the dimensionality of the data before applying a supervised learning model.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#clustering","title":"Clustering","text":"<p>Intuitively, we want to divide points into clusters: </p> <ul> <li>Points in the same cluster should be close. </li> <li>Points in different clusters  should be far away from each other. </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#k-means","title":"K-Means","text":"<p>Fix \\(K\\) to be the number of clusters. The way we do K-means clustering is by defining a clustering assignment map:</p> \\[ f:\\{1, \\dots, n\\} \\rightarrow\\{1, \\dots, K\\} \\] <p>where \\(n \\geq K\\) is the number of data points, \\(x^{(i)}\\). Assume that each cluster has a center or mean \\(\\mu_j\\in \\mathbb{p}\\), \\(j=1,\\dots, K\\). We want to minimize the loss function:</p> \\[ L\\left(f, \\mu_1, \\dots, \\mu_k\\right)=\\sum_{i=1}^n \\left\\|x^{(i)}-\\mu_{f(i)}\\right\\|^2, \\] <p>where \\(\\left\\|x^{(i)}-\\mu_j\\right\\|^2\\) is the squared distance between the data point and the cluster center. We want to minimize this loss function over all possible assignments \\(f\\) and all possible cluster centers \\(\\mu_1,\\dots,\\mu_K\\), usually iteratively (like backfitting)(1). </p> <ol> <li>We first fix \\(\\mu_j\\) and find the optimal \\(\\hat{f}\\), then fix \\(f=\\hat{f}\\) and find the optimal \\(\\hat{\\mu}_j\\)'s. Then, fix the \\(\\mu_j=\\hat{\\mu}_j\\) 's and find the optimal \\(\\hat{f}\\) again, and so on until convergence.</li> </ol> <p>Fix \\(\\mu_1,\\dots,\\mu_K\\), and find the optimal \\(f\\): We just find the optimal value of \\(f(i)\\in \\{1,\\dots,K\\}\\) for every \\(i = 1,\\dots,n\\), that is to minimize </p> \\[ \\left\\|x^{(i)}-\\mu_{f(i)}\\right\\|^2. \\] <p>We simply assign point \\(x^{(i)}\\) to the closest cluster \\(\\mu_j\\), \\(j=1,\\dots, K\\). </p> <p>Fix \\(f\\), and find the optimal \\(\\mu_j\\): As \\(\\mu_j\\) only appears in the \\(i\\)-th term if \\(f(i) = j\\), we just minimize </p> \\[ \\hat{\\mu}_j = \\operatorname*{arg\\, min}_{\\mu_j} \\sum_{i\\mid f(i)=j} \\left\\|x^{(i)}-\\mu_{j}\\right\\|^2. \\] <p>Taking the gradient to be 0, we have</p> \\[ \\sum_{i\\mid f(i)=j} x^{(i)} = \\sum_{i\\mid f(i)=j} \\mu_j = \\mu_j\\cdot \\hat{n}_j,  \\] <p>where \\(\\hat{n}_j\\) is the number of data points assigned to cluster \\(j\\). Therefore, </p> \\[ \\hat{\\mu}_j = \\frac{1}{\\hat{n}_j} \\sum_{i\\mid f(i)=j} x^{(i)}.  \\] <p>Note that the loss function \\(L\\left(f, \\mu_1, \\dots, \\mu_k\\right)\\) is non-convex and thus, it has local but not global minima: </p> <ul> <li>The result of our algorithm is sensitive to the choice of starting values for \\(\\mu_1,\\dots,\\mu_K\\). </li> <li>It is a good idea to run the algorithm multiple times with different starting points. </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#scaling-features","title":"Scaling Features","text":"<p>Scaling of the features in K-means is important. Usually we will center and rescale them by the feature's standard deviation. We define</p> \\[ \\bar{x}=\\frac{1}{n} \\sum x^{(i)} \\] <p>and we then map \\(x^{(i)} \\mapsto x^{(i)}-\\bar{x}\\). Then for each feature \\(j\\), we scale by \\(\\hat{\\sigma}\\).</p> \\[ \\hat{\\sigma}_j=\\frac{1}{\\sqrt{n}} \\sqrt{\\sum_{i=1}^{n}\\left(x^{(i)}_j-\\bar{x}\\right)^2}. \\] <p>And so we obtain </p> \\[ x^{(i)}-\\bar{x} \\mapsto \\frac{x^{(i)}-\\bar{x}}{\\hat{\\sigma}_j} \\] <p>for each feature. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#choosing-the-number-of-clusters","title":"Choosing the Number of Clusters","text":"<p>For unsupervised learning, cross-validation generally doesn't work. The elbow method is a way to determine the number of clusters \\(K\\). Previously we wanted to minimize</p> \\[ L\\left(f, \\mu_1, \\dots, \\mu_k\\right)=\\sum_{j=1}^K \\sum_{i\\mid f(i)=j}\\left\\|x^{(i)}-\\mu_j\\right\\|^2, \\] <p>where \\(f\\) is the assignment function and \\(\\mu_j\\) is the cluster center.</p> <p>For any \\(K\\) define </p> \\[ \\hat{L}_k=\\min _{f, \\mu_1, \\dots, \\mu_k} \\sum_{j=1}^K \\sum_{i\\mid f(i)=j}\\left\\|x^{(i)}-\\mu_j\\right\\|^2. \\] <p>As \\(K\\) gets larger, then \\(\\hat{L}_k\\) should decrease. In the extreme case where \\(K=n\\), \\(\\hat{L}_n=0\\).</p> <p>Plot \\(\\hat{L}_k\\) vs. \\(K=1,2, \\dots, n\\), and look at the succesive differences of \\(\\hat{L}_k\\), i.e. \\(\\hat{L}_k - \\hat{L}_{K+1}\\). Ideally what we want is that the first few clusters drop a lot (lower loss), but then as we add additional clusters, the decrease in loss is smaller. </p> <p>The way we can determine where the elbow is to find the largest difference. It is also useful to regenerate the plot of \\(\\hat{L}_k\\) vs. \\(K\\) multiple times if possible to account for any convergence issues.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering is to fit a sequence of cluster assignments, which allows for a \"continuous\" \\(K\\) to be chosen. Here we introduce agglomerative hierarchical clustering, which starts with \\(n\\) clusters, then sequentially fuse the clusters together until we have one.</p> <p>The way to merge the clusters is to use a notion of dissimilarity between clusters. There are various choices for dissimilarity measures.</p> <p>Average Linkage: The dissimilarity between two clusters is defined as the average distance(1) between all pairs of points. More precisely,</p> <ol> <li>This is not necessary to be a distance, i.e. it may not satisfy the triangle inequality, such as \\(\\|x-y\\|^2\\). In fact, any \\(f(|x-y|)\\), \\(x,y\\in\\mathbb{R}^p\\), where \\(f\\) is a monotone increasing function of the Euclidean norm, can induce a dissimilarity measure between clusters by using the maximum, average or minimum values of this quantity between pairs of points in clusters.</li> </ol> \\[ d_{\\mathrm{avg}}\\left(C_1, C_2\\right)=\\frac{1}{\\left|C_1\\right| \\cdot\\left|C_2\\right|} \\sum_{x \\in C_1} \\sum_{y \\in C_2} d(x, y) \\] <p>where \\(C_1\\) and \\(C_2\\) are the two clusters, \\(d(x, y)\\) is the distance between points \\(x\\) and \\(y\\), and \\(\\left|C_1\\right|\\) and \\(\\left|C_2\\right|\\) are the sizes of the clusters. Usually we take \\(d(x, y)=\\|x-y\\|^2\\).(1)</p> <ol> <li>Suppose we have DNA sequences from different organisms. We often use the Hamming distance (see Problem 3,(b)) to cluster them based on their similarity. Using these distances, we can construct a dendrogram to visualize the evolutionary relationships between the organisms. The sequences with smaller Hamming distances will be clustered together first, reflecting their closer evolutionary relationship.</li> </ol> <p>Complete Linkage: The dissimilarity between two clusters is defined as the maximum distance between any pair of points. More precisely, </p> \\[ d_{\\mathrm{com}}\\left(C_1, C_2\\right)=\\max _{x \\in C_1, y \\in C_2} d(x, y). \\] <p>Single Linkage: Instead of considering the furthest points, we consider the closest points between two clusters. The dissimilarity is defined as:</p> \\[ S\\left(C_1, C_2\\right)=d_{\\text {single }}\\left(C_1, C_2\\right)=\\min _{x \\in C_1, y \\in C_2} d(x, y).  \\] <p>Complete linkage tends to give more balanced clusters, and single linkage tends to give less balanced, long, \"chain-like\" clusters. Average linkage is somewhere in between.</p> <p>Algorithm for Hierarchical Clustering: At every step we want to find two clusters, \\(C_i, C_j\\), that minimize \\(d\\left(C_i, C_J\\right)\\). Where the clusters are obtained from the previous step.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#dendrograms","title":"Dendrograms","text":"<p>A dendrogram (generally depicted as an upside-down tree) is built starting from the leaves and combining clusters up to the trunk(1). In this, the horizontal axis represents the individual data points or clusters, and the vertical axis shows the dissimilarity at which clusters were merged. See the below example. </p> <ol> <li>There are many ways to display the same dendrogram (i.e. you can permute the leaves)</li> </ol> <p></p> <ul> <li>Left: a dendrogram obtained from hierarchically clustering. </li> <li>Center: the dendrogram from the left-hand panel, cut at a height of nine (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors. </li> <li>Right: the dendrogram from the left-hand panel, now cut at a height of five. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure.</li> </ul> <p>Choosing the Number of Clusters: We can \"cut\" the dendrogram at the height where the largest vertical gap occurs, as this indicates a natural separation between clusters.</p> <p>Example (Hamming Distance on DNA Sequencing): Suppose we have DNA sequences from different organisms, and we want to cluster them based on their similarity. One way to measure similarity is the Hamming distance, which counts the number of positions at which the corresponding nucleotides differ between two sequences.(1) Using these distances, we can construct a dendrogram to visualize the evolutionary relationships between the organisms. The sequences with smaller Hamming distances will be clustered together first, reflecting their closer evolutionary relationship.</p> <ol> <li>For example, consider the following DNA sequences: \\(\\displaystyle {\\mathrm S1:}\\ ACGTAC,\\ {\\mathrm S2:}\\  ACGTTC,\\ {\\mathrm S3:}\\ ACGGAC\\). The Hamming distances are: \\(d(S1,S2) = 1\\) (only the fifth nucleotide differs), \\(d(S1,S3) = 1\\) (only the fourth nucleotide differs), and \\(d(S2,S3) = 2\\) (the fourth and fifth nucleotides differ).</li> </ol> <p>We can also look at divisive clustering, which is the opposite to the agglomerative clustering techniques we have looked at so far. The idea is to start with one big cluster, then split it into two smaller clusters, then continue splitting until each point is in its own cluster.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<p>K-Means did a \"hard assignment\" at points to clusters, i.e. each point is assigned to a single cluster. GMM, on the other hand, is a \"soft assignment\" algorithm, in that a point can be assigned to multiple clusters according to a probability.</p> <p>A GMM assumes that the data is generated from a mixture of \\(K\\) Gaussian distributions. The idea is to choose the cluster centers, and assign a multivariate Gaussian distribution at each cluster center. More precisely, the data generating mechanism for \\(x\\): </p> <ol> <li>Roll a \\(K\\) sided dice with probability of each side \\(\\pi_1,\\dots,\\pi_K\\), i.e. draw \\(z \\sim {\\rm Multinomial}_K(\\boldsymbol{\\pi},1)\\). </li> <li>If \\(z=j\\), where \\(j=1,\\dots,K\\), then draw \\(x\\) from the distribution \\(\\mathcal{N}_p\\left(\\mu_j,\\sigma_j^2I_p\\right)\\). </li> </ol> <p>Therefore, the probability density function for a point \\(x\\) is given by:</p> \\[ p(x)=\\sum_{j=1}^K \\pi_j p\\left(x \\mid \\mu_j, \\sigma_j^2\\right).  \\] <p>We assume that the data we observe \\(x^{(1)},\\dots,x^{(n)}\\) are all generated i.i.d. from this process, although we don't know the parameters \\(\\boldsymbol{\\pi},\\mu_1,\\dots,\\mu_K,\\sigma_1^2,\\dots,\\sigma_K^2\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#estimate-the-parameters","title":"Estimate the Parameters","text":"<p>The likelihood for a single \\(x^{(i)}\\) is</p> \\[ \\begin{aligned} p\\left(x^{(i)}\\right) &amp; =\\sum_{j=1}^k p\\left(x^{(i)}, z_i=j\\right) \\\\ &amp; =\\sum_{j=1}^k p\\left(x^{(i)} \\mid z_i=j\\right) p\\left(z_i=j\\right) \\\\ &amp; =\\sum p_{\\mathcal{N}_j}\\left(x^{(i)}\\right) \\pi_j,  \\end{aligned} \\] <p>where \\(p_{\\mathcal{N}_j}\\) is the pdf of the multivariate normal distribution \\(\\mathcal{N}_p\\left(\\mu_j,\\sigma_j^2I_p\\right)\\). Using MLE, we want to maximize</p> \\[ \\log \\left(\\prod_{i=1}^n p\\left(x^{(i)} \\mid \\pi, \\mu_1, \\ldots, \\mu_k, \\sigma_1^2, \\ldots, \\sigma_k^2\\right)\\right) \\] <p>with respect to \\(\\boldsymbol{\\pi},\\mu_1,\\dots,\\mu_K,\\sigma_1^2,\\dots,\\sigma_K^2\\).  EM algorithm to maximize GMM's </p> <p>We can use the expectation maximization (EM) algorithm to find the MLEs since there is no closed form solution if we try to differentiate and solve algebraically. (As the loss function can be non-convex, the EM algorithm can find local minima but not necessary a global minimum.)</p>"},{"location":"notes/lecture_notes/stat541/stat541_week12/#em-algorithm","title":"EM Algorithm","text":"<p>Define the responsibility of cluster \\(j\\) to the data point \\(i\\) as</p> \\[ \\hat{r}_{i j}=\\frac{\\hat{\\pi}_j \\hat{p}_{\\mathcal{N}_j}\\left(x^{(i)}\\right)}{\\sum_{l=1}^k \\hat{\\pi_l} \\hat{p}_{\\mathcal{N}_l}\\left(x^{(i)}\\right)}, \\] <p>which can be interpreted as the current estimate of the probability that \\(x^{(i)}\\) was generated from cluster \\(j\\). This is based on the estimators \\(\\hat{\\boldsymbol{\\pi}},\\hat{\\mu}_1,\\dots,\\hat{\\mu}_K,\\hat{\\sigma}_1^2,\\dots,\\hat{\\sigma}_K^2\\) from previous step of the algorithm. </p> <p>E-step: Compute the responsibilities \\(r_{i j}\\) for all \\(i\\) and \\(j\\) (note that because these responsibilities are probabilities, that \\(\\sum_{j=1}^K r_{i j}=1\\) ). </p> <p>M-step: Update the parameters as follows:</p> \\[ \\begin{aligned} \\pi_j &amp; =\\frac{\\hat{n}_j}{n} \\\\ \\mu_j &amp; =\\frac{\\sum_{i=1}^n \\hat{r}_{i j} x^{(i)}}{\\hat{n}_j} \\\\ \\sigma_j^2 &amp; =\\frac{1}{\\hat{n}_j}\\sum_{i=1}^n \\hat{r}_{i j}\\cdot \\frac{1}{p}\\left\\|x^{(i)}-\\mu_j\\right\\|^2 \\end{aligned} \\] <p>where \\(p\\) is the dimensionality of \\(x^{(i)}\\) and \\(\\hat{n}_j=\\sum_{i=1}^n \\hat{r}_{i j}\\).</p> <p>Since we are modeling a probability distribution, we can generate new data from \\(p(x)=\\sum_j p(x \\mid z=j)\\cdot p(z=j)\\) (e.g. a very simple case of generative AI). We can also modify the Gaussian assumption very easily: \\(x^{(i)} \\mid z=j \\sim N\\left(\\mu_j, \\Sigma_j\\right)\\) for a general covariance matrix instead, and we can still model using a similar process to find the right estimates for \\(\\mu_j, \\Sigma_j, \\pi\\). </p> <p>Finally, we can approximate any continuous probability density function using a mixture model, if we take our value \\(K\\) to be large enough (i.e. as long as we have enough clusters we can capture a lot of the behavior of the probability density).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week13/","title":"Week 13","text":""},{"location":"notes/lecture_notes/stat541/stat541_week13/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA is a method for dimensionality reduction. Start with data, \\(x^{(i)} \\in \\mathbb{R}^p\\). The goal is to convert the data into a lower dimensional space, \\(\\beta^{(i)} \\in \\mathbb{R}^k\\) where \\(k&lt;p\\).(1)</p> <ol> <li>Clustering can also be thought of as dimensionality reduction: $f: x^{(i)}\\mapsto \\mu_{f(i)}, rather than retaining all \\(n\\times p\\) entries \\(x^{(1)}, \\dots, x^{(n)}\\), instead represent data by the cluster assignment \\(f\\) and \\(k\\times p\\) entries \\(\\mu_1, \\dots, \\mu_k\\). </li> </ol> <p>First we fix a \\(k\\) that reprents the dimension of the affine subspace onto which we want to project.</p> <p>Definition  Affine Subspace: An affine subspace is a translation of a vector subspace. Formally, a subset \\(S\\subset \\mathbb{R}^p\\) is an affine subspace if there exists a vector subspace \\(V\\) and a point (vector) \\(x_0 \\in S\\) such that \\(S=x_0+V\\). In other words, \\(S\\) is the set of all points that can be written as \\(x_0+v\\) for some \\(v \\in V\\).</p> <p>To find the affine subspace, \\(u+W\\), we can formulate two equivalent optimization problems:</p> <p>Variance Maximization: We can maximize the variance of the projected \\(x^{(i)}\\)'s.(1) Let \\(P_{u, W}\\left(x^{(i)}\\right)\\) denote the projection onto the affine space \\(u+W\\). We want to maximize the variance of the projected data. (For details see Problem 2(a-d).) </p> <ol> <li>The variance of the projected \\(x^{(i)}\\)'s indicates how much information we preserved from the original data. For example, in the following figure. The projected \\(x^{(i)}\\)'s onto \\(x\\)-axis have larger variance than those onto \\(y\\)-axis. </li> </ol> <p>Distance Minimization: We can find the least squares error of the projection (i.e. minimizing the distance between \\(x^{(i)}\\)'s and their projections). Let \\(P_{u, W}\\left(x^{(i)}\\right)\\) denote the projection onto the affine space \\(u+W\\). We want to minimize the least squares error of the projected data:</p> \\[ \\operatorname*{arg\\, min}_{u, W} \\sum_{i=1}^{n}\\left\\|x^{(i)}-P_{u, W}\\left(x^{(i)}\\right)\\right\\|^2. \\]"},{"location":"notes/lecture_notes/stat541/stat541_week13/#find-the-affine-subspace","title":"Find the Affine Subspace","text":"<p>Let \\(P_{u, W}(x)=u+w\\), for some \\(w \\in W\\). We can choose an orthonormal basis for \\(W\\), and put this basis into the columns of \\(V \\in \\mathbb{R}^{p \\times k}\\). We can then find a \\(\\beta\\in \\mathbb{R}^k\\) such that \\(w=V \\beta\\). We can then write the projection as:</p> \\[ P_{u, W}(x)=u+w=u+V \\beta.  \\] <p>Then to find the affine subspace with the least squares error of the projection, we only need to find the vector \\(u\\) and the orthogonal matrix \\(V\\). </p> <p>Now we have</p> \\[ \\operatorname*{arg\\, min}_{u, V} \\sum_{i=1}^{n}\\left\\|x^{(i)}-P_{u, W}\\left(x^{(i)}\\right)\\right\\|^2=\\operatorname*{arg\\, min}_{u, V} \\sum_{i=1}^{n}\\left\\|\\left(x^{(i)}-u\\right)-V\\beta\\right\\|^2.  \\] <p>Finding the \\(\\beta\\) minimizing this square loss error, is a regression problem and we have</p> \\[ \\hat{\\beta}=\\left(V^T V\\right)^{-1} V^T\\left(x^{(i)}-u\\right)=V^T\\left(x^{(i)}-u\\right). \\] <p>Plugging this back into the equation, we get:</p> \\[ \\begin{aligned} &amp;\\operatorname*{arg\\, min}_{u, V}  \\sum_{i=1}^{n}\\left\\|x^{(i)}-P_{u, W}\\left(x^{(i)}\\right)\\right\\|^2 \\\\ = &amp;\\operatorname*{arg\\, min}_{u, V}  \\sum\\left\\|x^{(i)}-u-V V^T\\left(x^{(i)}-u\\right)\\right\\|^2 \\\\ = &amp;\\operatorname*{arg\\, min}_{u, V}  \\sum\\left\\|\\left(I-V V^T\\right)\\left(x^{(i)}-u\\right)\\right\\|^2 \\\\ = &amp;\\operatorname*{arg\\, min}_{u, V}  \\sum\\left(x^{(i)}-u\\right)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}-u\\right) \\\\ = &amp;\\operatorname*{arg\\, min}_{u, V}  \\sum\\left(x^{(i)}+\\bar{x}-\\bar{x}-u\\right)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}+\\bar{x}-\\bar{x}-u\\right) \\\\ = &amp;\\operatorname*{arg\\, min}_{u, V}  \\left({\\color{red} \\sum\\left(x^{(i)}-\\bar{x}\\right)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}-\\bar{x}\\right)} + {\\color{green} \\sum(\\bar{x}-u)^T\\left(I-V V^T\\right)^2(\\bar{x}-u)}\\right. \\\\ &amp;\\quad \\left. - {\\color{blue} \\sum\\left(x^{(i)}-\\bar{x}\\right)^T\\left(I-V V^T\\right)^2(\\bar{x}-u)} - {\\color{blue} \\sum(\\bar{x}-u)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}-\\bar{x}\\right)} \\right), \\end{aligned} \\] <p>where the two \\({\\color{blue}\\rm blue}\\) terms are both 0. Therefore, we only need to minimize</p> \\[ \\operatorname*{arg\\, min}_{u, V}  \\left({\\color{red} \\sum\\left(x^{(i)}-\\bar{x}\\right)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}-\\bar{x}\\right)} + {\\color{green} \\sum(\\bar{x}-u)^T\\left(I-V V^T\\right)^2(\\bar{x}-u)}\\right). \\] <p>Find the \\(u\\): As the \\({\\color{red}\\rm red}\\) term is independent of \\(u\\), we find \\(u\\) via minimizing the \\({\\color{green}\\rm green}\\) term. Since the matrix \\(\\left(I-V V^T\\right)^2\\) is positive semi-definite, we can use the fact that the minimum is achieved when the quantity is 0 . Thus, we simply set \\(u=\\bar{x}\\). Note that this minimizes the \\({\\color{green}\\rm green}\\) term no matter what \\(V\\) is. </p> <p>Find the \\(V\\): We now need to minimize the \\({\\color{red}\\rm red}\\) term over \\(V\\). Set the centered matrix as</p> \\[ X_c=\\begin{bmatrix} \\left(x^{(1)} - \\bar{x}\\right)^T \\\\ \\vdots \\\\ \\left(x^{(n)} - \\bar{x}\\right)^T \\end{bmatrix},  \\] <p>and assume \\(X_c\\) has the SVD</p> \\[ X_c = UD\\tilde{V}^T,  \\] <p>where the singular values are ordered in decreasing order, i.e. \\(D = {\\rm diag}\\{d_{11},\\dots,d_{pp}\\}\\) with \\(d_{11}\\geq d_{22}\\geq \\dots\\geq d_{pp}\\geq 0\\). And the optimal \\(V\\) is the first \\(k\\) columns of the SVD matrix \\(\\tilde{V}\\), denoted as \\(V_k\\). (For details see the last section.)</p>"},{"location":"notes/lecture_notes/stat541/stat541_week13/#pc-scores-and-vectors","title":"PC Scores and Vectors","text":"<p>The projections onto our space is</p> \\[ \\begin{aligned} \\bar{x}+V_k V_k^T\\left(x^{(i)}-\\bar{x}\\right) &amp; =\\bar{x}+V_k \\hat{\\beta}_i \\\\ &amp; \\approx x^{(i)} \\end{aligned} \\] <p>where, \\(V_k \\hat{\\beta}_i=\\sum v_j \\hat{\\beta}_{i j}\\). The \\(\\hat{\\beta}_i=V_k^T\\left(x^{(i)}-\\bar{x}\\right) \\in \\mathbb{R}^k\\) are called the principal component scores of each data point, \\(x^{(i)}\\). The column vectors of \\(V_k\\) are called the principal component directions. </p> <p>We can then rewrite the \\(\\hat{\\beta}_i\\) 's:</p> \\[ \\begin{aligned} \\begin{bmatrix}\\hat{\\beta}_1, \\dots, \\hat{\\beta}_n\\end{bmatrix}  &amp; =V_k^T X_c^T  =V_k^T\\left(U D V^T\\right)^T  =V_k^T V D U^T \\\\ &amp; =\\begin{bmatrix} v_1^T \\\\ \\vdots \\\\ v_k^T \\end{bmatrix}\\begin{bmatrix} v_1 &amp; \\cdots &amp; v_p \\end{bmatrix} D U^T \\\\ &amp; =\\begin{bmatrix}I_k &amp; \\vdots &amp; 0\\end{bmatrix} D U^T \\\\ &amp; =D_k U_k^T, \\end{aligned} \\] <p>where</p> \\[ D_k=\\begin{bmatrix} d_{11} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_{22} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; d_{k k} \\end{bmatrix}, \\quad  U_k=\\begin{bmatrix} u_1 &amp; \\dots &amp; u_k \\end{bmatrix}.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week13/#choosing-k-dimension-of-the-subspace","title":"Choosing K (Dimension of the Subspace)","text":"<p>Consider the sample covariance matrix:</p> \\[ S=\\frac{1}{n} \\sum_{i=1}^n\\left(x^{(i)}-\\bar{x}\\right)\\left(x^{(i)}-\\bar{x}\\right)^T. \\] <p>If we run an eigendecomposition on \\(S=V D^2 V^T\\), we have the following:</p> <ol> <li> <p>\\(V=\\left[\\begin{array}{lll}v_1 &amp; \\ldots &amp; v_p\\end{array}\\right]\\), where each \\(v_i\\) is both a PC direction and an eigenvector of \\(S\\).</p> </li> <li> <p>The reconstruction error upon using a \\(k\\)-dimensional PCA projection is \\(\\displaystyle \\sum_{i=k+1}^p d_{i i}^2\\), where \\(d_{i i}\\) are the eigenvalues of \\(S\\).</p> </li> </ol> <p>To find a reasonable dimension \\(k\\), we can form an elbow plot of the eigenvalues of \\(S\\), and pick the \\(k\\) corresponding with the sharpest elbow.</p> <p>We can also consider plotting the quantities \\(\\frac{\\lambda_i}{\\sum \\lambda_i}\\), where \\(\\lambda_i\\) is an eigenvalue of \\(S\\) and choosing \\(k\\) where there is an elbow in the plot once again.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week13/#find-the-affine-subspace-find-the-optimal-v","title":"*Find the Affine Subspace: Find the Optimal \\(V\\)","text":"<p>Recall that when finding the optimal vector subspace, \\(V\\), we need to minimize</p> \\[ \\operatorname*{arg\\, min}_{V} {\\color{red} \\sum_{i=1}^{n}\\left(x^{(i)}-\\bar{x}\\right)^T\\left(I-V V^T\\right)^2\\left(x^{(i)}-\\bar{x}\\right)}. \\] <p>With centered matrix \\(X_c\\) and its SVD, \\(X_c = UD\\tilde{V}\\), the above minimization is equivalent to </p> \\[ \\operatorname*{arg min}_{V}\\, \\operatorname{tr}\\left(X_c\\left(I-V V^T\\right)^2 X_c^T\\right). \\] <p>Recall that we assume \\(V\\) has orthogonal columns. We have </p> \\[ \\left(I-V V^T\\right)^2 = I - 2V V^T + V (V^TV) V^T = I - V V^T. \\] <p>Thus, by cyclicity of trace, we have </p> \\[ \\begin{aligned} \\operatorname*{arg min}_{V}\\,  \\operatorname{tr}\\left(X_c\\left(I-V V^T\\right)^2 X_c^T\\right)  &amp;= \\operatorname*{arg min}_{V}\\, \\operatorname{tr}\\left(\\left(I-V V^T\\right) X_c^T X_c\\right) \\\\ &amp;= \\operatorname*{arg max}_{V}\\, \\operatorname{tr}\\left(V V^T X_c^T X_c\\right) \\\\ &amp;= \\operatorname*{arg max}_{V}\\, \\operatorname{tr}\\left(V^T SV\\right) \\\\ &amp;= \\operatorname*{arg max}_{V}\\, \\sum_{i=1}^k v_i^T Sv_i,  \\end{aligned} \\] <p>where we denote \\(S = X_c^T X_c\\) and \\(V = \\begin{bmatrix}v_1, v_2, \\dots, v_k\\end{bmatrix}\\). </p> <p>Start with the ratio </p> \\[ r(\\boldsymbol{x})=\\frac{\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x}}{\\boldsymbol{x}^{\\mathrm{T}} \\boldsymbol{x}}, \\quad \\forall \\boldsymbol{x}\\in \\mathbb{R}^n\\backslash \\{\\boldsymbol{0}\\}. \\] <p>This is called the Rayleigh quotient. To maximize \\(r(\\boldsymbol{x})\\), set its partial derivatives to zero, i.e. \\(\\nabla r=\\boldsymbol{0}\\). Those derivatives are messy(1) and here is the result -- one vector equation for the winning \\(\\boldsymbol{x}\\):</p> <ol> <li>Apply matrix calculus \\(\\nabla_x \\left(x^TAx\\right) = 2Ax\\) or \\(\\nabla_{x^T} \\left(x^TAx\\right) = 2x^TA\\). </li> </ol> \\[  \\quad S \\boldsymbol{x}=r(\\boldsymbol{x}) \\boldsymbol{x}. \\] <p>So the winning \\(\\boldsymbol{x}\\) is an eigenvector of \\(S\\). The maximum ratio \\(r(\\boldsymbol{x})\\) is the largest eigenvalue of \\(S\\). As \\(V\\) has orthogonal columns and \\(d_{11}^2,\\dots,d_{kk}^2\\) are the first \\(k\\) largest eigenvalues of \\(S\\)(1), we have </p> <ol> <li>See the discussion about eigenvalues of \\(A^TA\\) and \\(AA^T\\). </li> </ol> \\[ \\max_{V}\\, \\operatorname{tr}\\left(V^T SV\\right) = \\max_{V}\\, \\sum_{i=1}^k \\frac{v_i^T Sv_i}{v_i^T v_i} = \\sum_{i=1}^k d_{ii}^2,  \\] <p>with \\(V\\) is given by the first \\(k\\) columns of the SVD matrix \\(\\tilde{V}\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/","title":"Week 2","text":""},{"location":"notes/lecture_notes/stat541/stat541_week2/#a-useful-technique","title":"A Useful Technique","text":"<p>Recall that the oracle predictor is given by</p> \\[ E(y|x) = \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y, \\] <p>which is a function of \\(x\\), denoted as \\(f^*(x)\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#key-property","title":"Key Property","text":"<p>We will prove the following equation</p> \\[ E_{(x,y)}\\left(g(x)\\cdot E(y|x)\\right) = E_{(x,y)}\\left(g(x)\\cdot y\\right), \\] <p>which holds for all functions \\(g\\) where \\(Var\\left(g(x)\\right)&lt;+\\infty\\)(1). Specifically, when taking \\(g(x)\\equiv 1\\), we obtain the law of total expectation, i.e. \\(E\\left(y\\right) = E\\left(E(y|x)\\right)\\)(2).</p> <ol> <li>This a technical assumption which is usually satisfied in practical problems.</li> <li>One special case states that if \\(\\left\\{A_i\\right\\}\\) is a finite or countable partition of the sample space, then \\(E(X)=\\sum_i E\\left(X | A_i\\right) \\cdot Pr\\left(A_i\\right)\\).</li> </ol> <p>Obviously, it is equivalent to</p> \\[ E_{(x,y)}\\left(g(x)\\left(y-E(y|x)\\right)\\right) = 0, \\] <p>which is an orthogonality property.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#proof","title":"Proof","text":"\\[ \\begin{aligned} E_(x,y)\\left(g(x)\\cdot E(y|x)\\right)  &amp;= E_x\\left(g(x)\\cdot E(y|x)\\right) \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot E(y|x) p(x) \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}}g(x)\\cdot p(x) \\int_{\\mathcal{Y}} y\\cdot p(y|x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} g(x)\\cdot y \\cdot p(y|x) p(x) \\,\\mathrm{d}y \\,\\mathrm{d}x \\\\ &amp;= E_{(x,y)}\\left(g(x)\\cdot y\\right), \\end{aligned} \\] <p>where the last equation is from \\(p(y|x) p(x) = p(x,y)\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#reprove-the-oracle-predictor","title":"Reprove the Oracle Predictor","text":"<p>Let us show again that \\(E(y|x)\\) is the oracle predictor under the squared error loss:</p> \\[ \\begin{aligned} R(f,P)  &amp;= E_{(x,y)}\\left(\\left(f(x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)+E(y|x)-y\\right)^2\\right) \\\\ &amp;= E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right) + 2E_{(x,y)}\\left({\\color{red} \\left(f(x)-E(y|x)\\right)}\\left(E(y|x)-y\\right)\\right) \\\\  &amp;\\quad + E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right) \\\\ &amp;= {\\color{green} E_{(x,y)}\\left(\\left(f(x)-E(y|x)\\right)^2\\right)} + {\\color{blue} E_{(x,y)}\\left(\\left(E(y|x)-y\\right)^2\\right)}, \\end{aligned} \\] <p>where the last equation is from the equivalent form of the Key Property regarding the above \\(\\color{red}{\\text{red}}\\) term as a function of \\(x\\). Note that both the \\(\\color{green}{\\text{green}}\\) and \\(\\color{blue}{\\text{blue}}\\) term are non-negative, and the \\(\\color{blue}{\\text{blue}}\\) term is independent of \\(f\\). Therefore, to minimize \\(R(f,P)\\), it is sufficient to minimize the \\(\\color{green}{\\text{green}}\\) term, and the minimizer \\(f^*(x) = E(y|x)\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#geometric-illustration","title":"Geometric Illustration","text":"<p>Pythagorean Decomposition of \\(R(f,P)\\): \\(E(y|x)\\) can be viewed as an orthogonal projection of \\(y\\) on the space of random variables that are functions of \\(x\\). </p> <p>To illustrate this decomposition, consider the space of all r.v. with finite variance(1) and the set of all r.v. that are functions of \\(x\\) becomes a hyperplane lying in this space. Assume \\(y\\) cannot simply written as certain function of \\(x\\). The Pythagorean decomposition of \\(R(f,P)\\) is shown in the following figure. </p> <ol> <li>This space is a \\(L^2\\) space. </li> </ol> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#an-example-of-decomposition","title":"An Example of Decomposition","text":"<p>If taking \\(f(x)=E(y)\\), a constant, in the decomposition of the prediction error, we have</p> \\[ \\begin{aligned}     Var(y)     &amp;= E_{(x,y)}\\left(\\left(y-E(y)\\right)^2\\right) \\\\     &amp;= E_{(x,y)}\\left(\\left(y-E(y|x)\\right)^2\\right) + E_{(x,y)}\\left(\\left(E(y|x)-E(y)\\right)^2\\right) \\\\     &amp;= Var\\left(E(y|x)\\right) + E\\left(Var(y|x)\\right).  \\end{aligned} \\] <p>The last equation is because \\(E\\left(E(y|x)\\right) = E(y)\\), by Key Property. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>Denote the data set as \\(\\mathcal{D}_n\\) and the model based on learning algorithm \\(\\phi_n\\) and this data set as \\(\\hat{f}_{\\mathcal{D}_n}(x)\\) (1). Consider the expected risk of the learning algorithm \\(\\phi_n\\), denoted as \\(\\mathcal{R}\\), </p> <ol> <li>The model is the function \\(f\\) we are looking for, which uses features to predict outputs. For details, see Learning Algorithm. </li> </ol> \\[ \\begin{aligned}     \\mathcal{R}(\\phi_n,P)     &amp;= E_{\\mathcal{D}_n}\\left(R\\left(\\hat{f}_{\\mathcal{D}_n},P\\right)\\right) \\\\     &amp;= E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - y\\right)^2\\right)\\right) \\\\     &amp;= {\\color{red} E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2\\right)\\right)} + {\\color{green} E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right)},  \\end{aligned} \\] <p>where the \\({\\color{green} \\text{green}}\\) term is the oracle predict error \\(R^*(P)\\) and we will focus on the \\({\\color{red} \\text{red}}\\) term. </p> <p>We can apply Fubinis's theorem to the \\({\\color{red} \\text{red}}\\) term. To illustrate this, we consider the following integral: </p> \\[ \\begin{aligned}     &amp;E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = \\left(\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}}\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}}\\cdots\\right) \\left(\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}} \\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2 p(x,y)\\,\\mathrm{d}x\\mathrm{d}y\\right) \\prod_{i=1}^{n}p(x^{(i)},y^{(i)})\\,\\mathrm{d}x^{(1)}\\mathrm{d}y^{(1)}\\cdots\\mathrm{d}x^{(n)}\\mathrm{d}y^{(n)},   \\end{aligned} \\] <p>where we can exchange the integrals by Fubinis's theorem and finally get the integral of \\(x,y\\) to be the most outside integral. Then, we have </p> \\[ \\begin{aligned}     &amp; E_{\\mathcal{D}_n}\\left(E_{(x,y)}\\left(\\left(y - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E(y|x)\\right)^2\\right)\\right)\\\\     &amp;\\quad = E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(\\hat{f}_{\\mathcal{D}_n} - E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\right)^2\\right)\\right) + E_{(x,y)}\\left(E_{\\mathcal{D}_n}\\left(\\left(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right) - E(y|x)\\right)^2\\right)\\right) \\\\     &amp;\\quad = {\\color{blue} E_{(x,y)}\\left(Var_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}(x)\\right)\\right)} + {\\color{gray} E_{(x,y)}\\left(\\left(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right) - E(y|x)\\right)^2\\right)},  \\end{aligned} \\] <p>where the two terms are \\({\\color{blue} \\text{variance term}}\\) (at a fixed \\(x\\)) and \\({\\color{gray} \\text{bias term}}\\), respectively. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week2/#illustrations-by-polynomial-fitting","title":"Illustrations by Polynomial Fitting","text":"<p>To illustrate this, if the oracle predictor \\(E(y|x)\\) is some 3-degree polynomial, assume we fit data sets via linear functions and 3-degree polynomials. For the linear learning algorithms, since \\(E(y|x)\\) is some 3-degree polynomial and \\(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\) is some linear function, the \\({\\color{gray} \\text{bias term}}\\) is likely to be large; on the other hand, \\({\\color{blue} \\text{variance term}}\\) may not be too large, for linear functions vary slightly compared among linear functions. This 'small variance' is shown in the following figure,    where the gray thick curve is \\(E(y|x)\\), and  colored curves are different prediction functions based on different data sets, and dashed curve is \\(E_{\\mathcal{D}_n}\\left(\\hat{f}_{\\mathcal{D}_n}\\right)\\). Conversely, for 3-degree-polynomial learning algorithms, the \\({\\color{gray} \\text{bias term}}\\) is likely to be small and \\({\\color{blue} \\text{variance term}}\\) may be large. This 'large variance' is shown in the following figure.  </p> <p>To better understand this, we can consider an extreme example. If we use very high degree polynomials to fit data sets, we can make our prediction function exactly go through each training data point. In this case, the bias would be small(1). However, higher degree polynomials can oscillate more freely than lower degree polynomials, and this usually leads to our prediction function shaping drastically different to each other when coming from different data sets. </p> <ol> <li>The empirical risk is 0 in this case. </li> </ol> <p>Even if \\(E(y\\mid x)\\) is a degree \\(d\\) polynomial. It can still be better (for small to medium(1) \\(n\\)) to fit a lower degree polynomial. For lager \\(n\\), we can consider more complex models.</p> <ol> <li>Usually, we say \\(n\\leq 50\\) is small, \\(50&lt;n\\leq 100\\) is medium, and \\(n&gt;100\\) is large, but there is actually no formal standard. The best way to select a model is to compare different models using the cross-validation method. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week3/","title":"Week3","text":""},{"location":"notes/lecture_notes/stat541/stat541_week3/#recap-on-singular-value-decomposition","title":"Recap on Singular Value Decomposition","text":""},{"location":"notes/lecture_notes/stat541/stat541_week3/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>Let \\(V\\in \\mathbb{R}^{p\\times k}\\), given by</p> \\[ V = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_k \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}. \\] <p>Then \\(V\\) has orthogonal columns if</p> \\[ v_i^T v_j =\\left\\{\\begin{matrix}     &amp;  1, \\quad \\text{ if } i=j, \\\\     &amp;  0, \\quad \\text{ if } i\\neq j.  \\end{matrix}\\right. \\] <p>For such \\(V\\), we have \\(V^T V = I_k\\). Note that it is often NOT the case that \\(V V^T = I_p\\).</p> <p>A square matrix \\(V\\in \\mathbb{R}^{k\\times k}\\) with orthogonal columns is called an orthogonal matrix. Note that \\(V^{-1} = V^T\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#thin-svd","title":"Thin SVD","text":"<p>For a matrix \\(A\\in \\mathbb{R}^{m\\times n}\\) (\\(m\\geq n\\)), whose rank is \\(r\\).The thin SVD of \\(A\\) is a representation of the form</p> \\[ A = U \\Sigma V^{T}, \\quad U\\in \\mathbb{R}^{m\\times n}, \\Sigma\\in \\mathbb{R}^{n\\times n}, V\\in \\mathbb{R}^{n\\times n}, \\] <p>where \\(U,V\\) have normal orthogonal columns, and \\(\\Sigma\\) is diagonal with the first \\(r\\) values \\(\\sigma_1 \\geq \\dots \\geq \\sigma_r&gt;0\\), where \\(\\sigma_i\\) is the length of \\(A\\boldsymbol{v}_i\\). </p> <p>The column vectors of \\(U\\) and \\(V\\) are respectively called the left and right singular vectors. The singular values of \\(A\\) are the diagonal elements of \\(\\Sigma\\).</p> <p>Remarkable results: Every matrix \\(A\\in \\mathbb{R}^{m\\times n}\\) has an SVD.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#general-svd","title":"General SVD","text":"<p>We consider the singular vectors: the \\(\\boldsymbol{u}\\)'s and \\(\\boldsymbol{v}\\)'s give bases for the four fundamental subspaces:</p> <ul> <li>\\(\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_r \\quad\\) is an orthonormal basis for the column space</li> <li>\\(\\boldsymbol{u}_{r+1}, \\dots, \\boldsymbol{u}_m\\) is an orthonormal basis for the left nullspace \\(\\boldsymbol{N}\\left(A^T\\right)\\)</li> <li>\\(\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_r \\quad\\) is an orthonormal basis for the row space</li> <li>\\(\\boldsymbol{v}_{r+1}, \\dots, \\boldsymbol{v}_n\\) is an orthonormal basis for the nullspace \\(\\boldsymbol{N}(A)\\).</li> </ul> <p>More than just orthogonality, these basis vectors diagonalize the matrix \\(A\\) :</p> \\[ A \\boldsymbol{v}_1=\\sigma_1 \\boldsymbol{u}_1,\\quad A \\boldsymbol{v}_2=\\sigma_2 \\boldsymbol{u}_2,\\quad \\dots,\\quad A \\boldsymbol{v}_r=\\sigma_r \\boldsymbol{u}_r. \\] <p>Those singular values \\(\\sigma_1\\) to \\(\\sigma_r\\) will be positive numbers: \\(\\sigma_i\\) is the length of \\(A \\boldsymbol{v}_i\\). The \\(\\sigma\\)'s go into a diagonal matrix that is otherwise zero. That matrix is \\(\\Sigma\\). </p> <p>Then the equations \\(A \\boldsymbol{v}_i=\\sigma_i \\boldsymbol{u}_i\\) tell us column by column that \\(\\boldsymbol{A} \\boldsymbol{V}_{\\boldsymbol{r}}=\\boldsymbol{U}_{\\boldsymbol{r}} \\boldsymbol{\\Sigma}_{\\boldsymbol{r}}\\) :</p> \\[ A\\begin{bmatrix}\\boldsymbol{v}_1 \\cdots \\boldsymbol{v}_r\\end{bmatrix}=\\begin{bmatrix} \\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_r \\end{bmatrix}\\begin{bmatrix} \\sigma_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\end{bmatrix}. \\] <p>Those \\(\\boldsymbol{v}\\)'s and \\(\\boldsymbol{u}\\)'s account for the row space and column space of \\(A\\). We have \\(n-r\\) more \\(\\boldsymbol{v}\\)'s and \\(m-r\\) more \\(\\boldsymbol{u}\\)'s, from the nullspace \\(\\boldsymbol{N}(A)\\) and the left nullspace \\(\\boldsymbol{N}\\left(A^T\\right)\\). They are automatically orthogonal to the first \\(\\boldsymbol{v}\\)'s and \\(\\boldsymbol{u}\\)'s. We now include all the \\(\\boldsymbol{v}\\)'s and \\(\\boldsymbol{u}\\)'s in \\(V\\) and \\(U\\), so these matrices become square. We still have \\(\\boldsymbol{A} \\boldsymbol{V}=\\boldsymbol{U} \\boldsymbol{\\Sigma}\\):</p> \\[ A\\begin{bmatrix}\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_r, \\dots, \\boldsymbol{v}_n\\end{bmatrix}=\\begin{bmatrix}\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_r, \\dots, \\boldsymbol{u}_m\\end{bmatrix} \\begin{bmatrix} \\Sigma_r &amp; &amp; &amp; \\\\ &amp; 0 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp;  \\end{bmatrix}. \\] <p>The new \\(\\Sigma\\) is \\(m\\times n\\). It is just the \\(r\\times r\\) matrix \\(\\Sigma_r\\) with \\(m-r\\) extra zero rows and \\(n-r\\) new zero columns. The real change is in the shapes of \\(U\\) and \\(V\\). Those are square orthogonal matrices. So \\(A V=U \\Sigma\\) can become \\(\\boldsymbol{A}=\\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{\\mathbf{T}}\\). This is the Singular Value Decomposition. I can multiply columns \\(\\boldsymbol{u}_i \\sigma_i\\) from \\(U \\Sigma\\) by rows of \\(V^{T}\\):</p> \\[ A=U \\Sigma V^{T}=u_1 \\sigma_1 v_1^{T}+\\cdots+u_r \\sigma_r v_r^{T}. \\] <p>Each \\(\\sigma_i^2\\) is an eigenvalue of \\(A^T A\\)(1) and also \\(A A^{T}\\)(2). When we put the singular values in descending order(3), \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\sigma_r&gt;0\\), the above splitting gives the \\(r\\) rank-one pieces of \\(A\\) in order of importance. Then \\(\\sigma_1\\) is the maximum of the ratio: </p> <ol> <li>with \\(\\boldsymbol{v}\\)'s as orthonormal eigenvectors</li> <li>with \\(\\boldsymbol{u}\\)'s as orthonormal eigenvectors</li> <li>For a matrix \\(A\\in \\mathbb{R}^{m\\times n}\\), we can define a column swapping matrix \\(P_{i,j}\\in \\mathbb{R}^{n\\times n}\\), which swaps the \\(i\\)-th column and \\(j\\)-th column of \\(A\\) by right multiplication, i.e. \\(AP_{i,j}\\). Using \\(P_{i,j}\\), we can swap the positions of \\(i\\)-th singular value and \\(j\\)-th singular value of \\(\\Sigma\\): \\(U\\Sigma V^T = \\left(UP_{i,j}\\right) \\left(P_{i,j}^T \\Sigma P_{i,j}\\right) \\left(V P_{i,j}\\right)^T\\).  The column swapping matrix \\(P_{i,j}\\) has the following properties: (let \\(A\\) have rows \\(v_i\\), \\(i=1,\\dots, m\\) and columns \\(w_j\\), \\(j=1,\\dots,n\\))<ul> <li>\\(AP_{i,j}\\): \\(v_i \\leftrightarrow v_j\\). </li> <li>\\(P_{i,j}^T A = \\left(A^TP_{i,j}\\right)^T\\): \\(w_i \\leftrightarrow w_j\\). </li> <li>\\(P_{i,j}^T P_{i,j} = I\\). </li> <li>\\(P_{i,j} P_{i,j}^T = P_{i,j}^T P_{i,j} P_{i,j} P_{i,j}^T = I\\). </li> </ul> </li> </ol> \\[ \\max_{\\|x\\| = 1}\\frac{\\|A \\boldsymbol{x}\\|}{\\|\\boldsymbol{x}\\|}. \\]"},{"location":"notes/lecture_notes/stat541/stat541_week3/#spectral-decomposition","title":"Spectral Decomposition","text":"<p>A spectral decomposition of a symmetric matrix \\(A\\in \\mathbb{R}^{n\\times n}\\) is a representation of \\(A\\) as</p> \\[ A = VDV^T, \\quad V\\in \\mathbb{R}^{n\\times n}, D\\in \\mathbb{R}^{n\\times n}, \\] <p>where \\(V = \\begin{bmatrix}v_1, v_2, \\dots, v_n \\end{bmatrix}\\) is orthogonal and \\(D = {\\rm diag}\\{d_1,d_2,\\dots,d_n\\}\\) is diagonal.</p> <p>The columns of \\(V\\) are the eigenvector of \\(A\\) and \\(d_i\\) is the associated eigenvalue:</p> \\[ Av_i = VDV^Tv_i = d_i v_i. \\] <p>Remarkable results (spectral theorem): Every symmetric matrix \\(A\\in \\mathbb{R}^{n\\times n}\\) has a spectral decomposition.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#recap-on-multivariate-statistics","title":"Recap on Multivariate Statistics","text":""},{"location":"notes/lecture_notes/stat541/stat541_week3/#notations","title":"Notations","text":"<p>Let \\(\\boldsymbol{X}\\) be a \\(k\\)-dimensional random vector (a special case of random matrices)</p> \\[ \\boldsymbol{X}= \\begin{bmatrix}  X_1 \\\\ \\vdots \\\\ X_k \\end{bmatrix}, \\] <p>or a \\(p\\times q\\) random matrix</p> \\[ \\boldsymbol{X}= \\begin{bmatrix} X_{11} &amp;\\cdots &amp; X_{1q} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ X_{p1} &amp; \\cdots &amp; X_{pq} \\end{bmatrix}, \\] <p>where each \\(X_i\\in\\) or \\(X_{ij}\\) is a random variable. \\(E(\\boldsymbol{X})\\) is defined componentwise, i.e.</p> \\[ E(\\boldsymbol{X})= \\begin{bmatrix} E(X_{11}) &amp;\\cdots &amp; E(X_{1q}) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ E(X_{p1}) &amp; \\cdots &amp; E(X_{pq}) \\end{bmatrix}, \\] <p>and \\(E(\\boldsymbol{X})\\) has linearity: for constant matrices \\(A\\in\\mathbb{R}^{d\\times p}\\), $B\\in\\mathbb{R}^{q\\times q}, \\(C\\in\\mathbb{R}^{d\\times q}\\),</p> \\[ E(A\\boldsymbol{X}B + C) = AE(\\boldsymbol{X})B + C. \\] <p>For random vectors \\(\\boldsymbol{X}\\in \\mathbb{R}^p\\), its covariance matrix is defined as the \\(p\\times p\\) symmetric matrix \\(Cov(\\boldsymbol{X})\\), given by</p> \\[ [Cov(\\boldsymbol{X})]_{ij} = Cov(X_i,X_j). \\] <p>Similar to the covariance of random variable, we have</p> \\[ \\begin{aligned}     Cov(\\boldsymbol{X})      &amp;= E\\left(\\left(\\boldsymbol{X}-E(\\boldsymbol{X})\\right)\\left(\\boldsymbol{X}-E(\\boldsymbol{X})\\right)^T\\right) \\\\     &amp;= E(\\boldsymbol{X} \\boldsymbol{X}^T) - E(\\boldsymbol{X})E(\\boldsymbol{X})^T.   \\end{aligned} \\] <p>From the above, we have</p> \\[ Cov(A\\boldsymbol{X}) = A \\cdot Cov(X) \\cdot A^T. \\]"},{"location":"notes/lecture_notes/stat541/stat541_week3/#multivariate-normal-distribution","title":"Multivariate Normal Distribution","text":"<p>The multivariate normal distribution of a \\(k\\)-dimensional random vector \\(\\boldsymbol{X}=\\left(X_1, \\ldots, X_k\\right)^{T}\\) can be written in the following notation:</p> \\[ \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\quad \\text{ or } \\quad \\boldsymbol{X} \\sim \\mathcal{N}_k(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), \\] <p>with \\(k\\)-dimensional mean vector</p> \\[ \\boldsymbol{\\mu}=E(\\boldsymbol{X})=\\begin{bmatrix} E\\left(X_1\\right) \\\\ E\\left(X_2\\right)  \\\\ \\vdots \\\\ E\\left(X_k\\right) \\end{bmatrix} \\] <p>and \\(k \\times k\\) covariance matrix</p> \\[ \\Sigma_{i, j}=E\\left(\\left(X_i-\\mu_i\\right)\\left(X_j-\\mu_j\\right)\\right)=Cov\\left(X_i, X_j\\right), \\quad \\forall i,j = 1,\\dots, k. \\] <p>\\(\\boldsymbol{\\Sigma}\\) is assumed to be positive definite (i.e. non-degenerate) and therefore, \\(\\boldsymbol{\\Sigma}^{-1}\\) is also positive definite. In this case, the density of \\(\\boldsymbol{X}\\) is given by</p> \\[ p(\\boldsymbol{z}) = \\frac{1}{\\sqrt{(2\\pi)^k\\det(\\boldsymbol{\\Sigma})}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{z}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{z}-\\boldsymbol{\\mu})\\right). \\] <p>Fact: for a full-rank matrix \\(A\\in \\mathbb{R}^{p\\times q}\\) (\\(q\\leq p\\)) and \\(b\\in\\mathbb{R}^q\\), we have</p> \\[ A\\boldsymbol{X} + b\\sim \\mathcal{N}_k(A\\boldsymbol{\\mu}+b, A\\boldsymbol{\\Sigma}A^T). \\]"},{"location":"notes/lecture_notes/stat541/stat541_week3/#linear-regression","title":"Linear Regression","text":"<p>The basic idea of linear regression is to assume that</p> \\[ y \\approx \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i, \\] <p>where \\(\\boldsymbol{X} = [x_1, x_2, \\dots, x_p]^T\\) (for now we assume \\(\\boldsymbol{x}\\in \\mathbb{R}^p\\)). More precisely, we make a assumption about \\(p(y\\mid\\boldsymbol{x})\\) as follows:</p> \\[ y = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i + \\varepsilon, \\] <p>where \\(\\varepsilon\\) is noise with \\(\\varepsilon \\sim \\mathcal{N}({0,\\sigma^2})\\) and \\(\\boldsymbol{x}\\) is independent of the noise. Thus, we have</p> \\[ y\\mid \\boldsymbol{x} \\sim \\mathcal{N}(\\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i, \\sigma^2). \\] <p>We know that (under squared error loss) the oracle predictor is</p> \\[ E(y\\mid\\boldsymbol{x}) = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x_i = f^*(\\boldsymbol{x}). \\] <p>The goal is to find \\(\\beta_0,\\dots,\\beta_p\\) and thus \\(f^*\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#rewrite-training-data-in-matrix-form","title":"Rewrite Training Data in Matrix Form","text":"<p>Assume training data \\(\\{(\\boldsymbol{x}^{(i)},y^{(i)})\\}_{i=1}^n\\) were i.i.d. generated via the assumption about \\(p(y\\mid\\boldsymbol{x})\\). To simplify the notations, we define</p> \\[ \\boldsymbol{Y} = \\begin{bmatrix}  y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{bmatrix}, \\quad  \\boldsymbol{\\varepsilon} = \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\vdots  \\\\  \\varepsilon^{(n)} \\end{bmatrix}, \\quad  \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta^{(0)} \\\\ \\vdots \\\\ \\beta^{(n)} \\end{bmatrix}, \\] <p>and design matrix \\(\\boldsymbol{X}\\in \\mathbb{R}^{n\\times(p+1)}\\) given by</p> \\[ \\boldsymbol{X} = \\begin{bmatrix} 1 &amp; \\left(\\boldsymbol{x}^{(1)}\\right)^T \\\\ 1 &amp; \\left(\\boldsymbol{x}^{(2)}\\right)^T \\\\ \\qquad \\vdots \\\\ 1 &amp; \\left(\\boldsymbol{x}^{(n)}\\right)^T  \\end{bmatrix}. \\] <p>Then training data can written as</p> \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}. \\] <p>Now we want to use training data to estimate \\(\\beta_0,\\dots,\\beta_p\\) and thus \\(f^*\\).</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#recap-on-likelihood-function","title":"Recap on Likelihood Function","text":"<p>The likelihood function \\(L(\\theta\\mid \\boldsymbol{x})\\) illustrates the probability of \\(\\theta\\) given data set \\(\\boldsymbol{x} = (x_1,\\dots,x_n)^T\\).</p> <p>More precisely, assume we have \\(n\\) samples \\((x_1,\\dots,x_n)\\) observed from a distribution \\(p_{\\theta}(x)\\) with an unknown parameter \\(\\theta\\), and our goal is to estimate \\(\\theta\\) using the observed data.</p> <p>We view the observed samples are realizations of some random variables \\(X_1, X_2, \\dots, X_n\\), which has a joint density function \\(p\\left(X_1, \\dots, X_n \\mid \\theta\\right)\\). Given \\(X_1=x_1, \\dots, X_n=x_n\\), we may consider the probability of this event being observed, which is the likelihood function (a function of \\(\\theta\\)) defined by:</p> \\[ L(\\theta)=L\\left(\\theta \\mid x_1, \\dots, x_n\\right)=p\\left(X_1=x_1, \\dots, X_n=x_n \\mid \\theta\\right). \\] <p>Note that the likelihood function is NOT a probability density function. It measures the support provided by the data for each possible value of the parameter. If we compare the likelihood function at two parameter points and find that</p> \\[ L\\left(\\theta_1 \\mid \\boldsymbol{x}\\right)&gt;L\\left(\\theta_2 \\mid \\boldsymbol{x}\\right) \\] <p>then the sample we actually observed is more likely to have occurred if \\(\\theta=\\theta_1\\) than if \\(\\theta=\\theta_2\\). This can be interpreted as \\(\\theta_1\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_2\\). Therefore, one approach to estimate \\(\\theta\\) is to choose the value of \\(\\theta\\) which gives you the highest probability among all possible values.</p> <p>With the assumption that samples are drawn i.i.d., the joint probability is given by multiplied probabilities, i.e.</p> \\[ p\\left(X_1, \\dots, X_n \\mid \\theta\\right) = \\prod_{i=1}^n p_\\theta\\left(X_i\\right), \\] <p>hence taking the log transforms this into a summation, which is usually easier to maximize analytically. Thus, we often write the log-likelihood rather than the likelihood.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#estimate-the-coefficients-beta_i","title":"Estimate the Coefficients \\(\\beta_i\\)","text":"<p>Here we find an estimate of \\(\\boldsymbol{\\beta}\\) by maximum likelihood estimation:</p> <p>By \\(\\boldsymbol{X}\\) is independent of the noise (and vice versa), we have</p> \\[ \\boldsymbol{Y} \\mid \\boldsymbol{X} \\sim \\mathcal{N}_{n}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\sigma^2I_n), \\] <p>and thus, the (log-)likelihood function is given by the density function of the above multivariate normal distribution.</p> <p>By maximizing the likelihood function, we can estimate \\(\\boldsymbol{\\beta}\\):</p> \\[ \\begin{aligned} \\hat{\\boldsymbol{\\beta}} &amp;= \\operatorname*{arg\\, max}_\\beta \\ln(p(\\boldsymbol{Y} \\mid \\boldsymbol{X})) \\\\ &amp;= \\operatorname*{arg\\, max}_\\beta \\left(\\frac{1}{2\\sigma^2} (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta})^T (\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta})\\right) \\\\ &amp;= \\operatorname*{arg\\, max}_\\beta \\|\\boldsymbol{Y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\|^2, \\end{aligned} \\] <p>which becomes a least squares problem. We take a gradient with respect to \\(\\boldsymbol{\\beta}\\) and set the gradient for 0, and after solving for \\(\\boldsymbol{\\beta}\\), we have</p> \\[ \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}, \\] <p>where we assume \\(\\boldsymbol{X}\\) is a full-rank matrix and thus \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) has an inverse(1).</p> <ol> <li>Otherwise, if \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) is not invertible, there exists \\(v\\neq \\boldsymbol{0}\\in \\mathbb{R}^{p+1}\\) such that \\(\\boldsymbol{X}^T \\boldsymbol{X} v = \\boldsymbol{0}\\), which indicates columns of \\(\\boldsymbol{X}\\) are linearly dependent. And columns of \\(\\boldsymbol{X}\\) are always linearly dependent when \\(p\\geq n\\).</li> </ol> <p>Furthermore, we can find the distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) conditional on the training features \\(\\boldsymbol{X}\\):</p> \\[ E(\\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X}) = E\\left(\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}\\mid \\boldsymbol{X}\\right) = \\boldsymbol{\\beta}, \\] <p>which indicates that \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased;</p> \\[ \\begin{aligned} Cov(\\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X})  &amp;= Cov\\left(\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}\\mid \\boldsymbol{X}\\right) \\\\ &amp;= \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\cdot Cov\\left(\\boldsymbol{Y}\\mid \\boldsymbol{X}\\right) \\cdot \\boldsymbol{X} \\left(\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1}\\right)^T \\\\ &amp;= \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}; \\end{aligned} \\] <p>and therefore,</p> \\[ \\hat{\\boldsymbol{\\beta}}\\mid \\boldsymbol{X} \\sim \\mathcal{N}_{p+1}(\\boldsymbol{\\beta}, \\sigma^2(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}). \\]"},{"location":"notes/lecture_notes/stat541/stat541_week3/#evaluate-ols-prediction-estimate","title":"Evaluate OLS Prediction Estimate","text":"<p>The above estimation is called ordinary least squares (OLS) in statistics. Now we evaluate our estimation when receiving a new data pair \\((\\boldsymbol{x_{*}},y_*)\\). We make our prediction by</p> \\[ \\hat{f}(\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T \\hat{\\boldsymbol{\\beta}}, \\quad \\tilde{\\boldsymbol{x}} = \\begin{bmatrix} 1 \\\\ \\boldsymbol{x_{*}} \\end{bmatrix}. \\] <p>Consider the bias of \\(\\hat{f}\\) (conditional on \\(\\boldsymbol{X},\\boldsymbol{x_{*}}\\)),</p> \\[ E(\\hat{f}(\\boldsymbol{x_{*}}) \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T E(\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}. \\] <p>We know that \\(E(y_* \\mid \\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T \\boldsymbol{\\beta}\\), and thus the OLS prediction is unbiased(1).</p> <ol> <li>This conclusion relied on the assumption that the data has a linear relationship. In practice, our model is only an approximation to the true distribution. So we will have bias.</li> </ol> <p>Now, we consider the variance:</p> \\[ Var(\\hat{f}(\\boldsymbol{x_{*}}) \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) = \\tilde{\\boldsymbol{x}}^T\\cdot Var(\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X},\\boldsymbol{x_{*}}) \\tilde{\\boldsymbol{x}} = \\sigma^2\\tilde{\\boldsymbol{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}. \\] <p>Roughly speaking, if \\(\\boldsymbol{x_{*}}\\) is similar to the training data features, the variance will be small; otherwise, the variance will be large.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#interval-estimate","title":"Interval Estimate","text":"<p>We want to make a prediction interval for \\(y_*\\) in the form of</p> \\[ \\left[C_{low}(\\boldsymbol{Y}, \\boldsymbol{X}, \\boldsymbol{x_{*}}),C_{high}(\\boldsymbol{Y}, \\boldsymbol{X}, \\boldsymbol{x_{*}})\\right]. \\] <p>We hope \\(y_*\\) to be contained in this with probability of at least \\(1-\\alpha\\) (usually \\(\\alpha = 0.05\\) or \\(\\alpha = 0.01\\)), i.e.</p> \\[ Pr\\left(y_*\\in \\left[C_{low},C_{high}\\right] \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) \\geq 1-\\alpha. \\] <p>Recall the assumption that \\(y_*\\mid \\boldsymbol{\\tilde{x}} \\sim \\mathcal{N}(\\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}}, \\sigma^2)\\). By</p> \\[ E(y_* - \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}) = 0, \\] <p>and \\(Cov\\left(y_*, \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right)=0\\)(1) and thus</p> <ol> <li>This is because \\(\\hat{\\boldsymbol{\\beta}}\\) only depends on the training data and we assume newly received data pair is independent of training data.</li> </ol> \\[ \\begin{aligned} &amp;Var(y_* - \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}) \\\\ &amp;= Var\\left(y_*\\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) - 2 Cov\\left(y_*, \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) + Var\\left(\\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) \\\\ &amp;= \\sigma^2 + \\sigma^2 \\boldsymbol{\\tilde{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}, \\end{aligned} \\] <p>we know that \\(y_* - \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}}\\) is still normally distributed, given by</p> \\[ y_* - \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}} \\sim \\mathcal{N}\\left(0, \\sigma^2\\left(1+\\boldsymbol{\\tilde{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}\\right)\\right). \\] <p>To obtain the interval estimation, we also need to get rid of \\(\\sigma^2\\) as it is unknown. Recall the assumption that the noise is normally distributed. We can estimate the variance \\(\\sigma^2\\) by the scaled residuals, i.e.</p> \\[ \\hat{\\sigma}^2 = \\frac{\\|\\boldsymbol{Y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}\\|^2}}{n-p}. \\] <p>Then we have</p> \\[ \\frac{y_*-\\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\hat{\\sigma}^2\\left(1+\\boldsymbol{\\tilde{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}\\right)}} \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}} \\] <p>follows a t-distribution with \\(n-p\\) degree of freedom. Let \\(t_{n-p,\\alpha/2}\\) and \\(t_{n-p,1-\\alpha/2}\\) be the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantile of the this distribution. Denote \\(\\sqrt{\\hat{\\sigma}^2\\left(1+\\boldsymbol{\\tilde{x}}^T(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}\\tilde{\\boldsymbol{x}}\\right)}\\) as \\(C\\). Then, we have</p> \\[ Pr\\left(\\frac{y_*-\\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}}}{C}\\in \\left[t_{n-p,\\alpha/2}, t_{n-p,1-\\alpha/2}\\right] \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) \\geq 1-\\alpha, \\] <p>that is</p> \\[ Pr\\left(y_*\\in \\left[C t_{n-p,\\alpha/2} + \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}},C t_{n-p,1-\\alpha/2} + \\boldsymbol{\\tilde{x}}^T\\hat{\\boldsymbol{\\beta}}\\right] \\mid \\boldsymbol{X}, \\boldsymbol{x_{*}}\\right) \\geq 1-\\alpha. \\]"},{"location":"notes/lecture_notes/stat541/stat541_week3/#examples-and-discrete-features","title":"Examples and Discrete Features","text":"<p>So far we assume \\(\\boldsymbol{x}\\) lies in \\(\\mathbb{R}^p\\). Here we will introduce discrete features and mixed features by giving some examples.</p> <p>Example Polynomial Regression (\\(\\mathcal{X} = \\mathbb{R}\\)):</p> <p>The model takes \\(x\\) and transforms it to a new feature</p> \\[ x \\mapsto (1,x,x^2,\\dots,x^d), \\] <p>and the model becomes</p> \\[ y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_d x^d + \\varepsilon, \\] <p>use this new feature in a regression. For instance, assume we have 4 data pairs for training and use a 2-degree polynomial in regression: training data is given by</p> \\[ \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)} \\\\  x^{(4)} \\end{bmatrix} =  \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\\\ 5 \\end{bmatrix}. \\] <p>We first map the following training data to the design matrix \\(\\boldsymbol{X}\\), given by</p> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 4 \\\\ 1 &amp; -1 &amp; 1 \\\\ 1 &amp; 3 &amp; 9 \\\\ 1 &amp; 5 &amp; 25 \\end{bmatrix}. \\] <p>The OLS estimate is still \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y}\\). This allows us to fit \\(y\\) wit ha non-linear function of \\(x\\).</p> <p>Example Categorical Feature:</p> <p>A categorical feature could be, for instance 3 categories as </p> \\[ x= \\left\\{\\begin{matrix} 1, \\quad &amp;\\text{ if Age} &lt;20, \\\\ 2, \\quad &amp;\\text{ if } 20\\leq\\text{Age}&lt;40, \\\\ 3, \\quad &amp;\\text{ if Age} \\geq 40.   \\end{matrix} \\right. \\] <p>We transform categorical \\(x\\)'s via indicator functions. </p> <p>First way is the dummy variable method: By choosing a baseline class ( say class '3'), we transform</p> \\[ x \\mapsto \\left(1,I(x=1),I(x=2)\\right), \\] <p>and the regression model becomes</p> \\[ y =  \\beta_0 + \\beta_1 I(x=1) + \\beta_2 I(x=2) + \\varepsilon, \\] <p>where we have \\(E(y\\mid x=3) = \\beta_0\\), \\(E(y\\mid x=1) = \\beta_0+\\beta_1\\), and \\(E(y\\mid x=2) = \\beta_0 + \\beta_2\\). </p> <p>Another way is more symetric -- we have the model</p> \\[ y =  \\beta_0 + \\beta_1 I(x=1) + \\beta_2 I(x=2) + \\beta_3 I(x=3) + \\varepsilon, \\] <p>where we have \\(E(y\\mid x=1) = \\beta_0 + \\beta_1\\), \\(E(y\\mid x=2) = \\beta_0+\\beta_2\\), and \\(E(y\\mid x=3) = \\beta_0 + \\beta_3\\). Since \\(\\beta_0\\) is redundant, it can be dropped. </p> <p>Assume we have training data: </p> \\[ \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)} \\\\ x^{(4)} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 1 \\end{bmatrix}, \\] <p>and the corresponding design matrix is </p> \\[ \\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{bmatrix}. \\] <p>Example Mixed Features</p> <p>We can combine continous and categorical features. For instance, if \\(x\\in \\{1,2,3\\}\\times\\mathbb{R}\\), we can do similar mapping:</p> \\[ x\\mapsto \\left(1,I(x_1=1),I(x_1=2),x_2,x_2^2\\right).  \\] <p>Assume the training data is </p> \\[ \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)}  \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 3.5\\\\ 2 &amp; 7 \\\\ 3 &amp; -2.1 \\end{bmatrix}, \\] <p>then the design matrix is given by</p> \\[ \\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 3.5 &amp; 12.25 \\\\ 1 &amp; 0 &amp; 1 &amp; 7 &amp; 49\\\\ 1 &amp; 1 &amp; 0 &amp; -2.1 &amp; 4.41 \\end{bmatrix}. \\] <p>Example Intersactions between Features</p> <p>We can also have intersactions between features. For instance, if \\(\\mathcal{X}\\in\\mathbb{R}^2\\), then we can consider thier multiplication in the mapping:</p> \\[ x\\mapsto \\left(1,x_1,x_2,x_1x_2,x_1^2,x_2^2\\right).  \\] <p>Remark Why don't we just use lots of features? I.e. in polynomial regression, why don't we take degree \\(d\\) to be large? That is because problems owith overfitting! This is mentioned in the variance-bias tradeoff.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week3/#variable-selection","title":"Variable Selection","text":"<p>Assume we have many features, i.e. \\(\\boldsymbol{X}\\) has many columns. Given an index set \\(I = \\left(i_1,i_2,\\dots,i_k\\right)\\), where \\(i_1\\leq i_2\\leq \\dots\\leq i_k\\), define </p> \\[ \\boldsymbol{X}_I = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ x_{i_1} &amp; \\cdots &amp; x_{i_k} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}, \\quad  \\boldsymbol{\\beta}_I = \\begin{bmatrix} \\beta_{i_1} \\\\ \\vdots \\\\ \\beta_{i_k} \\end{bmatrix}. \\] <p>Then we can obtain a submodel with features in \\(I\\) give nby </p> \\[ \\boldsymbol{Y} = \\boldsymbol{X}_I \\boldsymbol{\\beta}_I + \\varepsilon.  \\] <p>Because of the problem of overfitting, we may not want to use empirical risks to evaluate the submodel (the empirical risk will always decrease when adding new features). A better idea is to add a penalty for including new features: </p> \\[ \\begin{aligned} &amp; \\operatorname{AIC}(I) = \\frac{1}{n}\\left(\\|\\boldsymbol{Y} - \\boldsymbol{X}_I \\hat{\\boldsymbol{\\beta}}_I\\|^2 + 2k\\hat{\\sigma}^2\\right), \\\\ &amp; \\operatorname{BIC}(I) = \\frac{1}{n}\\left(\\|\\boldsymbol{Y} - \\boldsymbol{X}_I \\hat{\\boldsymbol{\\beta}}_I\\|^2 + \\ln(n) k\\hat{\\sigma}^2\\right), \\end{aligned} \\] <p>where if \\(\\hat{\\boldsymbol{\\beta}}\\) is estimated by using the full model, \\(\\hat{\\sigma}\\) is given by</p> \\[ \\hat{\\sigma}^2 = \\frac{\\|\\boldsymbol{Y} - \\boldsymbol{X}_I \\hat{\\boldsymbol{\\beta}}\\|^2}{n-p}.  \\] <p>We want to choose proper index set \\(I\\) to maximize AIC or BIC. For simplicity, we may view \\(I\\) as a string in length \\(p\\) consists of 0 and 1, where 0 indicates the feature is not included and 1 indicates the feature is included: for instance, </p> \\[ (0,1,0,0,1) \\] <p>is an index set of 5 features where the second and fifth feature are included. In this way, \\(I\\) is a point in \\(\\{0,1\\}^p\\) and we may visualize the choice of \\(I\\) as picking a vertex of a hypercube.  Minimizing AIC is the same as searching for the optimal vertex on the cube. </p> <p>For \\(p&gt;40\\), search becomes infeasible. If we enumerate the AIC for every \\(I\\), this called best subset selection. Instead, we can do greedy search over the cube. </p> <p>Forward selection: </p> <ol> <li> <p>Start with trivial null model \\(y = \\beta_0 + \\varepsilon\\). </p> </li> <li> <p>Add a feature by searching for the model \\(y = \\beta_0 + \\beta_i x_i + \\varepsilon\\) over \\(i\\) that has the smallest AIC/BIC. Call this best \\(i\\) to be \\(i_1\\). </p> </li> <li> <p>Continue by find an \\(j\\) where the model \\(y=\\beta_0 + \\beta_{i_1} x_{i_1} + \\beta_j x_j + \\varepsilon\\) maximizes AIC/BIC. Call this \\(j\\) to be \\(i_2\\). </p> </li> <li> <p>Keep on doing this until we reach the full model. </p> </li> <li> <p>Choose the model in our sequence with the smallest AIC/BIC. </p> </li> </ol> <p>Backward selection: start at full model and remove variables until reaching the null model. </p> <p>Best subset selection needs to fit \\(2^p\\) linear models. On the other hand, for forward (backward) selection needs to fit</p> \\[ p+(p-1)+\\dots + 1 = \\frac{p(p+1)}{2} \\] <p>models, which is an order of \\(O(p^2)\\) computations. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/","title":"Week 4","text":""},{"location":"notes/lecture_notes/stat541/stat541_week4/#ridge-regression","title":"Ridge Regression","text":""},{"location":"notes/lecture_notes/stat541/stat541_week4/#motivation","title":"Motivation","text":"<p>For polynomial regression, if the degree \\(d\\) is large, prediction can be highly variable. The idea here is to \"dampen\" the coefficients \\(\\hat{\\beta}_i\\) in order to constrain higher order terms in the prediction function. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#ridge-regression_1","title":"Ridge Regression","text":"<p>Suppose we have a \\(C&gt;0\\) (we assume a proper \\(C\\) is given by an oracle), and we will constrain \\(\\boldsymbol{\\beta}\\) (here \\(\\boldsymbol{\\beta}\\) starts from \\(\\beta_1\\) rather than \\(\\beta_0\\)(1)) so that </p> <ol> <li>In ridge, we don't want to penalize the \\(\\beta_0\\) but other \\(\\beta\\). </li> </ol> \\[ \\|\\boldsymbol{\\beta}\\|^2 = \\sum_{i=1}^{p} \\beta_i^2 \\leq C.  \\] <p>The ridge regression estimator \\(\\boldsymbol{\\beta}_{\\rm Ridge}\\) is defined as </p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm Ridge} = \\operatorname*{arg\\, min}_{\\|\\boldsymbol{\\beta}\\|^2\\leq C} \\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2.  \\] <p>Using Lagrange multiplies to convert the above minimization problem into an equivalent problem: </p> \\[ \\begin{aligned} \\hat{\\boldsymbol{\\beta}}_{\\rm Ridge} &amp;= \\operatorname*{arg\\, min}_{\\boldsymbol{\\beta}\\in \\mathbb{R}^p} \\left(\\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 + \\lambda \\left(\\|\\boldsymbol{\\beta}\\|^2-C\\right)\\right) \\\\ &amp;= \\operatorname*{arg\\, min}_{\\boldsymbol{\\beta}\\in \\mathbb{R}^p} \\left(\\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|^2\\right),  \\end{aligned} \\] <p>where the constant \\(\\lambda&gt;0\\) is dependent on \\(C\\). </p> <p>Ridge is an example of ERM plus regularization term (or penalty) \\(\\lambda \\|\\boldsymbol{\\beta}\\|^2\\). The general formulation of ERM + regularization is </p> \\[ \\operatorname*{arg\\, min}_{f} \\left(\\frac{1}{n} \\sum_{i=1}{n} L\\left(y^{(i)},f(x^{(i)})\\right) + \\operatorname{Penalty}(f)\\right),  \\] <p>where the \\(\\operatorname{Penalty}(f)\\) penalize \"complex\" \\(f\\) more. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#compute-the-ridge-estimator","title":"Compute the Ridge Estimator","text":"<p>To find \\(\\hat{\\boldsymbol{\\beta}}_{\\rm Ridge}\\), we let </p> \\[ \\begin{aligned} 0  &amp;= \\nabla_{\\boldsymbol{\\beta}}\\left(\\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|^2\\right) \\\\ &amp;= -2\\boldsymbol{X}^T\\boldsymbol{Y} + 2(\\boldsymbol{X}^T\\boldsymbol{X})\\boldsymbol{\\beta} + 2\\lambda \\boldsymbol{\\beta}.  \\end{aligned} \\] <p>Then we have </p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm Ridge} = (\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda I)^{-1} \\boldsymbol{X}^T\\boldsymbol{Y},  \\] <p>where the matrix \\(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda I\\) is always invertible unlike OLS. </p> <p>If assume \\(\\boldsymbol{X}\\) has SVD given by \\(X = UDV^T\\), then we have </p> \\[ \\begin{aligned} \\hat{\\boldsymbol{\\beta}}_{\\rm Ridge}  &amp;= (VDU^TUDV^T+\\lambda VV^T)^{-1}VDU^T\\boldsymbol{Y} \\\\ &amp;= (V^T)^{-1}(D^2+\\lambda I)^{-1} V^{-1} VDU^T\\boldsymbol{Y} \\\\ &amp;= V(D^2+\\lambda I)^{-1} DU^T\\boldsymbol{Y} \\end{aligned},  \\] <p>where \\((D^2+\\lambda I)^{-1} D\\) is a diagonal matrix given by \\(diag\\{\\frac{d_1}{d_1^2+\\lambda},\\dots,\\frac{d_p}{d_p^2+\\lambda}\\}\\) while OLS has \\(\\hat{\\boldsymbol{\\beta}}_{\\rm OLS} = VD^{-1}U^T\\boldsymbol{Y}\\). Therefore, the coefficients in \\(\\hat{\\boldsymbol{\\beta}}_{\\rm Ridge}\\) are usually smaller in magnitude compared to \\(\\hat{\\boldsymbol{\\beta}}_{\\rm OLS}\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#centering-and-scaling","title":"Centering and Scaling","text":"<p>Typically, we center our design matrix and response matrix: </p> \\[ \\boldsymbol{Y} = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{bmatrix} \\xrightarrow{\\text{center}} \\boldsymbol{Y}^* = \\begin{bmatrix} y^{(1)} - \\bar{y} \\\\ \\vdots \\\\ y^{(n)} - \\bar{y} \\end{bmatrix}, \\] <p>where \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y^{(i)}\\) is the average of the responses, and </p> \\[ \\boldsymbol{X} = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ x_{1} &amp; \\cdots &amp; x_{p} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix} \\xrightarrow{\\text{center}} \\tilde{\\boldsymbol{X}} = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ x_{1} - \\bar{x}_1 &amp; \\cdots &amp; x_{p} - \\bar{x}_p \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}, \\] <p>where \\(\\bar{x}_i =  \\frac{1}{n}\\sum_{j=1}^{n}x_j^{(i)}\\) is the average of the \\(i\\)-th column of \\(\\boldsymbol{X}\\)(1). We center because we don't want to penalize \\(\\beta_0\\). </p> <ol> <li>Note that there is no column of 1's added in \\(\\boldsymbol{X}\\). </li> </ol> <p>Moreover, we also want to scale  \\(\\tilde{\\boldsymbol{X}}\\),</p> \\[ \\tilde{\\boldsymbol{X}} = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ \\tilde{x}_{1} &amp; \\cdots &amp; \\tilde{x}_{p} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix} \\xrightarrow{\\text{scale}} \\boldsymbol{X}^* = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ \\frac{\\tilde{x}_{1}}{\\frac{1}{n}\\sum_{j=1}^{n}\\tilde{x}_{j1}} &amp; \\cdots &amp; \\frac{\\tilde{x}_{p}}{\\frac{1}{n}\\sum_{j=1}^{n}\\tilde{x}_{jp}} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}. \\] <p>We scale so all of the features have the same magnitude. </p> <p>After centering and scaling, we run ridge regression using the response \\(\\boldsymbol{Y}^*\\) and the design \\(\\boldsymbol{X}^*\\). When new data points come in, we do the centering and scaling using the \\(\\bar{y}\\), \\(\\bar{x}_i\\), and \\(\\frac{1}{n}\\sum_{j=1}^{n}\\tilde{x}_{ji}\\) of the training data. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#lasso-least-absolute-shrinkage-and-selection-operator","title":"LASSO (Least Absolute Shrinkage and Selection Operator)","text":"<p>LASSO is another method of ERM + regularization, which is also given by two formulations:</p> <p>Constrain Formulation</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm LASSO} = \\operatorname*{arg\\, min}_{\\|\\boldsymbol{\\beta}\\|_1\\leq C} \\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2. \\] <p>Constrain Formulation</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm LASSO} = \\operatorname*{arg\\, min}_{\\boldsymbol{\\beta}\\in \\mathbb{R}^p} \\left(\\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1\\right), \\] <p>where \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_i|\\). </p> <p>For \\(p=2\\) and \\(C=1\\), the constraints of ridge and LASSO are respectively \\(L^2\\) and \\(L^1\\) balls, and the (contour of) corresponding objective functions are shown as follows.</p> <p></p> <p>The \\(L^1\\) ball can usually impose sparsity (occurrence of zeros(1)) in the LASSO coefficient estimate, and thus LASSO does variable selection. </p> <ol> <li>In practice, the contour of the objective function usually intersects with the \\(L^1\\) ball at vertices. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#lasso-with-p1","title":"LASSO with \\(p=1\\)","text":"<p>For \\(p=1\\), find the LASSO estimate</p> \\[ \\operatorname*{arg\\, min} \\sum_{i=1}^{n} (\\tilde{y}^{(i)} - \\beta \\tilde{x}^{(i)})^2 + \\lambda|\\beta|, \\] <p>where \\(\\tilde{y}^{(i)} = y^{(i)}-\\bar{y}\\) and \\(\\tilde{x}^{(i)} = x^{(i)} - \\bar{x}\\). </p> <p>For \\(\\beta&gt;0\\), </p> \\[ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d} \\beta} \\left(\\sum_{i=1}^{n} (\\tilde{y}^{(i)} - \\beta \\tilde{x}^{(i)})^2 + \\lambda|\\beta|\\right) &amp;= \\frac{\\mathrm{d}}{\\mathrm{d} \\beta} \\left(\\beta^2\\sum_{i=1}^{n}(\\tilde{x}^{(i)})^2 - 2\\beta\\sum_{i=1}^{n}\\tilde{x}^{(i)}\\tilde{y}^{(i)} + \\lambda\\beta\\right) \\\\ &amp;= 2\\beta c- 2d + \\lambda = 0, \\end{aligned} \\] <p>where \\(c = \\sum_{i=1}^{n}(\\tilde{x}^{(i)})^2\\) and \\(d = \\sum_{i=1}^{n}\\tilde{x}^{(i)}\\tilde{y}^{(i)}\\). Then, we have </p> \\[ \\beta = \\frac{d}{c} - \\frac{\\lambda}{2c}.  \\] <p>Similarly, for \\(\\beta&lt;0\\), we have </p> \\[ \\beta = \\frac{d}{c} + \\frac{\\lambda}{2c}.  \\] <p>Therefore, the LASSO estimator for \\(p=1\\) is given by</p> \\[ \\begin{aligned} \\hat{\\beta}_{\\rm LASSO}  &amp;= \\left\\{\\begin{matrix} \\frac{d}{c} - \\frac{\\lambda}{2c} \\quad &amp;\\text{ if } \\frac{d}{c} - \\frac{\\lambda}{2c} &gt; 0, \\\\ \\frac{d}{c} + \\frac{\\lambda}{2c} \\quad &amp;\\text{ if } \\frac{d}{c} + \\frac{\\lambda}{2c} &lt; 0, \\\\ 0\\quad &amp;\\text{otherwise,} \\end{matrix} \\right. \\\\ &amp;= \\operatorname*{sgn}(\\frac{d}{c})\\cdot \\max\\{0,\\left|\\frac{d}{c}\\right|-\\frac{\\lambda}{2c}\\}. \\end{aligned} \\] <p>The following figure illustrates the difference of the OLS estimator and LASSO estimator, where we can see \\(\\hat{\\beta}_{\\rm LASSO}\\) applies a soft threshold to \\(\\pm\\frac{\\lambda}{2c}\\)(1).</p> <ol> <li>Compared to the soft threshold, a hard threshold would show \"jumps\" in figures, which indicates the function is not continuous. </li> </ol> <p></p> <p>For more features, we may use a coefficient plot with respect to \\(\\lambda\\), i.e. \\(\\beta_i\\)-\\(\\lambda\\) plot(1).  As \\(\\lambda\\) grows, the later a \\(\\beta_i\\) becomes 0, the more important this corresponding feature should be as it remains in our model even with large penalty. </p> <ol> <li>For \\(p=1\\), \\(\\beta\\) is linear with respect to \\(\\lambda\\). For \\(p&gt;1\\), we may choose a proper transformation (using parameters like \\(d\\) and \\(c\\)) of \\(\\lambda\\) to make the coefficient plot piecewise linear as the figure below. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#group-lasso","title":"Group LASSO","text":"<p>Suppose our regression coefficients \\(\\boldsymbol{\\beta}\\) is partitioned into groups as </p> \\[ \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta^{(1)} \\\\ \\vdots \\\\ \\beta^{(k)} \\end{bmatrix}, \\] <p>where \\(\\beta^{(i)}\\in \\mathbb{R}^{p_i}\\). We may want to set entire groups of coefficients \\(\\beta^{(i)}\\) to zero. Then we introduce the penalty</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm Group} = \\operatorname*{arg\\, min}_{\\boldsymbol{\\beta}\\in \\mathbb{R}^p} \\left(\\|\\boldsymbol{Y} - \\boldsymbol{X} \\boldsymbol{\\beta}\\|^2 + \\sum_{i=1}^{k} \\lambda_i\\|\\beta^{(i)}\\|\\right), \\] <p>where \\(\\|\\beta^{(i)}\\| = \\sqrt{\\sum_{j=1}^{p_i}(\\beta^{(i)}_j)^2}\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#data-splitting-and-cross-validation","title":"Data Splitting and Cross-Validation","text":"<p>Suppose we have two classes of functions (or models or learning algorithms) \\(\\mathcal{F}_1\\) and \\(\\mathcal{F_2}\\). We may not want to use the empirical risk, based on the whole data set, to evaluate these two classes. This is because we use the data set to fit our model under the guidance of ERM, and therefore, the model from a more complex class will achieve a lower empirical risk on this exactly same data set(1). </p> <ol> <li>An extreme example: we fit the data set using a very high degree polynomial, and we will find a zero-ERM polynomial model. If we follow the standard of ERM, this should be THE BEST model, which is clearly not the case. </li> </ol> <p>Solution is to split the data into training set and a validation set: </p> \\[ \\underbrace{\\left(x^{(1)},y^{(1)}\\right),\\dots,\\left(x^{(k)},y^{(k)}\\right)}_{\\text{Training Set } \\mathcal{D}_{\\rm Train}},\\underbrace{\\left(x^{(k+1)},y^{(k+1)}\\right),\\dots,\\left(x^{(n)},y^{(n)}\\right)}_{\\text{Validation Set } \\mathcal{D}_{Valid}}.  \\] <p>More precisely, our objective is to estimate \\(R(\\hat{f}_i,P)\\), \\(\\hat{f}_i\\in \\mathcal{F}_i\\), and find the model \\(i\\) that minimizes, which is achieved by the following steps: </p> <ol> <li> <p>Fit \\(\\hat{f}_i\\in \\mathcal{F}_i\\) via ERM (or another method) using only the data in \\(\\mathcal{D}_{\\rm Train}\\). </p> </li> <li> <p>Compute  \\(\\displaystyle \\frac{1}{|\\mathcal{D}_{\\rm Valid}|} \\sum_{\\left(x^{(i)},y{(i)}\\right)\\in \\mathcal{D}_{\\rm Valid}} L\\left(\\hat{f}_i(x^{(j)}),y^{(j)}\\right).\\)</p> </li> <li> <p>Find the model \\(i\\) that minimizes Step 2. </p> </li> <li> <p>Usually, we then fit \\(\\hat{f}\\) using the model in Step 3 on the entire data set \\(\\mathcal{D}_{\\rm Train}\\cup \\mathcal{D}_{\\rm Valid}\\). </p> </li> </ol> <p>If we consider the expected value of empirical risk conditional on \\(\\mathcal{D}_{\\rm Train}\\) (in this case, \\(\\hat{f} = \\hat{f}_{\\rm Train}\\) is regarded as fixed, which is obtained by \\(\\mathcal{D}_{\\rm Train}\\)),  we have </p> \\[ E_{\\mathcal{D}_{\\rm Valid}}\\left(\\frac{1}{|\\mathcal{D}_{\\rm Valid}|} \\sum_{\\left(x^{(i)},y{(i)}\\right)\\in \\mathcal{D}_{\\rm Valid}} L\\left(\\hat{f}_i(x^{(j)}),y^{(j)}\\right) \\mid \\mathcal{D}_{\\rm Train}\\right) = R(\\hat{f},P), \\] <p>where in converse, the empirical risk on whole data set, we will underestimate the true risk. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#example-of-ridge-regression","title":"Example of Ridge Regression","text":"<p>For each \\(\\lambda\\) (or equivalently \\(C\\)), we get a class of functions \\(\\mathcal{F}_{C_1},\\dots,\\mathcal{F}_{C_K}\\). For some choices of \\(C\\), if for some \\(i,j\\), \\(C_i&lt;C_j\\), then \\(\\mathcal{F}_{C_i}\\subset\\mathcal{F}_{C_j}\\); similarly, if \\(\\lambda_i&lt;\\lambda_j\\), then \\(\\mathcal{F}_{\\lambda_i}\\supset\\mathcal{F}_{\\lambda_j}\\). To find the most reasonable model, we follow the steps below: </p> <ol> <li> <p>Start off with a grid of \\(\\lambda\\)'s, for instance \\(\\{\\lambda_1 = 0.1, \\lambda_2 = 1, \\lambda_3 = 10, \\lambda_4 = 100\\}\\). </p> </li> <li> <p>For every \\(\\lambda_i\\), find the ridge estimator \\(\\hat{\\boldsymbol{\\beta}}_{\\lambda_i}\\) using only the training data. </p> </li> <li> <p>We compute \\(\\displaystyle \\frac{1}{|\\mathcal{D}_{\\rm Valid}|} \\sum_{\\left(x^{(i)},y{(i)}\\right)\\in \\mathcal{D}_{\\rm Valid}} \\left(\\left(x^{(j)}\\right)^T\\hat{\\boldsymbol{\\beta}}_{\\lambda_i} - y^{(j)}\\right)^2.\\)</p> </li> <li> <p>Choose the \\(\\lambda_i\\) with the smallest value in Step 3. </p> </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#proportions-to-split","title":"Proportions to Split","text":"<p>Usually, we take the 80% or 90% of the data set as the training set. </p> <p>The larger \\(\\mathcal{D}_{\\rm Valid}\\) is the lower the variance of \\(\\hat{R}(\\hat{f},P)\\) is. However, we don't want to take \\(\\mathcal{D}_{\\rm Valid}\\) too large. This is because if \\(\\mathcal{D}_{\\rm Train}\\) is small when fitting \\(\\hat{f}\\), then \\(\\hat{f}_{\\mathcal{D}_{\\rm Train}}\\) might be very different from \\(\\hat{f}_{\\mathcal{D}_{\\rm Train}\\cup\\mathcal{D}_{\\rm Valid}}\\), which means \\(\\hat{f}_{\\mathcal{D}_{\\rm Train}}\\) is biased. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week4/#k-fold-cross-validation","title":"\\(K\\)-Fold Cross-Validation","text":"<p>Split the data into \\(K\\)-groups of roughly the same size: </p> \\[ \\mathcal{D} = \\left(\\mathcal{D}^{(1)}, \\underbrace{\\mathcal{D}^{(2)}}_{\\text{called \"folds\"}}, \\dots, \\mathcal{D}^{(K)}\\right).  \\] <p>We estimate \\(R(\\hat{f},P)\\) by the following steps: </p> <ol> <li> <p>For \\(i=1,\\dots,K\\), fit a model using \\(\\mathcal{D}^{(1)}, \\dots, \\mathcal{D}^{(i-1)}, \\mathcal{D}^{(i+1)}, \\dots, \\mathcal{D}^{(K)}\\) to get \\(\\hat{f}^{(i)}\\). </p> </li> <li> <p>Compute \\(\\displaystyle CV^{(i)} = \\frac{1}{|\\mathcal{D}^{(i)}|} \\sum_{\\left(x,y\\right)\\in \\mathcal{D}^{(i)}} L\\left(\\hat{f}^{(i)}(x),y\\right).\\)</p> </li> <li> <p>Estimate \\(R(\\hat{f},P)\\approx \\frac{1}{K}\\sum_{i=1}^K CV^{(i)} = CV\\).</p> </li> </ol> <p>How many folds to choose? Usually \\(K\\) is chosen from 5-10. To show that it may not always be beneficial to choose large \\(K\\), we consider an example of \\(K=n\\) (size of the data set), which is called leave-one-out cross-validation (LOOCV)(1). LOOCV may not always give better estimates of \\(R(\\hat{f},P)\\): </p> <ol> <li>Specifically, for linear regression we can calculate the LOOCV error of \\(\\hat{f}(x) = \\boldsymbol{\\hat{\\beta}}^T x\\) without fitting \\(\\hat{f}_i\\), which has a closed form expression: \\(\\displaystyle R(\\hat{f},P)= \\frac{1}{n} \\sum_{i=1}^n\\left(\\frac{y^{(i)} - \\left(x^{(i)}\\right)^T\\hat{\\boldsymbol{\\beta}} }{1-\\left(x^{(i)}\\right)^T \\left(X^T X\\right)^{-1} x^{(i)}} \\right)^2\\). </li> </ol> \\[ \\begin{aligned} Var(CV)  &amp;= \\frac{1}{K^2} Cov\\left(\\sum_{i=1}^K CV^{(i)},\\sum_{j=1}^K CV^{(j)} \\right) \\\\ &amp;= \\frac{1}{K^2}\\sum_{i=1}^K\\sum_{j=1}^K Cov\\left(CV^{(i)},CV^{(j)} \\right),  \\end{aligned} \\] <p>which indicates that if \\(Cov\\left(CV^{(i)},CV^{(j)} \\right)=0\\) for \\(i\\neq j\\), then the larger \\(K\\) is better. However, if \\(K\\) is large, then \\(\\hat{f}^{(i)}\\approx\\hat{f}^{(j)}\\) as the training sets are nearly identical, and thus, \\(Cov\\left(CV^{(i)},CV^{(j)} \\right)\\) might be large and positive. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week5/","title":"Week 5","text":""},{"location":"notes/lecture_notes/stat541/stat541_week5/#classification","title":"Classification","text":"<p>Let \\(\\mathcal{Y}\\) is a finite set of classes, and for now assume \\(y^{(i)}\\in \\{0,1\\}\\). We want to use \\(x\\) to predict \\(y\\). We assume </p> \\[ y\\mid x \\sim {\\rm Bernoulli}\\left(g(x)\\right),  \\] <p>where \\(g\\) is an arbitrary function determining the probability of \\(y=1\\) for given \\(x\\). Then this Bernoulli distribution is completely general. See an example of this setting in the plane (\\(p=2\\)).  </p> <p>We could try to just do linear regression: under squared error loss, \\(E(y\\mid x)\\) is optimal and \\(E(y\\mid x) = g(x)\\). However, \\(g(x)\\in [0,1]\\) as it outputs a probability, but \\(x^T\\boldsymbol{\\beta}\\) takes values in \\(\\mathbb{R}\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week5/#logistic-regression","title":"Logistic Regression","text":"<p>Instead of modeling \\(y(x)=x^T\\boldsymbol{\\beta}\\), we will assume that the probability \\(g(x)\\)(1) is given by</p> <ol> <li>Here \\(\\sigma(x^T\\boldsymbol{\\beta})\\), the probability of \\(y\\mid x\\), should be in [0,1], which is easy to verify. </li> </ol> \\[ g(x)=  \\sigma(x^T\\boldsymbol{\\beta}), \\] <p>where \\(\\sigma\\) is the logistic function given by</p> \\[ \\sigma(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1+e^{-z}}.  \\] <p>This is called logistic regression model.   </p> <p>For \\(p=2\\), the logistic function is given by \\(\\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)\\). The following figure shows an example of \\(\\sigma(x+y)\\).  </p>"},{"location":"notes/lecture_notes/stat541/stat541_week5/#motivation-and-interpretation","title":"Motivation and Interpretation","text":"<p>On one hand, \\(\\sigma\\) is differentiable and easy to compute with. On the other hand, logistic regression assumes the log odds is a linear function. To further explain this, we need to introduce the following definitions. </p> <p>If \\(p\\) is a probability, the quantity \\(p /(1-p)\\) is called the (corresponding) odds, and can take on any value between 0 and \\(\\infty\\). Values of the odds close to 0 and \\(\\infty\\) indicate very low and very high probabilities of default, respectively. </p> <p>The log odds (or logit) of the probability is the logarithm of the odds, i.e.</p> \\[ \\operatorname{logit}(p) = \\log \\frac{p}{1-p}\\in (-\\infty,+\\infty). \\] <p>Hence, in logistic regression, we make an assumption that the log odds is linearly dependent on \\(x\\). </p> <p>Another more insightful motivation connects the logistic function to the exponential family from the prospective of generative models: </p> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_week5/#procedure-of-logistic-regression","title":"Procedure of Logistic Regression","text":"<p>How to use logistic regression for prediction:</p> <ol> <li> <p>Get training data \\((x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)})\\dots\\)</p> </li> <li> <p>Use training data to select a \\(\\hat{\\beta}\\), i.e. fit our model to the training data. </p> </li> <li> <p>Get a new \\(x\\) want to predict the class of \\(y\\): Compute the estimated probability \\(\\displaystyle \\sigma (\\hat{\\beta}^T x) = \\frac{e^{\\hat{\\beta}^T x}}{1 + e^{\\hat{\\beta}^T x}}\\). </p> <ul> <li> <p>If \\(\\sigma (\\hat{\\beta}^T x)&gt;0.5\\), predict that \\(y\\) is in class \\(1\\).  </p> </li> <li> <p>If \\(\\sigma (\\hat{\\beta}^T x)&lt;0.5\\), predict that \\(y\\) is in class \\(0\\).  </p> </li> </ul> </li> </ol> <p>This procedure is designed to minimize the misclassification error. However, some errors are \"worse\" than others. For example, for a spam email filter, misclassifying spams as normal emails is unlikely to cause big issue, but it may be terrible in the opposite. To account for this we can modify the 0.5 threshold into a larger number (such as 0.95) when predicting a email to be a spam.(1)</p> <ol> <li>This means the filter is surely confident when classify some email as spam.</li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week5/#multinomial-regression","title":"Multinomial Regression","text":"<p>Let \\(y^{(i)}\\) takes on value from multiple classes, we say \\(K\\)-class, and we assume </p> \\[ y\\mid x \\sim {\\rm Multinomial}_K\\left(p_1(x), p_2(x), \\dots, p_K(x)\\right).   \\] <p>For \\(p\\) features, we assume \\(x=(1, x_1,\\dots, x_p)\\). Then the multinomial regression is given by </p> \\[ \\begin{aligned} &amp; p_1(x) = \\frac{\\exp\\left(\\left(\\beta^{(1)}\\right)^T x\\right)}{1 + \\sum_{j=1}^{K-1}\\exp\\left(\\left(\\beta^{(j)}\\right)^T x\\right)}, \\\\ &amp; \\quad \\vdots \\\\ &amp; p_{K-1}(x) = \\frac{\\exp\\left(\\left(\\beta^{(K-1)}\\right)^T x\\right)}{1 + \\sum_{j=1}^{K-1}\\exp\\left(\\left(\\beta^{(j)}\\right)^T x\\right)}, \\\\ &amp; p_{K}(x) = \\frac{1}{1 + \\sum_{j=1}^{K-1}\\exp\\left(\\left(\\beta^{(j)}\\right)^T x\\right)},  \\end{aligned} \\] <p>where we regard the \\(K\\)-th class as the baseline class. It is easy to see \\(p_1,\\dots, p_K\\) are valid probabilities(1). A function with a form of \\(p_1,\\dots, p_{K-1}\\) is called softmax function. </p> <ol> <li>Non-negative and the sum is 1. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week5/#calculate-the-estimator","title":"Calculate the Estimator","text":"<p>We use maximum likelihood to estimate \\(\\beta\\) to get \\(\\hat{\\beta}\\). We take \\(K=2\\) as an example.</p> <p>Assume \\(y\\mid x \\sim {\\rm Bernoulli}\\left(\\pi\\right)\\), i.e. \\(Pr(y) = \\pi^y(1-\\pi)^{1-y}\\). To apply logistic regression, we assume \\(y^{(i)}\\sim {\\rm Bernoulli}\\left(\\sigma \\left((x^{(i)})^T \\beta\\right)\\right)\\). Then the log-likelihood function is </p> \\[ \\begin{aligned} L(\\beta)  &amp;= \\sum_{i=1}^n \\ln \\left(Pr(y^{(i)\\mid x^{(i)},\\beta})\\right) \\\\ &amp;= \\sum_{i=1}^n y^{(i)} \\ln\\left(\\sigma \\left((x^{(i)})^T \\beta\\right)\\right) + \\sum_{i=1}^n (1-y^{(i)})\\ln \\left(1- \\sigma \\left((x^{(i)})^T \\beta\\right)\\right).  \\end{aligned} \\] <p>Then the MLE is defined as </p> \\[ \\hat{\\beta}_{\\rm MLE} = \\operatorname*{arg\\, max} L(\\beta) = \\operatorname*{arg\\, min} (-L(\\beta)).  \\] <p>Unfortunately, unlike linear regression, \\(\\nabla L(\\beta)\\) here has no closed-form. Therefore, we may apply some other optimization method, such as gradient descent and Newton Raphson Algorithm(1). </p> <ol> <li>Reference: \u5218\u6d69\u6d0b[\u7b49]\u7f16\u8457, \u6700\u4f18\u5316:\u5efa\u6a21,\u7b97\u6cd5\u4e0e\u7406\u8bba (Optimization: modeling, algorithm and theory), Di 1 ban. Beijing: \u9ad8\u7b49\u6559\u80b2\u51fa\u7248\u793e, 2020.  For GD see section 6.2, and for NR see section 6.4.</li> </ol> <p>Brief comparison of GD and NR: </p> <ul> <li> <p>NR Usually takes more intelligent steps than GD. </p> </li> <li> <p>NR There is no tuning required while in GD we have to tune the step size. </p> </li> <li> <p>Problem with NR We have to invert the the Hessian matrix at each step. Inverting such a matrix has complexity \\(O(p^3)\\). For GD we only have to compute the gradient, which has complexity \\(O(p)\\).  </p> </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week6/","title":"Week 6","text":""},{"location":"notes/lecture_notes/stat541/stat541_week6/#generative-models-for-classification","title":"Generative Models for Classification","text":"<p>For classification(1), logistic regression involves directly modeling \\(g(x)\\) (i.e.  \\(Pr(y=1\\mid x)\\)) using the logistic function. We now consider an alternative and less direct approach to estimating these probabilities -- we model the distribution of the feature in each of the response classes, i.e. \\(p(x\\mid y)\\) for each \\(y\\in \\mathcal{Y}\\). We then use Bayes\u2019 theorem to flip these around into estimates for \\(Pr(y = 1 \\mid x)\\)(2).</p> <ol> <li> <p>For simplicity, we assume \\(\\mathcal{Y}=\\{0,1\\}\\) and \\(\\mathcal{X}=\\mathbb{R}\\) in this paragraph. </p> </li> <li> <p>This requires the knowledge of \\(Pr(x)\\) and \\(Pr(y)\\) for each \\(x\\in \\mathcal{X}, y\\in \\mathcal{Y}\\). We can either choose \\(Pr(y)\\) to be the proportion of each class among the whole data set, or regard it as unknown parameters which we will estimate later. As for \\(Pr(x)\\), since our goal is to give the best prediction for new data, this term will be canceled while comparing different predictions. </p> </li> </ol> <p>Compared with discriminative models(1), where we model \\(p(y\\mid x)\\) and use it to predict, for generative models, we model \\(p(y,x)\\) and then use it to compute \\(p(y\\mid x)\\) for predictions. </p> <ol> <li>Discriminative models, also referred to as conditional models, studies the \\({\\displaystyle P(y|x)}\\) or maps the given unobserved variable (target) \\(x\\) to a class \\(y\\) dependent on the observed variables (training samples). Types of discriminative models include logistic regression, conditional random fields, decision trees among many others. </li> </ol> <p>Specifically, we model </p> \\[ p(y,x) = p(x\\mid y)p(y) = p(y\\mid x) p(x),  \\] <p>and want to obtain \\(p(y\\mid x)\\) from this. Therefore, we need to specify a distribution for \\(p(x\\mid y)\\): assume that \\(x\\in \\mathbb{R}^p\\) and is continuous, and \\((x\\mid y)\\) follows certain Gaussian distributions. There are three kinds of assumptions on Gaussian distributions: quadratic discriminant analysis (QDA), linear discriminant analysis (LDA), and naive Bayes (NB). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#qda-lda-and-naive-bayes","title":"QDA, LDA, and Naive Bayes","text":"<p>Model assumptions (figures below show the Contour of \\(x\\mid y\\) with \\(p=2\\) and \\(y\\) in two different classes):</p> <ul> <li> <p>QDA assumes that \\(\\displaystyle x\\mid y=j \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)\\).  </p> </li> <li> <p>LDA assumes that \\(\\displaystyle x\\mid y=j \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)\\), where \\(\\Sigma\\) is the common covariance matrix across all classes.  </p> </li> <li> <p>Naive Bayes assumes that \\(\\displaystyle x\\mid y=j \\sim \\mathcal{N}_p(\\mu_j, D_j)\\), where \\(D_j\\) is diagonal for every class. This implies that the individual features are independent given \\(y\\). For NB, the eigenvectors of the covariance matrices are parallel to the axes.  </p> </li> </ul> <p>Among the three models, QDA is the most complicated; LDA and NB are different -- one is not more complicated than the other. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#procedure-of-prediction","title":"Procedure of Prediction","text":"<p>We assume \\(y\\) takes in \\(K\\)-classes following multinomial distribution</p> \\[ y \\sim {\\rm Multinomial}_K(\\pi),  \\] <p>where \\(\\pi = \\left(Pr(y=1),\\dots,Pr(y=K)\\right)\\) is a probability vector.</p> <ol> <li> <p>Choose a model QDA, LDA, or NB with unknown parameters \\(\\theta = \\pi, \\mu_j, \\Sigma_j\\) for \\(j=1,\\dots, p\\).  </p> </li> <li> <p>Find estimates of the unknown parameters using MLE. </p> </li> <li> <p>Compute \\(\\displaystyle p(y\\mid x,\\hat{\\theta})\\) and use this for classification. </p> </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#estimate-parameters","title":"Estimate Parameters","text":"<p>In QDA, the likelihood looks like </p> \\[ \\begin{aligned} L(\\theta)  &amp;= \\prod_{i=1}^{n} p\\left(x^{(i)}, y^{(i)}\\mid \\theta\\right) \\\\ &amp;= \\prod_{i=1}^{n} p\\left(x^{(i)}\\mid y^{(i)}, \\theta\\right) p(y^{(i)}\\mid \\theta) \\\\ &amp;= \\prod_{i=1}^{n} \\prod_{j=1}^{K} \\left(\\frac{1}{\\sqrt{(2\\pi)^p \\operatorname{det}(\\Sigma_j)}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_j\\right)^T\\Sigma_j^{-1}\\left(x^{(i)}-\\mu_j\\right)\\right)\\pi_j\\right)^{I(y^{(i)}=j)}.  \\end{aligned} \\] <p>Maximizing this with respect to \\(\\pi, \\mu_j, \\Sigma_j\\) for \\(j=1,\\dots, p\\), we obtain: denoting \\(n_j = \\sum_{i=1}^n I(y^{(i)}=j)\\), </p> \\[ \\begin{aligned} \\hat{\\pi_j} &amp;= \\frac{n_j}{n}, \\\\ \\hat{\\mu_j} &amp;= \\frac{1}{n_j} \\sum_{i\\mid y^{(i)}=j} x^{(i)}, \\\\ \\hat{\\Sigma_j} &amp;= \\frac{1}{n_j}\\sum_{i\\mid y^{(i)}=j} \\left(x^{(i)}-\\hat{\\mu_j}\\right)\\left(x^{(i)}-\\hat{\\mu_j}\\right)^T,  \\end{aligned} \\] <p>where \\(\\hat{\\pi_j}\\) is the proportion of \\(\\{y^{(i)}\\}\\) in class \\(j\\), and \\(\\hat{\\Sigma_j}\\) is the sample covariance matrix(1) of all observations in class \\(j\\). </p> <ol> <li>A sample of numbers is taken from a larger population of numbers, where \"population\" indicates not number of people but the entirety of relevant data, whether collected or not. The sample mean is the average value of the sample. The sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#compute-pymid-xhattheta-and-prediction","title":"Compute \\(p(y\\mid x,\\hat{\\theta})\\) and Prediction","text":"<p>For the 3rd step, we need to compute \\(p(y\\mid x,\\hat{\\theta})\\) and make predictions. For a new \\(x_*\\) coming, we predict that \\(y=j\\) if </p> \\[ p(y=j\\mid x_*,\\theta) &gt; p(y=l\\mid x_*,\\theta), \\] <p>for all \\(l \\neq j\\), that is </p> \\[ \\hat{f}(x_*) = \\operatorname*{arg\\, max}_{j\\in \\{1,\\dots,K\\}}\\, p(y=j\\mid x_*, \\theta).  \\] <p>Computing \\(p(y=j\\mid x, \\theta)\\) is actually similar with the computation in the motivation for the logistic function in logistic regression. </p> \\[ \\begin{aligned} p_\\theta(y=j\\mid x)  &amp;= \\frac{p(y=j,x)}{x}, \\\\ &amp;= \\frac{p(x\\mid y=j)p(y=j)}{p(x)} \\\\ &amp;= \\frac{p(x\\mid y=j)p(y=j)}{\\sum_{l=1}^{K}p(y=l,x)} \\\\ &amp;= \\frac{p(x\\mid y=j)p(y=j)}{\\sum_{l=1}^{K}p(x \\mid y=l) p(y=l)}. \\end{aligned} \\] <p>For QDA, since \\(x \\mid y=l \\sim \\mathcal{N}(\\mu_l,\\Sigma_l)\\), we have </p> \\[ p_\\theta(y=j\\mid x) = \\frac{\\frac{1}{\\sqrt{(2\\pi)^p \\operatorname{det}(\\Sigma_j)}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_j\\right)^T\\Sigma_j^{-1}\\left(x^{(i)}-\\mu_j\\right)\\right)\\pi_j}{\\sum_{l=1}^K\\frac{1}{\\sqrt{(2\\pi)^p \\operatorname{det}(\\Sigma_l)}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_l\\right)^T\\Sigma_l^{-1}\\left(x^{(i)}-\\mu_l\\right)\\right)\\pi_l},  \\] <p>which is indeed the softmax function we use in logistic regression. </p> <p>In fact, we don't need to exactly compute \\(p_\\theta(y=j\\mid x)\\): we predict \\(y=j\\) if for any \\(t\\neq j\\), </p> \\[ \\begin{aligned} &amp; p(y=j\\mid x) &gt; p(y=t\\mid x)\\\\ \\Leftrightarrow&amp; \\frac{p(y=j,x)}{p(x)}&gt;\\frac{p(y=t,x)}{p(x)} \\\\ \\Leftrightarrow&amp; p(y=j,x) &gt; p(y=t,x) \\\\ \\Leftrightarrow&amp; \\frac{1}{\\sqrt{(2\\pi)^p \\operatorname{det}(\\Sigma_j)}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_j\\right)^T\\Sigma_j^{-1}\\left(x^{(i)}-\\mu_j\\right)\\right)\\pi_j &gt; \\frac{1}{\\sqrt{(2\\pi)^p \\operatorname{det}(\\Sigma_t)}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_t\\right)^T\\Sigma_t^{-1}\\left(x^{(i)}-\\mu_t\\right)\\right)\\pi_t \\\\ \\Leftrightarrow&amp; \\ln \\pi_j -\\frac{1}{2}\\ln \\left(\\operatorname{det}(\\Sigma_j)\\right) - \\frac{1}{2}(x-\\mu_j)^T\\Sigma_j^{-1}(x-\\mu_j) &gt; \\ln \\pi_t -\\frac{1}{2}\\ln \\left(\\operatorname{det}(\\Sigma_t)\\right) - \\frac{1}{2}(x-\\mu_t)^T\\Sigma_t^{-1}(x-\\mu_t) \\\\ \\Leftrightarrow&amp; {\\color{red} \\ln \\frac{\\pi_j}{\\pi_t} + \\frac{1}{2}\\ln \\frac{\\operatorname{det}(\\Sigma_t)}{\\operatorname{det}(\\Sigma_j)} - \\frac{1}{2}\\mu_j^T\\Sigma_j^{-1}\\mu_j + \\frac{1}{2} \\mu_t^T\\Sigma_t^{-1}\\mu_t} + x^T{\\color{green} \\left(\\Sigma_j^{-1}\\mu_j-\\Sigma_t^{-1}\\mu_t\\right)} + x^T{\\color{blue} \\left(\\Sigma_t^{-1} - \\Sigma_j^{-1}\\right)}x&gt;0.  \\end{aligned} \\] <p>Denoting the \\(\\color{red}\\text{red}\\) term as \\(c\\in\\mathbb{R}\\), the \\(\\color{green}\\text{green}\\) term as \\(b\\in\\mathbb{R}^p\\), and the \\(\\color{blue}\\text{blue}\\) term as \\(A\\in\\mathbb{R}^{p\\times p}\\), we obtain </p> \\[ c+b^Tx+x^TAx &gt;0.  \\] <p>This is called quadratic discriminant analysis assuming that the log odds of the posterior probabilities (i.e. \\(p(y=j\\mid x)\\)) is quadratic. </p> <p>For LDA, since \\(\\Sigma_j=\\Sigma_t\\) for any \\(j,t\\), we have \\(A=0\\) and the inequality becomes</p> \\[ x^Tb+c&gt;0.  \\] <p>For NB, since \\(\\Sigma_j\\) is diagonal matrix, the term </p> \\[ x^TAx = \\sum_{i=1}^n A_{ii} x_i^2, \\] <p>where no terms \\(x_ix_j\\) for \\(i\\neq j\\) appears in the classifier. </p> <p>Based on the above expressions, the decision boundaries between two classes (i.e. the above inequalities becomes equal to 0) of QDA and NB are quadratic and the one of LDA is linear. The following figure shows an example.  </p>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#k-classes-classification-boundaries","title":"\\(K\\)-Classes Classification Boundaries","text":"<p>For \\(K&gt;2\\), we follow the same classification rule: select (predict) the class \\(j\\) where </p> \\[ p(y=j\\mid x) = \\max_{i\\in\\{1,\\dots,n\\}} p(y=i\\mid x).  \\] <p>Take \\(K=3\\) as an example. The decision boundaries divide the space into 3 subsets:  </p> \\[ \\mathcal{X} = \\bigcup_{j\\in\\{1,2,3\\}}\\left(\\bigcap_{t\\neq j}\\left\\{x\\left|\\, \\right. p(y=j\\mid x)&gt;p(y=t\\mid x)\\right\\}\\right), \\] <p>which is given by the following figure.  </p> <p></p>"},{"location":"notes/lecture_notes/stat541/stat541_week6/#compare-with-logistic-regression","title":"Compare with Logistic Regression","text":"<p>We should choose between LDA, QDA, NB, and Log regression via cross-validation. Generally, it is known that </p> <ul> <li> <p>When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable while generative models do not suffer from this problem.</p> </li> <li> <p>If \\(p(x\\mid y=j)\\) is approximately Gaussian, LDA, QDA, and NB are a bit better. If this fails, logistic regression is better. </p> </li> <li> <p>Logistic regression with feature transformation can be similar to LDA, QDA, and NB. Logistic regression with no feature transformation is similar to LDA, and both give linear decision boundaries. Logistic regression with quadratic feature transformation (with intersection terms) is similar to QDA. Logistic regression with quadratic feature transformation (without intersection terms) is similar to NB. </p> </li> <li> <p>Generative models are computationally easier than Log regression: To use QDA,LDA, and NB in practice, we estimate \\(A,b,c\\) by plugging the estimators for \\(\\mu,\\Sigma,\\pi\\), which is easy to compute. On the contrast, we need optimization techniques, such as GD and NR, in Log regression.  </p> </li> <li> <p>QDA can run into problems when some classes only have a few observations. This is because we have to estimate \\(\\Sigma_j\\) for each class. For example, consider image classification of animals. Observations \\(X\\) is high dimensional but only a couple of images of Walruses. So \\(\\Sigma_{\\rm Walrus}\\) is tough to estimate, and QDA performs poorly. </p> </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week8/","title":"Week 8","text":""},{"location":"notes/lecture_notes/stat541/stat541_week8/#spline","title":"Spline","text":"<p>Recall the linear regression model</p> \\[ f(x)=\\sum_{i=0}^p \\beta_i f_i(x),  \\] <p>where the \\(f_i\\)'s are simply feature transformations of \\(x\\). We will now want to consider different feature transformations and even optimize over the \\(f_i\\)'s. (As an example, recall that in polynomial regression we essentially had \\(\\left.f_1(x)=1, f_2(x)=x, \\dots, f_i(x)=x^{p}\\right)\\). </p> <p>The idea for splines is to fit two polynomial regressions on different pieces of the data. Then we will connect them in a nice way.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#basis-function","title":"Basis Function","text":"<p>The idea is to have at hand a family of functions or feature transformations that can be applied to a variable \\(X\\) : \\(b_1(X), b_2(X), \\dots, b_K(X)\\). Instead of fitting a linear model in \\(X\\), we fit the model</p> \\[ y^{(i)}=\\beta_0+\\beta_1 b_1\\left(x^{(i)}\\right)+\\beta_2 b_2\\left(x^{(i)}\\right)+\\beta_3 b_3\\left(x^{(i)}\\right)+\\cdots+\\beta_K b_K\\left(x^{(i)}\\right)+\\epsilon_i \\] <p>Note that the basis functions \\(b_1(\\cdot), b_2(\\cdot), \\ldots, b_K(\\cdot)\\) are fixed and known. For polynomial regression, the basis functions are \\(b_j\\left(x^{(i)}\\right)=\\left(x^{(i)}\\right)^j\\), and for piecewise constant functions they are \\(b_j\\left(x^{(i)}\\right)=I\\left(c_j \\leq x^{(i)}&lt;c_{j+1}\\right)\\). </p> <p>To fit two polynomial regressions on different pieces of the data, we consider basis functions:</p> \\[ \\beta_0 I(x&lt;\\theta)+\\beta_1 I(x&lt;\\theta) x+\\beta_2 I(x&lt;\\theta) x^2+\\tilde{\\beta}_0 I(x\\geq\\theta)+\\tilde{\\beta}_1 I(x\\geq\\theta) x+\\tilde{\\beta}_2 I(x\\geq\\theta) x^2 \\] <p>This is not necessarily continous at \\(\\theta\\) though, and we would ideally want a continous function. For example, we consider the linear case, </p> \\[ \\beta_0 I(x&lt;\\theta)+\\beta_1 I(x&lt;\\theta) x+\\tilde{\\beta}_0 I(x\\geq\\theta)+\\tilde{\\beta}_1 I(x\\geq\\theta) x \\]"},{"location":"notes/lecture_notes/stat541/stat541_week8/#linear-quadratic-and-cubic-spline","title":"Linear, Quadratic, and Cubic Spline","text":"<p>We can introduce the function \\(z_{+}=\\max \\{0, z\\}\\), and then consider the function \\(f(x)=\\) \\(\\beta_0+\\beta_1 x+\\beta_2(x-\\theta)_{+}\\)</p> <p>Linear Spline: A linear spline with knots at \\(\\theta_1, \\ldots, \\theta_l\\) is a function of the form</p> \\[ f(x)=\\beta_0+\\beta_1 x+\\beta_2(x-\\theta)_{+}+\\cdots+\\beta_{l+1}\\left(x-\\theta_l\\right)_{+}. \\] <p>This would essentially connect the two linear regressions with a line. </p> <p>We would like to make piecewise polynomials that are both continous and differentiable. So we can move to quadtratic functions.</p> <p>Quadratic Spline: A quadratic spline with knots at \\(\\theta_1, \\ldots, \\theta_l\\) is a function of the form</p> \\[ f(x)=\\beta_0+\\beta_1 x+\\beta_2 x^2 + \\beta_3(x-\\theta)_{+}^2+\\cdots+\\beta_{l+2}\\left(x-\\theta_l\\right)_{+}^2 \\] <p>Cubic Spline: A cubic spline with knots at \\(\\theta_1, \\ldots, \\theta_l\\) is a function of the form</p> \\[ f(x)=\\beta_0+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3 + \\beta_4(x-\\theta)_{+}^3+\\cdots+\\beta_{l+3}\\left(x-\\theta_l\\right)_{+}^3 \\] <p>Quadratic splines are continous and differentiable, but people generally use cubic splines. Cubic splines ensure that \\(f\\) is continous and has second order derivatives. This makes the cubic spline flexible and often less variable than fitting high degree polynomials.</p> <p>The next question is how do we choose the knots, \\(\\theta_i\\) ?</p>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#knot-selection","title":"Knot Selection","text":"<p>We may look at the plot and pick the points where the \"behavior\" of the points changes. More formally, we may take the following steps: </p> <ol> <li>Pick nots at the quantiles or percentiles of \\(x^{(1)},\\dots,x^{(n)}\\). </li> <li>Decide how many knots to choose -- look at AIC or BIC and do model selection: <ul> <li>Compute the AIC of a model with and without a knot and compare the models to determine if we need it. </li> </ul> </li> <li>Use cross-validation to select between candidate knot choices. </li> </ol> <p>For AIC/BIC, we need the number of parameters(1): if no constraints, there are \\(4(l+1)\\); there are 3 constraints(2) at each knot; therefore, there are \\(l+4\\) parameters. </p> <ol> <li>Here we consider cubic spline with \\(l\\) knots.</li> <li>Three constraints: continuous, continuous first and second order derivatives. </li> </ol> <p>We can use something called smoothing splines to improve this process. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#smoothing-splines","title":"Smoothing Splines","text":"<p>We will first start off with the optimization problem \\(\\left(x, x^{(i)} \\in \\mathbb{R}\\right)\\) :</p> \\[ \\hat{f}(x)=\\operatorname*{arg\\, min} _f \\left(\\sum_{i=1}^n\\left(f\\left(x^{(i)}\\right)-y^{(i)}\\right)^2+\\lambda \\int_{-\\infty}^{\\infty}\\left(f^{\\prime \\prime}(x)\\right)^2 d x\\right), \\] <p>where \\(\\lambda\\) is a fixed smoothing parameter. The first term measures closeness to the data, while the second term penalizes curvature in the function, and \\(\\lambda\\) establishes a tradeoff between the two(1). </p> <ol> <li>The penalty is 0 for linear functions of \\(f\\). If our function has curvature, the penalty applies, and the penalty will be larger for more 'wiggly' functions. </li> </ol> <p>Two special cases are:</p> <ul> <li>\\(\\lambda=0: f\\) can be any function that interpolates the data.</li> <li>\\(\\lambda=\\infty\\) : the simple least squares line fit, since no second derivative can be tolerated.</li> </ul> <p>Remarkable result: The minimizer of the optimization problem is a (regularized) natural cubic spline with knots (at every data point): \\(\\theta_1=x^{(1)},\\dots, \\theta_n=x^{(n)}\\). </p> <p>Natural Cubic Splines: A natural cubic spline is a cubic spline that is linear beyond the boundary knots, given by </p> \\[ f^{\\prime \\prime}(x)=0 \\quad \\text { for } x&lt;\\theta_1 \\text { or } x&gt;\\theta_m.  \\] <p>Similarly to cubic splines, natural cubic splines can be represented with a basis </p> \\[ f(x)=\\sum_{i=1}^m \\beta_i f_i(x).  \\] <p>For natural cubic splines with \\(m\\) knots, the number of parameters is \\(m\\) -- recall a cubic spline with \\(m\\) knots has \\(m+4\\) parameters, and in a natural cubic spline, we have 4 boundary constraints (2(1) on each side), so it will have \\(m\\) parameters.</p> <ol> <li>Two constraints: second and third order derivative being 0 at the boundary. </li> </ol> <p>Since we are putting a knot at every single data point, one would think we would have a very wiggly curve. However, we use the idea of regularization to mitigate this. First we will plug the basis functions into the minimization problem, and we have a minimization problem over the \\(\\beta_i\\)'s:</p> \\[ \\begin{aligned} \\hat{f}(x) &amp; =\\operatorname*{arg\\, min} _{\\beta_1, \\ldots, \\beta_m} \\sum_{i=1}^n\\left(\\sum_{j=1}^m \\beta_j f_j\\left(x^{(i)}\\right)-y^{(i)}\\right)^2+\\lambda \\int\\left(\\sum_{j=1}^m \\beta_j f_j^{\\prime \\prime}(x)\\right)^2 d x \\\\ &amp; =\\operatorname*{arg\\, min} _{\\beta_1, \\ldots, \\beta_m} \\sum_{i=1}^n\\left(\\sum_{j=1}^m \\beta_j f_j\\left(x^{(i)}\\right)-y^{(i)}\\right)^2+\\lambda \\int\\left(\\sum_{k=1}^m \\sum_{j=1}^m \\beta_j \\beta_k f_k^{\\prime \\prime}(x) f_j^{\\prime \\prime}(x)\\right) d x \\\\ &amp; =\\operatorname*{arg\\, min} _{\\beta_1, \\ldots, \\beta_m} \\sum_i\\left(\\sum_j \\beta_j f_j\\left(x^{(i)}\\right)-y^{(i)}\\right)^2+\\lambda \\int\\left(\\beta_j \\beta_k \\sum_k \\sum_j f_k^{\\prime \\prime}(x) f_j^{\\prime \\prime}(x)\\right) d x.  \\end{aligned} \\] <p>Define a \\(m\\times m\\) matrix \\(\\Omega\\) with \\(\\Omega_{ij}=\\int f^{\\prime \\prime}_i f^{\\prime \\prime}_j \\,\\mathrm{d}x\\), and a design matrix</p> \\[ X=\\begin{bmatrix} f_1\\left(x^{(1)}\\right) &amp; f_2\\left(x^{(1)}\\right) &amp; \\ldots &amp; f_m\\left(x^{(1)}\\right) \\\\ f_1\\left(x^{(2)}\\right) &amp; f_2\\left(x^{(2)}\\right) &amp; \\ldots &amp; f_m\\left(x^{(2)}\\right) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ f_1\\left(x^{(n)}\\right) &amp; f_2\\left(x^{(n)}\\right) &amp; \\ldots &amp; f_m\\left(x^{(n)}\\right) \\end{bmatrix}, \\] <p>and let \\(\\boldsymbol{\\beta}=\\left(\\beta_1, \\ldots, \\beta_m\\right)^T\\). Then the optimization problem becomes</p> \\[ \\operatorname*{arg\\, min} _{\\boldsymbol{\\beta}}\\|Y-X \\boldsymbol{\\beta}\\|^2+\\lambda \\boldsymbol{\\beta}^T \\Omega \\boldsymbol{\\beta}, \\] <p>and thus this implies that</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\rm smoothing spline }=\\left(X^T X+\\lambda \\Omega\\right)^{-1} X^T Y.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week8/#degrees-of-freedom-and-smoother-matrices","title":"Degrees of Freedom and Smoother Matrices","text":"<p>We have not yet indicated how \\(\\lambda\\) is chosen for the smoothing spline. To illustrate the meaning of \\(\\lambda\\), we first recall in linear regression (with \\(p\\) features), where predictions at observed points are</p> \\[ \\hat{Y}=X \\hat{\\beta}=X \\left(X^T X\\right)^{-1} X^T Y.  \\] <p>We can interpret each \\(\\hat{Y}_i\\) as a linear combination of the \\(y^{(i)}\\) 's(1), i.e. \\(\\hat{Y}_i=\\sum_{j=1}^n H_{i j} Y_j\\), where </p> <ol> <li>If \\(H_{i i} \\approx 1, H_{i j: i \\neq j} \\approx 0\\), we can interpolate the data points for a flexible fit.</li> </ol> \\[ H = X \\left(X^T X\\right)^{-1} X^T.  \\] <p>Recall for any two square matrices, we have \\(\\operatorname{tr}(AB) = \\operatorname{tr} (BA)\\). Then we have </p> \\[ \\operatorname{tr}(H)=\\operatorname{tr}\\left(X\\left(X^T X\\right)^{-1} X^T\\right)=\\operatorname{tr}\\left(\\left(X^T X\\right)^{-1} X^T X\\right)=p, \\] <p>which is the number of parameters. We will call the \\(\\operatorname{tr}(H)\\) as the number of effective parameters(1).  By analogy we now define the number of effective parameters (a.k.a. the effective degrees of freedom) of a smoothing spline. </p> <ol> <li>The linear operator \\(H\\) is a projection operator, also known as the hat matrix in statistics. The expression \\(\\operatorname{trace}\\left(H\\right)\\) gives the dimension of the projection space, which is also the number of basis functions, and hence the number of parameters involved in the fit. </li> </ol> <p>For smoothing splines, we have </p> \\[ \\hat{\\beta}=\\left(X^T X+\\lambda \\Omega\\right)^{-1} X^T Y \\Longrightarrow \\hat{Y}=X \\hat{\\beta}=X\\left(X^T X+\\lambda \\Omega\\right)^{-1} X^T Y.  \\] <p>We define the smoothing matrix, \\(S_\\lambda=X\\left(X^T X+\\lambda \\Omega\\right)^{-1} X^T\\), so that we have: \\(\\hat{Y}=S_{\\lambda} Y\\). The effective number of parameters is \\(\\operatorname{tr}\\left(S_{\\lambda}\\right)\\), which generalizes the result for linear regression. Also note that the effective degrees of freedom here may not be an integer. </p> <p>The effective degrees of freedom(1) is a heuristic parameter count, which helps us compare the smoothing splines with other regression models, like polynomial regression. </p> <ol> <li>Although a natural cubic spline with \\(n\\) knots has \\(n\\) parameters, the penalty term in the minimization problem regularizes the minimizer resulting in the smoothing spline has less than \\(n\\) effective number of parameters. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#summary","title":"Summary","text":"<p>Overall smoothing splines provide nice and flexible fits and the choice of \\(\\lambda\\) is straightforward as we don't have to choose knot locations. </p> <p>The main drawback is that the design matrix \\(X=\\in \\mathbb{R}^{n\\times n}\\), so if \\(n\\) is large then computing \\(\\left(X^TX + \\lambda\\Omega\\right)^{-1}\\) is expensive. In polynomial regression, the number of features, \\(p\\), does not depend on \\(n\\), but for smoothing splines it does.  </p>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#extensions","title":"Extensions","text":"<p>MARS: Consider interaction effects for multivariate features. MARS use basis functions of the form of products of \\(\\left(x_i-\\theta_i\\right)_{+}\\). For example, we can form basis functions over \\(p=2\\) of the form: </p> \\[ f\\left(x_1, x_2\\right)=\\left(x_1-\\theta_1\\right)_{+}\\cdot\\left(x_2-\\theta_2\\right)_{+}.  \\] <p>Logistic Regression: Splines can also be used in logistic regression. All we need to do is change our basis functions:</p> \\[ \\ln \\left(\\frac{p(x)}{1-p(x)}\\right)=\\sum_{i=1}^l \\beta_i f_i(x), \\] <p>where we choose a spline basis for \\(f_i\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week8/#kernel-smoothing-and-regression","title":"Kernel Smoothing and Regression","text":"<p>With smoothing splines we would try to fit a polynomial over each fixed window determined by the knots. For kernel method we will instead use a window that slides based on the point \\(x_*\\) where we want to make a prediction. We can then predict \\(y_\u2217\\) by averaging over all points in our window. Here we introduce the boxcar kernel function \\(K_\\lambda:\\mathbb{R}\\times\\mathbb{R}\\rightarrow [0,\\infty)\\), given by </p> \\[ K_\\lambda(x,y) = I(|x-y|\\leq \\lambda).  \\] <p>Then we make prediction at \\(x_*\\) via</p> \\[ \\begin{aligned} \\hat{f}\\left(x_*\\right) &amp; =\\text { Average of } y^{(i)} \\text { for } x^{(i)} \\in\\left[x_*-\\lambda, x_*+\\lambda\\right] \\\\ &amp; =\\frac{\\sum y^{(i)} I\\left(-\\lambda \\leq x_*-x^{(i)} \\leq \\lambda\\right)}{\\sum I\\left(-\\lambda \\leq x_*-x^{(i)} \\leq \\lambda\\right)} \\\\ &amp; =\\frac{\\sum K_\\lambda\\left(x^{(i)}, x_*\\right) y^{(i)}}{\\sum K_\\lambda\\left(x^{(i)}, x_*\\right)}.  \\end{aligned} \\]"},{"location":"notes/lecture_notes/stat541/stat541_week8/#general-kernel-smoothing","title":"General Kernel Smoothing","text":"<p>With the boxcar kernel the whole fit can be jagged because of the discontinuity of the kernel. Rather than give all the points in the neighborhood equal weight, we can assign weights that die off smoothly with distance from the target point: </p> <ul> <li>Gaussian Kernel: \\(\\displaystyle K_\\lambda(x, y)=\\exp \\left(\\frac{-(x-y)^2}{\\lambda}\\right)\\). </li> <li>Epanechnikov kernel: \\(\\displaystyle K_\\lambda(x, y)=\\left(1 - \\left(\\frac{x-y}{\\lambda}\\right)^2\\right)_+\\). </li> </ul> <p>The following figure shows the images of boxcar kernel with \\(K_1(x,0)\\), Gaussian kernel with \\(K_2(x,0)\\), and Epanechnikov kernel \\(K_3(x,0)\\).  </p> <p>The interpretation of kernel \\(K_\\lambda(x, y)\\) is that it measures how similar or close \\(x\\) is to \\(y\\). Generally, kernels have the form:</p> \\[ K_\\lambda(x, y)=f\\left(\\frac{|x-y|}{\\lambda}\\right), \\] <p>where \\(f\\) is non-increasing and non-negative. \\(\\lambda\\) is called the bandwidth parameter and determines the spread of the kernel. </p> <p>The general kernel smoothed prediciton is defined as:</p> \\[ \\hat{f}\\left(x_*\\right)=\\frac{\\sum_{i=1}^n y^{(i)} K_\\lambda\\left(x_*, x^{(i)}\\right)}{\\sum_{i=1}^n K_\\lambda\\left(x_*, x^{(i)}\\right)}.  \\] <p>If we define a function \\(w_i\\left(x_*\\right)=\\frac{K_\\lambda\\left(x_*, x^{(i)}\\right)}{\\sum_{i=1}^n K_\\lambda\\left(x_*, x^{(i)}\\right)}\\), we have a bunch of weights that sum to 1 , i.e., \\(\\sum w_i\\left(x_*\\right)=1\\), and \\(w_i \\geq 0\\). Thus, the kernel smoother is:</p> \\[ \\hat{f}\\left(x_*\\right)=\\sum_{i=1}^n w_i\\left(x_*\\right) y^{(i)}.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week8/#effective-degrees-of-freedom","title":"Effective Degrees of Freedom","text":"<p>The choice of kernel function is actually not super important, but the choice of the bandwidth parameter is important: Large \\(\\lambda\\) implies lower variance (averages over more observations) but higher bias(1). Small \\(\\lambda\\) gives the opposite.</p> <ol> <li>we essentially assume the true function is constant within the window. </li> </ol> <p>Note that the predictions at our observed data points, \\(x^{(i)}\\) have the form</p> \\[ \\hat{y}_i=\\hat{f}\\left(x^{(i)}\\right)=\\sum_{j=1}^n w_j\\left(x^{(i)}\\right) y^{(i)},  \\] <p>which holds for all \\(i=1, \\ldots, n\\). Thus,</p> \\[ \\hat{Y}=\\begin{bmatrix} w_1\\left(x_1\\right) &amp; w_2\\left(x_1\\right) &amp; \\cdots &amp; w_n\\left(x_1\\right) \\\\ w_1\\left(x_2\\right) &amp; w_2\\left(x_2\\right) &amp; \\cdots &amp; w_n\\left(x_2\\right) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_1\\left(x_n\\right) &amp; w_2\\left(x_n\\right) &amp; \\cdots &amp; w_n\\left(x_n\\right) \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\] <p>Usually we can just call the weight matrix \\(W\\). Thus, we have that \\(\\hat{Y}=W Y\\). We define a similar notion of the effective degrees of freedom for kernel smoothing as we did for splines:</p> \\[ \\operatorname{tr}(W)=\\sum_{i=1}^n w_i\\left(x^{(i)}\\right).  \\] <p>This can be thought of as the approximation of how many parameters are in the model.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/","title":"Week 9","text":""},{"location":"notes/lecture_notes/stat541/stat541_week9/#local-polynomial-regression","title":"Local Polynomial Regression","text":"<p>We can think of local polynomial regression as a generalization of kernel smoothing. Given an \\(x_*\\), we want to find \\(\\hat{\\beta}_0\\left(x_*\\right)\\) and \\(\\hat{\\beta}_1\\left(x_*\\right)\\) using least squares, but we only include data points within a \\(\\lambda\\)-window of \\(x_*\\). We can write this as:</p> \\[ \\min _{\\beta_0, \\beta_1} \\sum_{i=1}^n K_\\lambda\\left(x_*, x^{(i)}\\right)\\left(y^{(i)}-\\beta_0-\\beta_1\\left(x_*\\right)\\right)^2 \\] <p>Once we find \\(\\hat{\\beta}_0\\left(x_*\\right)\\), and \\(\\hat{\\beta}_1\\left(x_*\\right)\\), we can predict </p> \\[ \\hat{f}\\left(x_*\\right)=\\hat{\\beta}_0\\left(x_*\\right)+\\hat{\\beta}_1\\left(x_*\\right)\\left(x_*\\right) \\] <p>We do not need to necessarily use a boxcar kernel, we can use any kernel function. We can also incorporate polynomial fits. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#general-formulation-of-local-polynomial-regression","title":"General Formulation of Local Polynomial Regression","text":"<p>Consider</p> \\[ \\begin{aligned} &amp;\\left(\\hat{\\beta}_0\\left(x_*\\right), \\hat{\\beta}_1\\left(x_*\\right), \\dots, \\hat{\\beta}_d\\left(x_*\\right)\\right) \\\\ &amp;=\\operatorname*{arg\\, min}_{\\beta_0, \\dots, \\beta_d} \\sum_{i=1}^n K_\\lambda\\left(x_*, x^{(i)}\\right)\\left(y^{(i)}-\\beta_0-\\beta_1\\left(x^{(i)}\\right)-\\dots-\\beta_d\\left(x^{(i)}\\right)^d\\right)^2. \\end{aligned} \\] <p>The prediction is given by </p> \\[ \\hat{f}\\left(x_*\\right)=\\hat{\\beta}_0\\left(x_*\\right)+\\sum_{i=1}^d \\hat{\\beta}_i\\left(x_*\\right) x_*^i. \\] <p>The optimization is a weighted least squares problem, which we can formulate by defining \\(X\\) to be the polynomial regression design matrix:</p> \\[ K=\\begin{bmatrix} K_\\lambda\\left(x_1, x_*\\right) &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; \\dots &amp; K_\\lambda\\left(x_n, x_*\\right) \\end{bmatrix} \\] <p>Then the optimization problem is the same as </p> \\[ \\operatorname*{arg\\, min}_\\beta(Y-X \\beta)^T K(Y-X \\beta), \\] <p>where the minimizer is given by</p> \\[ \\hat{\\beta}=\\left(X^T K X\\right)^{-1} X^T K Y.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week9/#advantages-of-local-regression","title":"Advantages of Local Regression","text":"<ul> <li>Fit has less bias than kernel smoothing as kernel smoothing will trim peaks and troughs. As averaging the data points within the window, the prediction made by kernel smoothing will be smaller than the highest (larger than the smallest) points, while local regression may achieve lower bias at the minimum or maximum points.  </li> <li>A local quadratic regression will have less bias as we fit a quadratic polynomial in the window. </li> <li>Kernel smoothing fails at picking up boundary trends. As kernel smoothing continues linearly, local regression is a bit better. The following figure shows when outside the boundary, kernel smoothing can only will undershoot the trend while local (polynomial) regression can avoid this issue.  </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#k-nearest-neighbors-regression","title":"K-Nearest Neighbors Regression","text":"<p>We want to make a prediction at a point, \\(x_\u2217 \\in \\mathbb{R}\\). We will look at the training set, \\(x^{(1)},\\dots,x^{(n)}\\) and find the \\(K\\) observations that are closest (smallest values of \\(|x_*-x^{(i)}|\\)) to \\(x_\u2217\\). We can then predict \\(y_\u2217\\) by averaging over the \\(K\\) observations. In this, \\(K\\) plays a similar role to \\(\\lambda\\) in a kernel smoother since it determines the complexity.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#motivation","title":"Motivation","text":"<p>If we have a lot of data, and we apply a kernel smoother to one region in the data, where there are sparse amounts of points, only a few points will be captured by the weighting (because the kernel function is small). This gives us a pretty variable fit around that particular region (high variance). In another region where we have lots of points densley sampled, we might want the bandwidth of the kernel to be smaller. That is, we want to use different bandwidths based on how many points are nearby, which is effectively what KNN does. It can be viewed as a kernel smoother where the bandwidth, \\(\\lambda\\) depends on \\(x_*\\). </p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#knn-as-a-smoothing-kernel","title":"KNN as a Smoothing Kernel","text":"<ol> <li>Define the distance metric, \\(d\\left(x^{(i)}, x_*\\right)=\\left|x^{(i)}-x_*\\right|\\).</li> <li>Assume \\(d\\left(x^{(1)}, x_*\\right) \\leq d\\left(x^{(2)}, x_*\\right) \\leq \\ldots \\leq d\\left(x^{(n)}, x_*\\right)\\).</li> <li>Let \\(\\lambda\\left(x_*\\right)=d\\left(x^{(K)}, x_*\\right)\\).</li> <li>Define the kernel function as:</li> </ol> \\[ \\begin{aligned} \\hat{f}_{\\mathrm{KNN}}\\left(x_*\\right) &amp;=\\frac{\\sum_{i=1}^n I\\left(d\\left(x^{(i)},x_*\\right) \\leq \\lambda\\left(x_*\\right)\\right) y^{(i)}}{\\sum_{i=1}^n I\\left(d\\left(x^{(i)},x_*\\right) \\leq \\lambda\\left(x_*\\right)\\right)} \\\\ &amp;=\\frac{1}{K}\\sum_i I\\left(d\\left(x^{(i)},x_*\\right) \\leq \\lambda\\left(x_*\\right)\\right) y^{(i)}.  \\end{aligned} \\] <p>We can then write,</p> \\[ \\hat{Y}=\\begin{bmatrix} \\frac{1}{K} &amp; 0 &amp; \\cdots &amp; \\frac{1}{K} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{1}{K} &amp; \\cdots &amp; 0 &amp; \\frac{1}{K} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \\frac{1}{K} &amp; 0 &amp; \\cdots &amp; \\frac{1}{K} &amp; 0 &amp; \\cdots &amp; \\frac{1}{K} \\end{bmatrix} \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ \\vdots \\\\ y^{(n)} \\end{bmatrix}, \\] <p>where this matrix is symmetric and has entries of 0 and \\(\\frac{1}{K}\\)(1), and there are only \\(\\frac{1}{K}\\) on the diagonal. This is a very sparse matrix. The effective degrees of freedom is </p> <ol> <li>Each row (or column) has \\(n-K\\) entries of 0 and \\(K\\) entries of \\(\\frac{1}{K}\\). In general, the weighting matrix will not have strings of repeated \\(\\frac{1}{K}\\) in the rows, there may be intermittent 0's.</li> </ol> \\[ \\operatorname{tr}(W)=\\frac{n}{K}.  \\] <p>This suggests that \\(K\\) should scale proportionally to \\(n\\) to prevent overly complex fits. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#advantages-of-knn","title":"Advantages of KNN","text":"<ul> <li> <p>With enough data can fit any function and give very flexible fits (may need lots of data). </p> </li> <li> <p>Interpretable (e.g. KNN is easy to make sense of because we\u2019re just taking the \\(K\\) nearest data points and averaging them). </p> </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#multivariate-feature-spaces","title":"Multivariate Feature Spaces","text":""},{"location":"notes/lecture_notes/stat541/stat541_week9/#multivariate-knn","title":"Multivariate KNN","text":"<p>In KNN, to extend to spaces beyond the real line, we simply need to introduce a distance over the feature space, \\(d(x, y)\\), for any \\(x, y \\in \\mathbb{R}^p\\). If we take \\(d\\) to be Euclidean distance, we can compute</p> \\[ d_i\\left(x_i, x_*\\right)=\\left\\|x_i-x_*\\right\\| \\] <p>to find the K-smallest \\(d_i\\) 's and the prediction is the average.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#multivariate-kernel-smoothing","title":"Multivariate Kernel Smoothing","text":"<p>We can extend the notion of our Kernel function to the multivariate setting:</p> \\[ K_\\lambda: \\mathbb{R}^{p}\\times \\mathbb{R}^{p} \\rightarrow[0, \\infty) \\] <p>Often times the multivariate kernel has the form</p> \\[ K\\left(\\frac{\\|\\boldsymbol{x}-\\boldsymbol{y}\\|}{\\lambda}\\right).  \\] <p>FOr example, the Gaussian Kernel (a.k.a. Radial Basis-Function, RBF) is defined as</p> \\[ K_\\lambda(x, y)=\\exp \\left(-\\frac{\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^2}{\\lambda}\\right).  \\] <p>The kernel smoother predicts </p> \\[ \\hat{f}(x_*) = \\frac{\\sum_{i=1}^n K_{\\lambda}(x_*,x^{(i)})y^{(i)}}{\\sum_{i=1}^n K_{\\lambda}(x_*,x^{(i)})}.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week9/#scaling","title":"Scaling","text":"<p>In multivariate kernels or distances, scaling of features is very important. For example, suppose we have information about a player (Scottie Barnes) from the Toronto Raptors. Say his salary is 5 Million CAD, and his height is 200 cm, and we don't know his points. We can find the 'closest' player on the Raptors to him by using the Euclidean distance from the point \\((5000000,200)\\), where the first component is salary, and the second is heights in cm. The issue with this approach is that the salary is much greater than the height, so any distances we compute will only really be affected by the salaries, since they are so much bigger. We need to scale our data by its standard deviation to account for this.</p> <p>We will scale every feature, \\(x_i\\) by its standard deviation.</p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#classification","title":"Classification","text":"<p>It turns out that splines, local regression, and KNN can extend to classification settings.</p> <ul> <li> <p>For splines and local regression, we can combine them with logistic regression to solve classification problems, i.e. fitting the log-odds ratio using splines and local regression. </p> </li> <li> <p>For KNN, just predict the class that appears most often when looking at the \\(K\\)-nearest neighbors. We can use this to define a decision boundary by computing the KNN at every point in the plane (assume \\(p=2\\)). </p> </li> </ul>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#memory-based-method","title":"Memory-Based Method","text":"<p>The methods, KNN, smoothing splines, and local regression, are memory-based, which means the algorithm \"memorize\" the entire training data set when making predictions(1). The computational complexity of making a new prediction for memory-based methods is \\(O(n)\\)(2). </p> <ol> <li>For example, for KNN, we can't throw away any data point as it may the nearest neighbor. </li> <li>KNN need to query \\(O(n)\\) data points to find the nearest neighbors. </li> </ol> <p>On the other hand, linear regression only requires the coefficients \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_p\\) to make predictions, which compresses the training data into \\(p+1\\) parameters. Then the computational complexity is roughly \\(O(p)\\). </p> <p>Therefore, in terms of making predictions on-the-fly, memory-based methods are not always the best since the computation will be expensive if \\(n\\) is large. </p>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<p>Curse of dimensionality occurs when the features are too many. In high dimensions, there are a few points that are close to each other(1). Thus, it is impossible to simultaneously maintain localness (i.e. low bias) and a sizable sample in the neighborhood (i.e. low variance) as the dimension increases, without the total sample size increasing exponentially in \\(p\\)(2).</p> <ol> <li>For \\(p=1\\), let \\(x\\) be a uniform random variable on \\([0,1]\\) and we consider the distance between 0 and \\(x\\). The probability of \\(x\\in [0,\\epsilon]\\) is \\(\\epsilon\\). For higher dimension \\(p=k\\), the probability of the uniform variable \\(x\\) being in \\([0,\\epsilon]^k\\) becomes \\(\\epsilon^k\\), which shows points that are within \\(\\epsilon\\)-distance to 0 is getting less when the space dimension grows. </li> <li>Conversely, if we increase the number of features without adding new data to training set, the predictions made by our model would likely be worse. </li> </ol>"},{"location":"notes/lecture_notes/stat541/stat541_week9/#generalized-additive-models-gams","title":"Generalized Additive Models (GAMs)","text":"<p>Additive models provide a useful extension of linear models, making them more flexible while still retaining much of their interpretability. These allow aus to model complex univariate relationships (with multivariates features). </p> <p>Assume that there are no interaction effects and a GAM assumes </p> \\[ f(x_1,\\dots,x_p) = \\sum_{i=1}^p f_i(x_i),  \\] <p>where we allow for complicated non-parametric \\(f_i\\), for instance, \\(f_i\\) are smoothing splines or local regression. Note that </p> \\[ \\frac{\\partial}{\\partial x_i} \\frac{\\partial}{\\partial x_j} f(x_1,\\dots,x_p) = 0, \\quad, \\text{ for } i\\neq j.  \\]"},{"location":"notes/lecture_notes/stat541/stat541_week9/#backfitting","title":"Backfitting","text":"<p>The backfitting procedure for fitting these models is simple and modular, allowing one to choose a fitting method appropriate for each input variable.The procedure is given by following steps: </p> <ol> <li>Start off with a simple fit for \\(f(x_1,\\dots,x_p)\\), for instance, set this equal to \\(\\displaystyle \\frac{1}{n}\\sum_{i=1}^{n} y^{(i)}\\) (constant). </li> <li>Assume that we have fits for \\(\\hat{f}_2(x_2),\\dots,\\hat{f}_p(x_p)\\). To find the optimal \\(f_1\\), define \\(\\displaystyle \\tilde{y}^{(i)} = y^{(i)} - \\sum_{j=2}^{p} \\hat{f}_j\\left(x^{(i)}_j\\right)\\). </li> <li>Fit our \\(\\hat{f}_1\\) using the training data \\(\\left(x_1^{(1)},\\tilde{y}^{(1)}\\right),\\dots,\\left(x_1^{(n)},\\tilde{y}^{(n)}\\right)\\). </li> <li>To find \\(\\hat{f}_2\\) given \\(\\hat{f}_1,\\hat{f}_3,\\dots,\\hat{f}_p\\), redefine \\(\\displaystyle \\tilde{y}^{(i)} = y^{(i)} - \\hat{f}_1\\left(x^{(i)}_1\\right) - \\sum_{j=3}^{p} \\hat{f}_j\\left(x^{(i)}_j\\right)\\). </li> <li>Fit our \\(\\hat{f}_1\\) using the training data \\(\\left(x_2^{(1)},\\tilde{y}^{(1)}\\right),\\dots,\\left(x_2^{(n)},\\tilde{y}^{(n)}\\right)\\).</li> <li>Repeat this process for \\(\\hat{f}_3,\\hat{f}_4,\\dots,\\hat{f}_p\\), and then come back to \\(\\hat{f}_1\\) and repeat the process. </li> </ol> <p>Example: If we wanted the \\(f_k\\) to be smoothing splines, then the step becomes </p> \\[ f_k(x) = \\sum_{i=1}^m \\beta_i g_i(x_k), \\] <p>where \\(g_i\\) is the spline basis functions. Obtain \\(\\beta_i\\) by solving the smoothing spline minimization problem:</p> \\[ \\left(\\hat{\\beta}_1,\\dots, \\hat{\\beta}_m\\right) = \\operatorname*{arg\\, min}_{\\beta_1,\\dots,\\beta_m} \\left(\\sum_{i=1}^n \\left(\\tilde{y}^{(i)} - \\sum_{j=1}^m \\beta_j g_j(x^{(i)}_k)\\right)^2 + \\lambda\\int_{-\\infty}^{+\\infty} \\left(\\sum_{j=1}^m \\beta_j g^{\\prime\\prime}(x)\\right)^2\\,\\mathrm{d}x\\right).  \\]"},{"location":"suggestions/acrobatbookmark/","title":"Acrobat\u81ea\u52a8\u6dfb\u52a0\u4e66\u7b7e","text":""},{"location":"suggestions/acrobatbookmark/#_1","title":"\u51c6\u5907","text":"<p>1\u3001Adobe Acrobat 2\u3001AutoBookmark Standard Plug-in\uff1aadobe acrobat\u7684\u81ea\u52a8\u751f\u6210\u4e66\u7b7e\u7684\u63d2\u4ef6(1)</p> <ol> <li>\u89e3\u538b\u540e\u5c06\u76ee\u5f55\u62f7\u8d1d\u5230Adobe\\AcrobatDC\\Acrobat\\plug_ins\u6587\u4ef6\u5939\u4e0b </li> </ol>"},{"location":"suggestions/acrobatbookmark/#_2","title":"\u6d41\u7a0b","text":""},{"location":"suggestions/acrobatbookmark/#1pdfword","title":"1\u3001\u5bfc\u51faPDF\u76ee\u5f55\u5185\u5bb9\u5230Word","text":"<p>\u5c06word \u5185\u5bb9\u62f7\u8d1d\u5230\u65b0\u5efa\u7684txt\u6587\u672c\u4e2d(TXT\u9700\u8981\u4f7f\u7528ANSI\u7f16\u7801(1))</p> <ol> <li>\u7528\u8bb0\u4e8b\u672c\u6253\u5f00txt\u6587\u4ef6\uff0c\u9009\u62e9\u53e6\u5b58\u4e3a\uff0c\u7f16\u7801\u4fee\u6539\u4e3aANSI </li> </ol>"},{"location":"suggestions/acrobatbookmark/#2txt","title":"2\u3001TXT\u6587\u4ef6\u683c\u5f0f\u7f16\u8f91","text":"<p>\u6700\u540eTXT\u6587\u4ef6\u5185\u5bb9\u683c\u5f0f\uff1a</p> <p>[\u6807\u7b7e\u540d\u79f0],[\u76ee\u5f55\u9875\u7801],[\u76ee\u5f55\u9875\u7801\u548c\u5b9e\u9645\u9875\u7801\u7684\u504f\u79fb\u91cf]</p> <p></p> <p>\u5904\u7406\u6d41\u7a0b\uff1a</p> <ol> <li>tab\u66ff\u6362\u4e3a\u7a7a\u683c</li> <li>\u591a\u4e2a\u7a7a\u683c\u66ff\u6362\u4e3a\u4e00\u4e2a\u7a7a\u683c</li> <li>\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u4fee\u6539\u6bcf\u884c\u7684\u5185\u5bb9</li> <li>\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u4fee\u6539\u5404\u7ea7\u76ee\u5f55\u7684\u7f29\u8fdb</li> </ol> <p>\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u6cd5\uff1a</p> <p>\\w: \u6570\u5b57\u6216\u5927\u5c0f\u5199\u5b57\u6bcd +: \u4e00\u6b21\u6216\u591a\u6b21\u5339\u914d\u524d\u9762\u7684\u5b57\u7b26 (): \u6355\u83b7\u5305\u542b\u5728\u62ec\u53f7\u4e2d\u7684\u8868\u8fbe\u5f0f\u5e76\u5bf9\u5176\u8fdb\u884c\u9690\u5f0f\u7f16\u53f7 $1: \u5f15\u7528\u7b2c\u4e00\u4e2a\u9690\u5f0f\u7f16\u53f7\u5185\u5bb9 e.g.: (\\w+,\\w)\\n \u66ff\u6362\u4e3a $1,+11\\n\\t</p> <p></p>"},{"location":"suggestions/acrobatbookmark/#3txt","title":"3\u3001\u5bfc\u5165TXT\uff0c\u81ea\u52a8\u751f\u6210\u4e66\u7b7e","text":"<ol> <li>\u589e\u6548\u5de5\u5177 -&gt; Bookmarks -&gt; Create From TextFile</li> <li>\u9009\u62e9TXT\u6587\u4ef6\uff0cField delimiter\u8981\u8bbe\u4e3aComma\uff0c\u70b9\u51fbOK\u540e\u81ea\u52a8\u751f\u6210\u4e66\u7b7e</li> </ol>"},{"location":"suggestions/creditcardcomp/","title":"Credit Cards Comparison","text":""}]}